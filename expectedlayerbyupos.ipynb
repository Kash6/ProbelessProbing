{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yrde5xcBr6uT",
        "outputId": "ee7a6628-d344-4849-eca6-2924bea572b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertviz\n",
            "  Downloading bertviz-1.4.0-py3-none-any.whl (157 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m799.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=2.0 in /usr/local/lib/python3.10/dist-packages (from bertviz) (4.38.2)\n",
            "Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.10/dist-packages (from bertviz) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bertviz) (4.66.2)\n",
            "Collecting boto3 (from bertviz)\n",
            "  Downloading boto3-1.34.86-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bertviz) (2.31.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from bertviz) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bertviz) (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0->bertviz)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0->bertviz)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0->bertviz)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0->bertviz)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0->bertviz)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0->bertviz)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0->bertviz)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0->bertviz)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0->bertviz)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0->bertviz)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0->bertviz)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0->bertviz)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.0->bertviz) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.0->bertviz) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.0->bertviz) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.0->bertviz) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.0->bertviz) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.0->bertviz) (0.4.2)\n",
            "Collecting botocore<1.35.0,>=1.34.86 (from boto3->bertviz)\n",
            "  Downloading botocore-1.34.86-py3-none-any.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->bertviz)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->bertviz)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bertviz) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bertviz) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bertviz) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bertviz) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.86->boto3->bertviz) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0->bertviz) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0->bertviz) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.86->boto3->bertviz) (1.16.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, bertviz\n",
            "Successfully installed bertviz-1.4.0 boto3-1.34.86 botocore-1.34.86 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 s3transfer-0.10.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install bertviz\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obglwrofsGSt",
        "outputId": "9e05844d-e056-4b2e-a4cc-c3440324e5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  streusle-4.4.zip\n",
            "f722870ea5043ffe6c9506a9d530587881203aeb\n",
            "   creating: streusle-4.4/\n",
            "  inflating: streusle-4.4/.gitignore  \n",
            "  inflating: streusle-4.4/.travis.yml  \n",
            "  inflating: streusle-4.4/ACKNOWLEDGMENTS.md  \n",
            "  inflating: streusle-4.4/ACL2018.md  \n",
            "  inflating: streusle-4.4/CONLLULEX.md  \n",
            "  inflating: streusle-4.4/EXCEL.md   \n",
            "  inflating: streusle-4.4/LEXCAT.txt  \n",
            "  inflating: streusle-4.4/LICENSE.txt  \n",
            "  inflating: streusle-4.4/MWES.txt   \n",
            "  inflating: streusle-4.4/README.md  \n",
            "  inflating: streusle-4.4/STATS.md   \n",
            "  inflating: streusle-4.4/SUPERSENSES.txt  \n",
            "  inflating: streusle-4.4/UDlextag2json.py  \n",
            "  inflating: streusle-4.4/conllulex2UDlextag.py  \n",
            "  inflating: streusle-4.4/conllulex2csv.py  \n",
            "  inflating: streusle-4.4/conllulex2json.py  \n",
            "  inflating: streusle-4.4/csv2conllulex.py  \n",
            "   creating: streusle-4.4/dev/\n",
            "  inflating: streusle-4.4/dev/LEXCAT.txt  \n",
            "  inflating: streusle-4.4/dev/MWES.txt  \n",
            "  inflating: streusle-4.4/dev/STATS.md  \n",
            "  inflating: streusle-4.4/dev/SUPERSENSES.txt  \n",
            "  inflating: streusle-4.4/dev/streusle.ud_dev.conllulex  \n",
            "  inflating: streusle-4.4/dev/streusle.ud_dev.govobj.json  \n",
            "  inflating: streusle-4.4/dev/streusle.ud_dev.json  \n",
            "  inflating: streusle-4.4/dev/streusle.ud_dev.vis  \n",
            "  inflating: streusle-4.4/example.png  \n",
            "  inflating: streusle-4.4/govobj.py  \n",
            "  inflating: streusle-4.4/json2conllulex.py  \n",
            "  inflating: streusle-4.4/lexcatter.py  \n",
            "  inflating: streusle-4.4/mwerender.py  \n",
            "  inflating: streusle-4.4/normalize_mwe_numbering.py  \n",
            "  inflating: streusle-4.4/psseval.py  \n",
            "   creating: streusle-4.4/pssid/\n",
            "  inflating: streusle-4.4/pssid/README.md  \n",
            "  inflating: streusle-4.4/pssid/helpers.py  \n",
            "  inflating: streusle-4.4/pssid/identify.py  \n",
            "   creating: streusle-4.4/pssid/models/\n",
            "  inflating: streusle-4.4/pssid/models/streusle.ud_train.bestF.model  \n",
            "  inflating: streusle-4.4/pssid/models/streusle.ud_train.bestR.model  \n",
            "  inflating: streusle-4.4/pssid/streusle.ud_dev.auto_id.conllulex  \n",
            "  inflating: streusle-4.4/pssid/streusle.ud_test.auto_id.conllulex  \n",
            "  inflating: streusle-4.4/pssid/streusle.ud_train.auto_id.conllulex  \n",
            "   creating: streusle-4.4/releaseutil/\n",
            "  inflating: streusle-4.4/releaseutil/helpers.py  \n",
            "  inflating: streusle-4.4/releaseutil/release.sh  \n",
            "  inflating: streusle-4.4/releaseutil/split.py  \n",
            "  inflating: streusle-4.4/releaseutil/stats.sh  \n",
            "  inflating: streusle-4.4/releaseutil/syncud.py  \n",
            "  inflating: streusle-4.4/releaseutil/ud_dev_sent_ids.txt  \n",
            "  inflating: streusle-4.4/releaseutil/ud_test_sent_ids.txt  \n",
            "  inflating: streusle-4.4/releaseutil/ud_train_sent_ids.txt  \n",
            "  inflating: streusle-4.4/setup.py   \n",
            "  inflating: streusle-4.4/streuseval.py  \n",
            "  inflating: streusle-4.4/streusle.conllulex  \n",
            "  inflating: streusle-4.4/streusvis.py  \n",
            "  inflating: streusle-4.4/supdate.py  \n",
            "  inflating: streusle-4.4/supersenses.py  \n",
            "  inflating: streusle-4.4/tagging.py  \n",
            "   creating: streusle-4.4/test/\n",
            "  inflating: streusle-4.4/test/LEXCAT.txt  \n",
            "  inflating: streusle-4.4/test/MWES.txt  \n",
            "  inflating: streusle-4.4/test/STATS.md  \n",
            "  inflating: streusle-4.4/test/SUPERSENSES.txt  \n",
            "  inflating: streusle-4.4/test/streusle.ud_test.conllulex  \n",
            "  inflating: streusle-4.4/test/streusle.ud_test.govobj.json  \n",
            "  inflating: streusle-4.4/test/streusle.ud_test.json  \n",
            "  inflating: streusle-4.4/test/streusle.ud_test.vis  \n",
            "  inflating: streusle-4.4/tox.ini    \n",
            "  inflating: streusle-4.4/tquery.py  \n",
            "   creating: streusle-4.4/train/\n",
            "  inflating: streusle-4.4/train/LEXCAT.txt  \n",
            "  inflating: streusle-4.4/train/MWES.txt  \n",
            "  inflating: streusle-4.4/train/STATS.md  \n",
            "  inflating: streusle-4.4/train/SUPERSENSES.txt  \n",
            "  inflating: streusle-4.4/train/streusle.ud_train.conllulex  \n",
            "  inflating: streusle-4.4/train/streusle.ud_train.govobj.json  \n",
            "  inflating: streusle-4.4/train/streusle.ud_train.json  \n",
            "  inflating: streusle-4.4/train/streusle.ud_train.vis  \n",
            "  inflating: streusle-4.4/tupdate.py  \n"
          ]
        }
      ],
      "source": [
        "!unzip streusle-4.4.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726,
          "referenced_widgets": [
            "7a0207d8cc954eb7bd5f31c7e99acd35",
            "6dd818ca61604cbbbb27fcc4f00967fe",
            "5b787e4fe8b24dde943e73c7db2ff841",
            "af4a40eb086243efa6fa1d21d16815fc",
            "9dc22ef36b2043c3abb99d68996546ec",
            "3cd48d2524fe4f2787e06d9522113ea9",
            "62334709da264209a608305893874ced",
            "c7c36aa00bf74c34a63a9aa1a704f66a",
            "542de6b91c48491cb08219d3e870df71",
            "d70f14c1d9ef4954996aae8706c6e671",
            "38e655ecbaa541c1b3ff9707dac6ccd1",
            "2034c74e9af6449f869ebde329efb7b4",
            "e203c2dbb2874371ae42f4c347a4a264",
            "482a4ac81f244af1920d9ed64b2e297c",
            "97faa8c43737489e95e1c2b6d064c640",
            "17246e9f9dbd4c3f8ed1403328225ed9",
            "e44f610976f34572bcf82ccdfe6354c9",
            "33de9fab55dd4a608babb18cd92f91f2",
            "cf2ad21e39204fc29721209d48d43dc6",
            "916bc11a08a94f11a7d4fe792c09ea7a",
            "46e59ecf49a04b71b2bdb98b0bbdb4d0",
            "0a9c869453b6435d85c6e27a7f19b368",
            "5acb23abcff441febf19d5cab9ee8b74",
            "d1a09f5904c14f2b85da39fb64c4fae7",
            "61be08e0e9624774b855d521da030c85",
            "3ed37deb32ea480d805b7dd2f4d73c90",
            "9f52c09f94e1472abebd9a894b5c42c5",
            "f7f0370d15b5432da1db9ea9c0dd0f4e",
            "c4ee8ce740c8491e9945af618a9b1304",
            "6e203a49fd734fd5b0d74d92c738630c",
            "f2ec41d698b54acfb973b11223ca506f",
            "64653220c76149aeb2fcb3db8c46d95f",
            "808ba03997a64faa9b7c38044d69cc43",
            "6dde54559e6945f7b0563f2829859b7b",
            "05b92727d01741bc8014396532f0a3b1",
            "d85b4f2fca9741e49b21527f48431243",
            "aaa5dfc544e646d9aaba569f2a871c72",
            "91db76b682cf490d95d3dd8762d8a5ac",
            "e7e5f8bfc8cb43f294a4679b80f47c57",
            "cf21181902ae4d0288962562dfb2a05e",
            "eebd9d66e5854f5daf43cd5f162e886c",
            "2a6ed28be5494151a85d0bcec83e0051",
            "89eda6ed499e461c8067b1e7832b5974",
            "08f98b93c72f4214bf201aba8ef3284c",
            "746a123458d24c4fa74412d69e994e48",
            "cb03462c0fa047de94b117b777e1bf89",
            "badd3cfa57a3438e9bf0b0e7e3563bba",
            "29afeada0dd843cbbfdd39d05bde7307",
            "6ffdca63f6bf45ceb4a927305deaa09e",
            "a8d9db59d43c41b2a007af4b01c5e4d6",
            "7d1e1fd3f71744e99bcc730361ca9a5f",
            "429a9bba7c3944289fc1dd5d590de26e",
            "f8f96a172e1e4d738d49a34b4ac61a5b",
            "135bfa486ccf4fdaabf6322b54badcf7",
            "d5e2049398bb4e53b47301d72270bf91"
          ]
        },
        "id": "YXPtUy5GxzlL",
        "outputId": "f87e7f87-dd79-4248-e715-946cfb9a06a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a0207d8cc954eb7bd5f31c7e99acd35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2034c74e9af6449f869ebde329efb7b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5acb23abcff441febf19d5cab9ee8b74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6dde54559e6945f7b0563f2829859b7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "746a123458d24c4fa74412d69e994e48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.10731099545955658,\n",
              "  'token': 4827,\n",
              "  'token_str': 'fashion',\n",
              "  'sequence': \"hello i'm a fashion model.\"},\n",
              " {'score': 0.08774477988481522,\n",
              "  'token': 2535,\n",
              "  'token_str': 'role',\n",
              "  'sequence': \"hello i'm a role model.\"},\n",
              " {'score': 0.05338403955101967,\n",
              "  'token': 2047,\n",
              "  'token_str': 'new',\n",
              "  'sequence': \"hello i'm a new model.\"},\n",
              " {'score': 0.04667219892144203,\n",
              "  'token': 3565,\n",
              "  'token_str': 'super',\n",
              "  'sequence': \"hello i'm a super model.\"},\n",
              " {'score': 0.02709587849676609,\n",
              "  'token': 2986,\n",
              "  'token_str': 'fine',\n",
              "  'sequence': \"hello i'm a fine model.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
        "unmasker(\"Hello I'm a [MASK] model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw9XbUhq5v-a",
        "outputId": "414ad69a-a946-4a92-fb9d-9c064b344f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word treated as VERB in UD, ADJ for supersenses: reviews-036753-0003 blown\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-036753-0005 blown\n",
            "Simplifying: removing weak group that interleaves with a strong gap: [24, 25, 28] this kebab shop is one of the best around the meat is good and fresh and the chilly sauce is the best , keep_ them$1 lovely kebabs _coming$1 and a happy_new_year to all the staff\n",
            "MWE string mismatch (may be due to simplification): this kebab shop is one of the best around the meat is good and fresh and the chilly sauce is the best , keep_ them~ lovely kebabs _coming and a happy_new_year to all the staff this kebab shop is one of the best around the meat is good and fresh and the chilly sauce is the best , keep_ them$1 lovely kebabs _coming$1 and a happy_new_year to all the staff reviews-037179-0002\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-043020-0014 cracked\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-047184-0005 filled\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-055207-0003 reduced\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-063347-0004 filled\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-071518-0002 cracked\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-071650-0005 lacking\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-072507-0004 focused\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-081116-0001 fried\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-112661-0004 Fried\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-115566-0001 Retired\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-116821-0009 crushed\n",
            "Simplifying: removing weak group that interleaves with a strong gap: [7, 8, 9, 10] ok but just becuse we where on_a_ tight$1 _budget$1 .\n",
            "MWE string mismatch (may be due to simplification): ok but just becuse we where on_a_ tight _budget . ok but just becuse we where on_a_ tight$1 _budget$1 . reviews-159371-0001\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-165018-0003 used\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-187266-0005 surprised\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-210066-0006 finished\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-210066-0007 finished\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-215460-0003 repeated\n",
            "MWE string mismatch: Ifa is an acronym for I m a F%#king Assh@%$e !!! Ifa is an acronym for Im a F%#king Assh@%$e !!! reviews-217359-0008\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-220214-0005 following\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-224906-0006 fried\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-235423-0003 stunk\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-235423-0013 steamed\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-240287-0002 priced\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-262722-0007 sized\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-268673-0003 Speaking\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-280844-0009 glowing\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-290594-0009 filled\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-303922-0004 recommended\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-313825-0002 refreshing\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-313825-0006 added\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-319816-0003 shredded\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-319816-0010 following\n",
            "Simplifying: removing weak group that interleaves with a strong gap: [9, 10, 11, 12, 13] On Monday I called and again it was a_ big$1 _to_-_do$1 to find anyone who knew anything about it .\n",
            "MWE string mismatch (may be due to simplification): On Monday I called and again it was a_ big _to_-_do to find anyone who knew anything about it . On Monday I called and again it was a_ big$1 _to_-_do$1 to find anyone who knew anything about it . reviews-319816-0026\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-329991-0002 Mixed\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-340848-0009 priced\n",
            "Simplifying: removing weak group that interleaves with a strong gap: [15, 16, 17] All you have_to do to make it authentic Jamaican food , is add a_ whole$1 _lot$1 of pepper .\n",
            "MWE string mismatch (may be due to simplification): All you have_to do to make it authentic Jamaican food , is add a_ whole _lot of pepper . All you have_to do to make it authentic Jamaican food , is add a_ whole$1 _lot$1 of pepper . reviews-342807-0005\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-342811-0008 left\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-354336-0002 born\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-354336-0002 trained\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-354336-0002 sourced\n",
            "Simplifying: removing gappy group that is wholly contained within another gap: [3, 4, 6] I have_ nothing_but~ fantastic ~things _to_say .\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-363873-0004 closed\n",
            "Simplifying: removing weak group that interleaves with a strong gap: [16, 17, 18, 19, 20, 21] If the animals are treated in the same way the customers are treated then this leaves_ a_lot$1 _to_be_desired$1 !\n",
            "MWE string mismatch (may be due to simplification): If the animals are treated in the same way the customers are treated then this leaves_ a_lot _to_be_desired ! If the animals are treated in the same way the customers are treated then this leaves_ a_lot$1 _to_be_desired$1 ! reviews-374604-0002\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-377347-0008 informed\n",
            "Word treated as VERB in UD, ADJ for supersenses: reviews-398243-0010 working\n"
          ]
        }
      ],
      "source": [
        "!python streusle-4.4/conllulex2json.py streusle-4.4/streusle.conllulex > streusle-4.4/streusle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "52cdf811fa1a429ebdfc7a26952b026c",
            "3f33895b09064cb7843fdc23e830c800",
            "b12cd11750ae42bdb1891a44fa5353a6",
            "05768e037d584fa2b5bdf5f34bdac0a7",
            "e496fb6ece9d4943910f6b1cd596a8b7",
            "7976d0abf363498897ca8bf1e0d08999",
            "86418c61262443999cb49f960d0ed1b1",
            "bc0e21c8255b4c17ad880a4ed6c11f2f",
            "e30566d719a74bdebab1e48f41a5f47f",
            "d5fd425818f5408d8f06c7b73e7bc516",
            "08cfdb214f6b42ab929d4f273c719687",
            "5e0558316ef441739127bd16a203c362",
            "4801be5d6a924a6c81a4a72e9361fcd8",
            "8c6e7734096f4b4aab4d5c9f78ecae22",
            "6ed10af62eda4278bf2a6528d31799b2",
            "1af225ddc1614da8a6d5011758e95308",
            "442ce855a39041038bf014e3ddf22c44",
            "39bd75748d014329a90fe55225db5354",
            "22e9e645a203495085d3042b43f58f8b",
            "57313bf489aa4588bab97806f102a365",
            "1fc27f10e9294228ac21ba4f055e48d6",
            "06570b4dceab45de8083dc6e1de87966",
            "8dda49002e9647e8ba785fd85dd253e4",
            "452dee80d1b64e7881d439874c472788",
            "77cee74c2a1a49c48d689c5619d5b544",
            "7bb767d7aac742348a38be85ffc0865c",
            "3edd9b9fd268409ca79470e559a031ba",
            "f3b93e2eddbb497cb982cb5b36cf002d",
            "f21e1bf57d5344539092db3c8c1e720f",
            "b50a00e634df4b7ca8822a2d8ec50cd3",
            "3fb5b6d966c9499eb71d3e4e869053b4",
            "5d87757fc3424196934d36c033119ad8",
            "1f775c1542b1440e9a72fcdf05782ad7"
          ]
        },
        "id": "tzeat5cI-4mJ",
        "outputId": "900771d7-ed6e-4913-9197-741f8663742b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.8.1-py3-none-any.whl (970 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m970.4/970.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji (from stanza)\n",
            "  Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
            "Installing collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.11.0 stanza-1.8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52cdf811fa1a429ebdfc7a26952b026c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: en (English) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/default.zip:   0%|          | 0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e0558316ef441739127bd16a203c362"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/en/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dda49002e9647e8ba785fd85dd253e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "WARNING:stanza:Language en package default expects mwt, which has been added\n",
            "INFO:stanza:Loading these models for language: en (English):\n",
            "===============================\n",
            "| Processor | Package         |\n",
            "-------------------------------\n",
            "| tokenize  | combined        |\n",
            "| mwt       | combined        |\n",
            "| pos       | combined_charlm |\n",
            "===============================\n",
            "\n",
            "INFO:stanza:Using device: cuda\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "#For POS!!!!\n",
        "!pip install stanza\n",
        "import stanza\n",
        "stanza.download('en')\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9_DA1g7D6G4M"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import json\n",
        "# from transformers import BertForMaskedLM, BertTokenizer\n",
        "# from collections import defaultdict\n",
        "\n",
        "# # Load the model and tokenizer\n",
        "# model_name = 'bert-base-uncased'\n",
        "# model = BertForMaskedLM.from_pretrained(model_name, output_hidden_states=True)\n",
        "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "# model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# # Load the data\n",
        "# data = []\n",
        "# file_path = 'streusle-4.4/streusle.json'\n",
        "# with open(file_path, 'r', encoding='utf-8') as file:\n",
        "#     data = json.load(file)\n",
        "\n",
        "# # Function to process the data\n",
        "# def process_data(data):\n",
        "#     accuracies_by_layer_and_upos = defaultdict(lambda: defaultdict(lambda: [0, 0]))\n",
        "\n",
        "#     for sentence in data:\n",
        "#         text = sentence[\"text\"]\n",
        "#         tokens = text.split()\n",
        "#         token_data = sentence[\"toks\"]\n",
        "\n",
        "#         for i, token_info in enumerate(token_data):\n",
        "#             original_token = token_info[\"word\"]\n",
        "#             upos_tag = token_info[\"upos\"]\n",
        "#             # Create a masked version of the sentence\n",
        "#             masked_tokens = tokens[:i] + ['[MASK]'] + tokens[i+1:]\n",
        "#             masked_sentence = \" \".join(masked_tokens)\n",
        "#             inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
        "\n",
        "#             # Get model output\n",
        "#             with torch.no_grad():\n",
        "#                 outputs = model(**inputs)\n",
        "#                 hidden_states = outputs.hidden_states  # hidden states from each layer\n",
        "\n",
        "#             # Masked index\n",
        "#             masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n",
        "\n",
        "#             # Analyze each layer's predictions\n",
        "#             for layer_index, layer_output in enumerate(hidden_states):\n",
        "#                 logits = layer_output[0, masked_index]\n",
        "#                 softmax_logits = torch.softmax(logits, dim=0)\n",
        "#                 predicted_token_id = softmax_logits.argmax().item()\n",
        "#                 predicted_token = tokenizer.decode([predicted_token_id]).strip()\n",
        "\n",
        "#                 # Update accuracies\n",
        "#                 correct = int(predicted_token.lower() == original_token.lower())\n",
        "#                 accuracies_by_layer_and_upos[upos_tag][layer_index][0] += correct\n",
        "#                 accuracies_by_layer_and_upos[upos_tag][layer_index][1] += 1\n",
        "\n",
        "#     return accuracies_by_layer_and_upos\n",
        "\n",
        "# # Calculate accuracies\n",
        "# accuracies_by_layer_and_upos = process_data(data)\n",
        "\n",
        "# # Compute the accuracies and print them\n",
        "# for upos, layers in accuracies_by_layer_and_upos.items():\n",
        "#     print(f\"\\nAccuracies for {upos}:\")\n",
        "#     for layer, (correct_count, total_count) in sorted(layers.items()):\n",
        "#         accuracy = correct_count / total_count if total_count > 0 else 0\n",
        "#         print(f\"  Layer {layer}: {accuracy:.2f} (Correct: {correct_count}, Total: {total_count})\")\n",
        "\n",
        "# # Calculate and print expected layers for each UPOS\n",
        "# print(\"\\nExpected layers for each UPOS:\")\n",
        "# for upos, layers in accuracies_by_layer_and_upos.items():\n",
        "#     differential_scores = [layers[i][0] / layers[i][1] - layers[i-1][0] / layers[i-1][1] if i > 0 and layers[i-1][1] > 0 else 0 for i in range(13)]\n",
        "#     expected_layer = sum(i * score for i, score in enumerate(differential_scores, start=1)) / sum(differential_scores) if sum(differential_scores) else 0\n",
        "#     print(f\"{upos}: {expected_layer:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYS2FSTE48Y_",
        "outputId": "7f60647d-a0e9-4489-eb46-873a72f49f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'out', 'of', 'business', '.', '[SEP]']\n",
            "Original: ['Nice', 'people...', 'I', 'hear.']\n",
            "Tokenized: ['[CLS]', 'nice', 'people', '.', '.', '.', 'i', 'hear', '.', '[SEP]']\n",
            "Original: ['Calls', 'are', 'now', 'forwarded', 'to', 'Malcolm', 'Smith', 'Motorsports', 'down', 'the', 'road.']\n",
            "Tokenized: ['[CLS]', 'calls', 'are', 'now', 'forward', '##ed', 'to', 'malcolm', 'smith', 'motorsports', 'down', 'the', 'road', '.', '[SEP]']\n",
            "Original: ['Anyone', 'else', 'find', 'it', 'a', 'little', 'suspicious', 'that', 'there', 'are', 'not', 'only', '20', 'reviews', 'for', 'this', 'dentist', '(a', 'HUGE', 'number', 'compared', 'to', 'the', 'others', 'in', 'the', 'area),', 'but', 'that', 'they', 'all', 'have', 'the', 'same', 'unique', 'grammar', 'structure?']\n",
            "Tokenized: ['[CLS]', 'anyone', 'else', 'find', 'it', 'a', 'little', 'suspicious', 'that', 'there', 'are', 'not', 'only', '20', 'reviews', 'for', 'this', 'dentist', '(', 'a', 'huge', 'number', 'compared', 'to', 'the', 'others', 'in', 'the', 'area', ')', ',', 'but', 'that', 'they', 'all', 'have', 'the', 'same', 'unique', 'grammar', 'structure', '?', '[SEP]']\n",
            "Original: ['And', 'they', 'seem', 'to', 'be', 'posted', 'at', 'fairly', 'regular', 'intervals?']\n",
            "Tokenized: ['[CLS]', 'and', 'they', 'seem', 'to', 'be', 'posted', 'at', 'fairly', 'regular', 'intervals', '?', '[SEP]']\n",
            "Original: ['I', 'like', 'I', 'Move', 'CA', '-', 'Los', 'Angeles', 'Movers,', 'they', 'moved', 'me', 'before,', 'but', 'this', 'time', 'they', 'were', 'awesome', ':)']\n",
            "Tokenized: ['[CLS]', 'i', 'like', 'i', 'move', 'ca', '-', 'los', 'angeles', 'move', '##rs', ',', 'they', 'moved', 'me', 'before', ',', 'but', 'this', 'time', 'they', 'were', 'awesome', ':', ')', '[SEP]']\n",
            "Original: ['Usually', 'very', 'quick', 'and', 'timely.']\n",
            "Tokenized: ['[CLS]', 'usually', 'very', 'quick', 'and', 'timely', '.', '[SEP]']\n",
            "Original: ['Doctor', 'Bogomilsky', 'knows', 'her', 'stuff', 'too.']\n",
            "Tokenized: ['[CLS]', 'doctor', 'bog', '##omi', '##ls', '##ky', 'knows', 'her', 'stuff', 'too', '.', '[SEP]']\n",
            "Original: ['Only', 'Concerned', 'With', 'Money']\n",
            "Tokenized: ['[CLS]', 'only', 'concerned', 'with', 'money', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'getting', 'my', 'treatments', 'for', 'a', 'few', 'months', 'now', 'and', 'have', 'seen', 'some', 'results', 'but', 'not', 'up', 'to', 'the', 'standards', 'that', 'I', 'was', 'told', 'I', 'should', 'expect.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'getting', 'my', 'treatments', 'for', 'a', 'few', 'months', 'now', 'and', 'have', 'seen', 'some', 'results', 'but', 'not', 'up', 'to', 'the', 'standards', 'that', 'i', 'was', 'told', 'i', 'should', 'expect', '.', '[SEP]']\n",
            "Original: ['The', 'people', 'there', 'attempt', 'to', 'come', 'across', 'and', 'professional', 'and', 'nice,', 'but', 'I', 'was', 'disappointed', 'with', 'their', 'customer', 'service.']\n",
            "Tokenized: ['[CLS]', 'the', 'people', 'there', 'attempt', 'to', 'come', 'across', 'and', 'professional', 'and', 'nice', ',', 'but', 'i', 'was', 'disappointed', 'with', 'their', 'customer', 'service', '.', '[SEP]']\n",
            "Original: ['Never', 'miss', 'an', 'appointment', 'because', 'they', 'will', 'charge', 'you', 'the', 'price', 'of', 'a', 'treatment,', 'even', 'if', 'you', 'had', 'an', 'emergency.']\n",
            "Tokenized: ['[CLS]', 'never', 'miss', 'an', 'appointment', 'because', 'they', 'will', 'charge', 'you', 'the', 'price', 'of', 'a', 'treatment', ',', 'even', 'if', 'you', 'had', 'an', 'emergency', '.', '[SEP]']\n",
            "Original: [\"It's\", 'pretty', 'ridiculous!']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'pretty', 'ridiculous', '!', '[SEP]']\n",
            "Original: ['They', 'want', 'to', 'squeeze', 'as', 'much', 'as', 'they', 'can', 'from', 'you', 'even', 'if', 'you', 'just', 'got', 'in', 'a', 'car', 'accident!!!!']\n",
            "Tokenized: ['[CLS]', 'they', 'want', 'to', 'squeeze', 'as', 'much', 'as', 'they', 'can', 'from', 'you', 'even', 'if', 'you', 'just', 'got', 'in', 'a', 'car', 'accident', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['They', \"don't\", 'care', 'one', 'bit', 'about', 'you!!!!!']\n",
            "Tokenized: ['[CLS]', 'they', 'don', \"'\", 't', 'care', 'one', 'bit', 'about', 'you', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['great', 'service/deals', '-', 'support', 'this', 'local', 'business']\n",
            "Tokenized: ['[CLS]', 'great', 'service', '/', 'deals', '-', 'support', 'this', 'local', 'business', '[SEP]']\n",
            "Original: ['I', 'have', 'used', 'these', 'guys', 'for', 'new', 'snows,', 'fixing', 'lots', 'of', 'flats,', 'used', 'replacement', 'tires,', 'and', 'oil', 'changes.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'used', 'these', 'guys', 'for', 'new', 'snow', '##s', ',', 'fixing', 'lots', 'of', 'flats', ',', 'used', 'replacement', 'tires', ',', 'and', 'oil', 'changes', '.', '[SEP]']\n",
            "Original: ['They', 'have', 'the', 'best', 'prices', 'locally', 'and', 'good', 'customer', 'service.']\n",
            "Tokenized: ['[CLS]', 'they', 'have', 'the', 'best', 'prices', 'locally', 'and', 'good', 'customer', 'service', '.', '[SEP]']\n",
            "Original: ['One', 'guy', 'is', 'a', 'little', 'surley,', 'but', 'who', 'gives', 'a', 'crap', 'as', 'long', 'as', 'your', \"car's\", 'work', 'is', 'outstanding.']\n",
            "Tokenized: ['[CLS]', 'one', 'guy', 'is', 'a', 'little', 'sur', '##ley', ',', 'but', 'who', 'gives', 'a', 'crap', 'as', 'long', 'as', 'your', 'car', \"'\", 's', 'work', 'is', 'outstanding', '.', '[SEP]']\n",
            "Original: ['AND', \"they're\", 'usually', 'able', 'to', 'help', 'you', 'as', 'a', 'walk-in,', 'and', \"they're\", 'fast.']\n",
            "Tokenized: ['[CLS]', 'and', 'they', \"'\", 're', 'usually', 'able', 'to', 'help', 'you', 'as', 'a', 'walk', '-', 'in', ',', 'and', 'they', \"'\", 're', 'fast', '.', '[SEP]']\n",
            "Original: ['Overall-good', 'stuff.']\n",
            "Tokenized: ['[CLS]', 'overall', '-', 'good', 'stuff', '.', '[SEP]']\n",
            "Original: ['Real', 'pros']\n",
            "Tokenized: ['[CLS]', 'real', 'pro', '##s', '[SEP]']\n",
            "Original: [\"I've\", 'had', 'writer', 'friends', 'describe', 'horror', 'stories', 'with', 'their', 'printers.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 've', 'had', 'writer', 'friends', 'describe', 'horror', 'stories', 'with', 'their', 'printers', '.', '[SEP]']\n",
            "Original: ['I', 'tell', 'them:', 'why', 'not', 'just', 'go', 'with', 'these', 'guys?']\n",
            "Tokenized: ['[CLS]', 'i', 'tell', 'them', ':', 'why', 'not', 'just', 'go', 'with', 'these', 'guys', '?', '[SEP]']\n",
            "Original: ['Richard', 'Joule', 'and', 'the', 'gang', 'are', 'pros', 'from', 'start', 'to', 'finish.']\n",
            "Tokenized: ['[CLS]', 'richard', 'jo', '##ule', 'and', 'the', 'gang', 'are', 'pro', '##s', 'from', 'start', 'to', 'finish', '.', '[SEP]']\n",
            "Original: ['They', 'set', 'out', 'to', 'exceed', 'your', 'expectations.']\n",
            "Tokenized: ['[CLS]', 'they', 'set', 'out', 'to', 'exceed', 'your', 'expectations', '.', '[SEP]']\n",
            "Original: ['Already', \"I'm\", 'considering', 'future', 'projects,', 'and', 'I', 'can', 'assure', 'you', 'that', 'for', 'my', 'printing', 'needs', 'I', 'will', 'be', 'choosing', 'no', 'other', 'than', 'Atlanta', 'Paperback', 'Book', 'Printing.']\n",
            "Tokenized: ['[CLS]', 'already', 'i', \"'\", 'm', 'considering', 'future', 'projects', ',', 'and', 'i', 'can', 'assure', 'you', 'that', 'for', 'my', 'printing', 'needs', 'i', 'will', 'be', 'choosing', 'no', 'other', 'than', 'atlanta', 'paperback', 'book', 'printing', '.', '[SEP]']\n",
            "Original: ['Lovely', 'Nails', 'on', 'Cayuga', 'St.', 'in', 'Lewiston,', 'NY']\n",
            "Tokenized: ['[CLS]', 'lovely', 'nails', 'on', 'ca', '##yu', '##ga', 'st', '.', 'in', 'lewis', '##ton', ',', 'ny', '[SEP]']\n",
            "Original: ['First', 'let', 'me', 'start', 'out', 'by', 'saying,', 'that', 'I', 'have', 'had', 'very', 'nice', 'pedicures', 'at', 'Lovely', 'Nails', 'on', 'Military', 'Road.']\n",
            "Tokenized: ['[CLS]', 'first', 'let', 'me', 'start', 'out', 'by', 'saying', ',', 'that', 'i', 'have', 'had', 'very', 'nice', 'pe', '##dic', '##ures', 'at', 'lovely', 'nails', 'on', 'military', 'road', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'very', 'excited', 'that', 'a', 'salon', 'was', 'opening', 'in', 'Lewiston,', 'as', 'I', 'live', 'in', 'Youngstown.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'very', 'excited', 'that', 'a', 'salon', 'was', 'opening', 'in', 'lewis', '##ton', ',', 'as', 'i', 'live', 'in', 'young', '##stown', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'in', 'two', 'weeks', 'ago', 'and', 'had', 'the', 'worst', 'pedicure', 'that', 'I', 'have', 'had', 'in', 'my', 'life.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'in', 'two', 'weeks', 'ago', 'and', 'had', 'the', 'worst', 'pe', '##dic', '##ure', 'that', 'i', 'have', 'had', 'in', 'my', 'life', '.', '[SEP]']\n",
            "Original: ['There', 'were', 'four', 'of', 'us', 'and', 'I', 'was', 'taken', 'first', 'by', 'a', 'gentleman.']\n",
            "Tokenized: ['[CLS]', 'there', 'were', 'four', 'of', 'us', 'and', 'i', 'was', 'taken', 'first', 'by', 'a', 'gentleman', '.', '[SEP]']\n",
            "Original: ['I', 'put', 'my', 'foot', 'in', 'the', 'water', 'and', 'it', 'was', 'cool.']\n",
            "Tokenized: ['[CLS]', 'i', 'put', 'my', 'foot', 'in', 'the', 'water', 'and', 'it', 'was', 'cool', '.', '[SEP]']\n",
            "Original: ['He', 'did', 'warm', 'it', 'up.']\n",
            "Tokenized: ['[CLS]', 'he', 'did', 'warm', 'it', 'up', '.', '[SEP]']\n",
            "Original: ['He', 'also', 'hurt', 'my', 'toes', 'will', 'pushing', 'my', 'cuticles', 'back.']\n",
            "Tokenized: ['[CLS]', 'he', 'also', 'hurt', 'my', 'toes', 'will', 'pushing', 'my', 'cut', '##icles', 'back', '.', '[SEP]']\n",
            "Original: ['If', 'the', 'pedicure', 'lasted', '20', 'mins.,', 'that', 'was', 'a', 'stretch.']\n",
            "Tokenized: ['[CLS]', 'if', 'the', 'pe', '##dic', '##ure', 'lasted', '20', 'min', '##s', '.', ',', 'that', 'was', 'a', 'stretch', '.', '[SEP]']\n",
            "Original: ['The', 'other', 'ladies', 'had', 'a', 'similar', 'experience,', 'both', 'had', 'nail', 'polish', 'on', 'a', 'couple', 'of', 'toes.']\n",
            "Tokenized: ['[CLS]', 'the', 'other', 'ladies', 'had', 'a', 'similar', 'experience', ',', 'both', 'had', 'nail', 'polish', 'on', 'a', 'couple', 'of', 'toes', '.', '[SEP]']\n",
            "Original: ['None', 'of', 'us', 'will', 'be', 'using', 'their', 'services', 'again,', 'which', 'is', 'a', 'shame.']\n",
            "Tokenized: ['[CLS]', 'none', 'of', 'us', 'will', 'be', 'using', 'their', 'services', 'again', ',', 'which', 'is', 'a', 'shame', '.', '[SEP]']\n",
            "Original: ['David', 'is', 'amazing']\n",
            "Tokenized: ['[CLS]', 'david', 'is', 'amazing', '[SEP]']\n",
            "Original: ['David', 'is', 'the', 'most', 'helpful', 'and', 'creative', 'photographer', 'that', 'I', 'have', 'used.']\n",
            "Tokenized: ['[CLS]', 'david', 'is', 'the', 'most', 'helpful', 'and', 'creative', 'photographer', 'that', 'i', 'have', 'used', '.', '[SEP]']\n",
            "Original: ['He', 'is', 'willing', 'to', 'do', 'whatever', 'you', 'need', 'from', 'him', 'without', 'hesitation.']\n",
            "Tokenized: ['[CLS]', 'he', 'is', 'willing', 'to', 'do', 'whatever', 'you', 'need', 'from', 'him', 'without', 'hesitation', '.', '[SEP]']\n",
            "Original: ['He', 'was', 'patient', 'and', 'adapted', 'when', 'everything', \"didn't\", 'go', 'according', 'to', 'schedule', 'on', 'my', 'wedding', 'day.']\n",
            "Tokenized: ['[CLS]', 'he', 'was', 'patient', 'and', 'adapted', 'when', 'everything', 'didn', \"'\", 't', 'go', 'according', 'to', 'schedule', 'on', 'my', 'wedding', 'day', '.', '[SEP]']\n",
            "Original: ['Both', 'the', 'engagement', 'and', 'wedding', 'pictures', 'that', 'he', 'took', 'for', 'us', 'we', 'absolutely', 'amazing.']\n",
            "Tokenized: ['[CLS]', 'both', 'the', 'engagement', 'and', 'wedding', 'pictures', 'that', 'he', 'took', 'for', 'us', 'we', 'absolutely', 'amazing', '.', '[SEP]']\n",
            "Original: ['He', 'got', 'the', 'pictures', 'back', 'to', 'me', 'quickly.']\n",
            "Tokenized: ['[CLS]', 'he', 'got', 'the', 'pictures', 'back', 'to', 'me', 'quickly', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'highly', 'recommend', 'David', 'to', 'anyone.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'highly', 'recommend', 'david', 'to', 'anyone', '.', '[SEP]']\n",
            "Original: ['He', 'will', 'exceed', 'your', 'expectations!']\n",
            "Tokenized: ['[CLS]', 'he', 'will', 'exceed', 'your', 'expectations', '!', '[SEP]']\n",
            "Original: ['He', \"doesn't\", 'just', 'take', 'pictures', 'he', 'makes', 'art', 'out', 'of', 'them', 'and', 'you', \"won't\", 'even', 'notice', 'that', \"there's\", 'a', 'camera', 'there.']\n",
            "Tokenized: ['[CLS]', 'he', 'doesn', \"'\", 't', 'just', 'take', 'pictures', 'he', 'makes', 'art', 'out', 'of', 'them', 'and', 'you', 'won', \"'\", 't', 'even', 'notice', 'that', 'there', \"'\", 's', 'a', 'camera', 'there', '.', '[SEP]']\n",
            "Original: ['Relish']\n",
            "Tokenized: ['[CLS]', 're', '##lish', '[SEP]']\n",
            "Original: ['A', 'Top', 'Quality', 'Sandwich', 'made', 'to', 'artistic', 'standards.']\n",
            "Tokenized: ['[CLS]', 'a', 'top', 'quality', 'sandwich', 'made', 'to', 'artistic', 'standards', '.', '[SEP]']\n",
            "Original: ['The', 'best', 'darlington', 'has', 'to', 'offer', 'in', 'contemporary', 'sandwicheering.']\n",
            "Tokenized: ['[CLS]', 'the', 'best', 'darlington', 'has', 'to', 'offer', 'in', 'contemporary', 'sandwich', '##eering', '.', '[SEP]']\n",
            "Original: ['Drum', 'and', 'bass', 'as', 'standard.']\n",
            "Tokenized: ['[CLS]', 'drum', 'and', 'bass', 'as', 'standard', '.', '[SEP]']\n",
            "Original: ['I', 'love', 'the', 'meat!']\n",
            "Tokenized: ['[CLS]', 'i', 'love', 'the', 'meat', '!', '[SEP]']\n",
            "Original: ['Fast', 'Service', 'Called', 'them', 'one', 'hour', 'ago', 'and', 'they', 'just', 'left', 'my', 'house', 'five', 'minutes', 'ago.']\n",
            "Tokenized: ['[CLS]', 'fast', 'service', 'called', 'them', 'one', 'hour', 'ago', 'and', 'they', 'just', 'left', 'my', 'house', 'five', 'minutes', 'ago', '.', '[SEP]']\n",
            "Original: ['My', 'house', 'already', 'feels', 'fresh', 'and', 'good', 'thanks', 'to', 'the', 'Battery', 'Park', 'Pest', \"I'm\", 'enjoying', 'my', 'time', 'indoors', 'much', 'better.']\n",
            "Tokenized: ['[CLS]', 'my', 'house', 'already', 'feels', 'fresh', 'and', 'good', 'thanks', 'to', 'the', 'battery', 'park', 'pest', 'i', \"'\", 'm', 'enjoying', 'my', 'time', 'indoors', 'much', 'better', '.', '[SEP]']\n",
            "Original: ['Will', 'never', 'use', 'again.']\n",
            "Tokenized: ['[CLS]', 'will', 'never', 'use', 'again', '.', '[SEP]']\n",
            "Original: ['Very', 'rude', 'and', 'unprofessional.']\n",
            "Tokenized: ['[CLS]', 'very', 'rude', 'and', 'un', '##pro', '##fe', '##ssion', '##al', '.', '[SEP]']\n",
            "Original: ['The', 'workers', 'sped', 'up', 'and', 'down', 'the', 'street', 'with', 'no', 'mind', 'to', 'the', 'small', 'children', 'playing.']\n",
            "Tokenized: ['[CLS]', 'the', 'workers', 'sped', 'up', 'and', 'down', 'the', 'street', 'with', 'no', 'mind', 'to', 'the', 'small', 'children', 'playing', '.', '[SEP]']\n",
            "Original: ['They', 'Suck']\n",
            "Tokenized: ['[CLS]', 'they', 'suck', '[SEP]']\n",
            "Original: ['Go', 'somewhere', 'else...']\n",
            "Tokenized: ['[CLS]', 'go', 'somewhere', 'else', '.', '.', '.', '[SEP]']\n",
            "Original: ['Wanted', 'to', 'buy', 'a', 'Rhino', '700', 'and', 'a', 'Grizzly', '700.']\n",
            "Tokenized: ['[CLS]', 'wanted', 'to', 'buy', 'a', 'rhino', '700', 'and', 'a', 'gr', '##izzly', '700', '.', '[SEP]']\n",
            "Original: ['After', 'searching', 'high', 'and', 'low', 'for', 'a', 'salesman,', 'I', 'was', 'treated', 'like', 'dirt,', 'and', 'we', 'left.']\n",
            "Tokenized: ['[CLS]', 'after', 'searching', 'high', 'and', 'low', 'for', 'a', 'salesman', ',', 'i', 'was', 'treated', 'like', 'dirt', ',', 'and', 'we', 'left', '.', '[SEP]']\n",
            "Original: ['Parts', 'department', 'blows,', 'Service', 'department', 'is', 'even', 'worse.']\n",
            "Tokenized: ['[CLS]', 'parts', 'department', 'blows', ',', 'service', 'department', 'is', 'even', 'worse', '.', '[SEP]']\n",
            "Original: ['I', 'live', '10', 'minutes', 'from', 'Cycle', 'City,', 'but', 'I', 'Drove', '50', 'mile', 'south', 'to', 'Peachstate', 'Powersports', 'in', 'LaGrange,', 'dealt', 'with', 'the', 'owner,', 'Levi,', 'and', 'was', 'well', 'taken', 'care', 'of.']\n",
            "Tokenized: ['[CLS]', 'i', 'live', '10', 'minutes', 'from', 'cycle', 'city', ',', 'but', 'i', 'drove', '50', 'mile', 'south', 'to', 'peach', '##sta', '##te', 'powers', '##ports', 'in', 'la', '##gra', '##nge', ',', 'dealt', 'with', 'the', 'owner', ',', 'levi', ',', 'and', 'was', 'well', 'taken', 'care', 'of', '.', '[SEP]']\n",
            "Original: ['Signs', 'of', 'Saltford', '-', 'an', 'excellent', 'supplier', 'of', 'value', 'for', 'money', 'signs', 'and', 'banners', 'etc.']\n",
            "Tokenized: ['[CLS]', 'signs', 'of', 'salt', '##ford', '-', 'an', 'excellent', 'supplier', 'of', 'value', 'for', 'money', 'signs', 'and', 'banners', 'etc', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'a', 'friend', 'and', 'customer', 'of', 'Signs', 'of', 'Saltford', 'for', 'well', 'over', '12', 'years', 'now', 'and', 'I', 'also', 'became', 'their', 'website', 'supplier', 'some', '3', 'years', 'ago.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'a', 'friend', 'and', 'customer', 'of', 'signs', 'of', 'salt', '##ford', 'for', 'well', 'over', '12', 'years', 'now', 'and', 'i', 'also', 'became', 'their', 'website', 'supplier', 'some', '3', 'years', 'ago', '.', '[SEP]']\n",
            "Original: ['Tina', 'is', 'the', 'driving', 'force', 'of', 'the', 'business', 'and', 'you', 'can', 'be', 'assured', 'that', 'she', 'will', 'endevour', 'to', 'satisfy', 'all', 'your', 'signage', 'requirements', 'at', 'the', 'most', 'cost', 'effective', 'rates.']\n",
            "Tokenized: ['[CLS]', 'tina', 'is', 'the', 'driving', 'force', 'of', 'the', 'business', 'and', 'you', 'can', 'be', 'assured', 'that', 'she', 'will', 'end', '##ev', '##our', 'to', 'satisfy', 'all', 'your', 'signage', 'requirements', 'at', 'the', 'most', 'cost', 'effective', 'rates', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'extremely', 'pleased', 'with', 'the', 'signs', 'and', 'pop-up', 'banners', 'she', 'has', 'supplied', 'to', 'me', 'over', 'the', 'years', '-', 'a', 'truly', 'first', 'class', 'family', 'business', 'run', 'by', 'Tina', 'and', 'her', 'husband', 'Chris.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'extremely', 'pleased', 'with', 'the', 'signs', 'and', 'pop', '-', 'up', 'banners', 'she', 'has', 'supplied', 'to', 'me', 'over', 'the', 'years', '-', 'a', 'truly', 'first', 'class', 'family', 'business', 'run', 'by', 'tina', 'and', 'her', 'husband', 'chris', '.', '[SEP]']\n",
            "Original: ['Great', 'service']\n",
            "Tokenized: ['[CLS]', 'great', 'service', '[SEP]']\n",
            "Original: ['We', 'at', 'R&L', 'Plumbing', 'Services', 'are', 'pleased', 'with', 'your', 'professionalism', 'and', 'the', 'extra', 'mile', 'you', 'went', 'to', 'get', 'out', 'computers', 'working', 'correctly,', 'you', 'will', 'be', 'our', 'first', 'call', 'if', 'anything', 'happens', 'again', 'and', 'we', 'will', 'refer', 'you', 'to', 'other', 'people', 'with', 'computer', 'issues.']\n",
            "Tokenized: ['[CLS]', 'we', 'at', 'r', '&', 'l', 'plumbing', 'services', 'are', 'pleased', 'with', 'your', 'professional', '##ism', 'and', 'the', 'extra', 'mile', 'you', 'went', 'to', 'get', 'out', 'computers', 'working', 'correctly', ',', 'you', 'will', 'be', 'our', 'first', 'call', 'if', 'anything', 'happens', 'again', 'and', 'we', 'will', 'refer', 'you', 'to', 'other', 'people', 'with', 'computer', 'issues', '.', '[SEP]']\n",
            "Original: ['Great', 'Job!']\n",
            "Tokenized: ['[CLS]', 'great', 'job', '!', '[SEP]']\n",
            "Original: ['Thanks', 'for', 'fixing', 'my', 'garage', 'door', 'A++++++++++++++++++']\n",
            "Tokenized: ['[CLS]', 'thanks', 'for', 'fixing', 'my', 'garage', 'door', 'a', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '[SEP]']\n",
            "Original: ['Dr.', 'Chao', 'you', 'are', 'the', 'best', 'dentist', 'I', 'have', 'ever', 'had.']\n",
            "Tokenized: ['[CLS]', 'dr', '.', 'chao', 'you', 'are', 'the', 'best', 'dentist', 'i', 'have', 'ever', 'had', '.', '[SEP]']\n",
            "Original: ['You', 'are', 'knowledgeable,', 'professional,', 'gentel', 'and', 'kind.']\n",
            "Tokenized: ['[CLS]', 'you', 'are', 'knowledge', '##able', ',', 'professional', ',', 'gen', '##tel', 'and', 'kind', '.', '[SEP]']\n",
            "Original: ['I', 'wish', 'I', 'had', 'you', 'as', 'my', 'dentist', 'early', 'on', 'in', 'my', 'life', '-', 'maybe', 'my', 'teeth', 'would', 'have', 'been', 'a', 'lot', 'better', 'then', 'they', 'are', 'now,', 'However', 'I', 'am', 'glad', 'you', 'are', 'my', 'dentist', 'now.']\n",
            "Tokenized: ['[CLS]', 'i', 'wish', 'i', 'had', 'you', 'as', 'my', 'dentist', 'early', 'on', 'in', 'my', 'life', '-', 'maybe', 'my', 'teeth', 'would', 'have', 'been', 'a', 'lot', 'better', 'then', 'they', 'are', 'now', ',', 'however', 'i', 'am', 'glad', 'you', 'are', 'my', 'dentist', 'now', '.', '[SEP]']\n",
            "Original: ['Even', 'though', 'you', 'are', 'expensive.']\n",
            "Tokenized: ['[CLS]', 'even', 'though', 'you', 'are', 'expensive', '.', '[SEP]']\n",
            "Original: ['Thank', 'you', 'for', 'helping', 'to', 'preserve', 'my', 'teeth.']\n",
            "Tokenized: ['[CLS]', 'thank', 'you', 'for', 'helping', 'to', 'preserve', 'my', 'teeth', '.', '[SEP]']\n",
            "Original: ['You', 'are', 'meticulous', 'in', 'your', 'work', 'and', 'it', 'shows', 'in', 'my', 'smile.']\n",
            "Tokenized: ['[CLS]', 'you', 'are', 'met', '##ic', '##ulous', 'in', 'your', 'work', 'and', 'it', 'shows', 'in', 'my', 'smile', '.', '[SEP]']\n",
            "Original: ['Smokers', 'Haven']\n",
            "Tokenized: ['[CLS]', 'smoke', '##rs', 'haven', '[SEP]']\n",
            "Original: ['Yeah,', 'this', 'complex', 'is', 'not', 'very', 'good.']\n",
            "Tokenized: ['[CLS]', 'yeah', ',', 'this', 'complex', 'is', 'not', 'very', 'good', '.', '[SEP]']\n",
            "Original: ['Our', 'bathroom', 'fan,', 'one', 'electric', 'outlet', 'and', '2', 'leaky', 'sinks', 'have', 'yet', 'to', 'be', 'fixed.']\n",
            "Tokenized: ['[CLS]', 'our', 'bathroom', 'fan', ',', 'one', 'electric', 'outlet', 'and', '2', 'leak', '##y', 'sinks', 'have', 'yet', 'to', 'be', 'fixed', '.', '[SEP]']\n",
            "Original: ['Both', 'bathrooms', 'look', 'like', 'they', 'were', 'flooded', 'and', 'the', 'wood', 'cabinets', 'are', 'thrashed', 'at', 'the', 'bottom', 'and', 'they', 'slapped', 'some', 'pieces', 'of', 'wood', 'over', 'to', 'try', 'to', 'cover', 'it', 'up.']\n",
            "Tokenized: ['[CLS]', 'both', 'bathrooms', 'look', 'like', 'they', 'were', 'flooded', 'and', 'the', 'wood', 'cabinets', 'are', 'thrash', '##ed', 'at', 'the', 'bottom', 'and', 'they', 'slapped', 'some', 'pieces', 'of', 'wood', 'over', 'to', 'try', 'to', 'cover', 'it', 'up', '.', '[SEP]']\n",
            "Original: ['And', 'non-smokers', 'beware!!!']\n",
            "Tokenized: ['[CLS]', 'and', 'non', '-', 'smoke', '##rs', 'be', '##ware', '!', '!', '!', '[SEP]']\n",
            "Original: ['I', 'think', '90', 'percent', 'of', 'the', 'tenants', 'are', 'smokers!']\n",
            "Tokenized: ['[CLS]', 'i', 'think', '90', 'percent', 'of', 'the', 'tenants', 'are', 'smoke', '##rs', '!', '[SEP]']\n",
            "Original: ['If', 'you', 'do', 'not', 'smoke,', 'do', 'not', 'move', 'here.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'do', 'not', 'smoke', ',', 'do', 'not', 'move', 'here', '.', '[SEP]']\n",
            "Original: ['Our', 'unit', 'reeks', 'of', 'old', 'cigarette', 'smoke', 'and', 'it', 'started', 'to', 'become', 'apparent', 'a', 'few', 'weeks', 'after', 'we', 'moved', 'in.']\n",
            "Tokenized: ['[CLS]', 'our', 'unit', 're', '##ek', '##s', 'of', 'old', 'cigarette', 'smoke', 'and', 'it', 'started', 'to', 'become', 'apparent', 'a', 'few', 'weeks', 'after', 'we', 'moved', 'in', '.', '[SEP]']\n",
            "Original: ['You', 'cannot', 'walk', '5', 'feet', 'without', 'smelling', 'that', 'disgusting', 'cigarette', 'smoke', 'and', 'it', 'blows', 'right', 'into', 'the', 'windows', 'all', 'day', 'and', 'all', 'night.']\n",
            "Tokenized: ['[CLS]', 'you', 'cannot', 'walk', '5', 'feet', 'without', 'smelling', 'that', 'disgusting', 'cigarette', 'smoke', 'and', 'it', 'blows', 'right', 'into', 'the', 'windows', 'all', 'day', 'and', 'all', 'night', '.', '[SEP]']\n",
            "Original: ['Great', 'Neighborhood', 'Hangout']\n",
            "Tokenized: ['[CLS]', 'great', 'neighborhood', 'hang', '##out', '[SEP]']\n",
            "Original: ['Great', 'place', 'to', 'catch', 'a', 'band', 'or', 'catch', 'up', 'with', 'friends.']\n",
            "Tokenized: ['[CLS]', 'great', 'place', 'to', 'catch', 'a', 'band', 'or', 'catch', 'up', 'with', 'friends', '.', '[SEP]']\n",
            "Original: ['Ice', 'cold', 'beer', 'and', 'good', 'prices.']\n",
            "Tokenized: ['[CLS]', 'ice', 'cold', 'beer', 'and', 'good', 'prices', '.', '[SEP]']\n",
            "Original: ['Kitchen', 'puts', 'out', 'good', 'food', 'and', 'has', 'daily', 'specials.']\n",
            "Tokenized: ['[CLS]', 'kitchen', 'puts', 'out', 'good', 'food', 'and', 'has', 'daily', 'specials', '.', '[SEP]']\n",
            "Original: ['We', 'just', 'got', 'our', 'sunroom', 'built', 'by', 'Patio', 'World', 'and', 'can', 'say', 'that', \"I'm\", 'extremely', 'happy', 'with', 'the', 'whole', 'thing.']\n",
            "Tokenized: ['[CLS]', 'we', 'just', 'got', 'our', 'sun', '##room', 'built', 'by', 'patio', 'world', 'and', 'can', 'say', 'that', 'i', \"'\", 'm', 'extremely', 'happy', 'with', 'the', 'whole', 'thing', '.', '[SEP]']\n",
            "Original: ['From', 'the', 'amount', 'of', 'time', 'spent', 'with', 'us', 'to', 'explain', 'things', 'during', 'the', 'initial', 'quote,', 'to', 'the', 'communication', 'through', 'the', 'approval', 'process', 'to', 'the', 'actual', 'workmanship', 'of', 'the', 'build', 'itself.']\n",
            "Tokenized: ['[CLS]', 'from', 'the', 'amount', 'of', 'time', 'spent', 'with', 'us', 'to', 'explain', 'things', 'during', 'the', 'initial', 'quote', ',', 'to', 'the', 'communication', 'through', 'the', 'approval', 'process', 'to', 'the', 'actual', 'work', '##manship', 'of', 'the', 'build', 'itself', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'nothing', 'bad', 'to', 'say.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'nothing', 'bad', 'to', 'say', '.', '[SEP]']\n",
            "Original: ['Very', 'glad', 'that', 'we', 'went', 'with', 'them.']\n",
            "Tokenized: ['[CLS]', 'very', 'glad', 'that', 'we', 'went', 'with', 'them', '.', '[SEP]']\n",
            "Original: ['professional']\n",
            "Tokenized: ['[CLS]', 'professional', '[SEP]']\n",
            "Original: ['Good', 'job', 'very', 'professional.']\n",
            "Tokenized: ['[CLS]', 'good', 'job', 'very', 'professional', '.', '[SEP]']\n",
            "Original: ['It', 'made', 'me', 'feel', 'good', 'to', 'see', 'people', 'work', 'so', 'hard', 'to', 'take', 'care', 'of', 'others', 'belongings.']\n",
            "Tokenized: ['[CLS]', 'it', 'made', 'me', 'feel', 'good', 'to', 'see', 'people', 'work', 'so', 'hard', 'to', 'take', 'care', 'of', 'others', 'belongings', '.', '[SEP]']\n",
            "Original: ['The', 'New', 'Italian', 'Kid', 'on', 'the', 'Block']\n",
            "Tokenized: ['[CLS]', 'the', 'new', 'italian', 'kid', 'on', 'the', 'block', '[SEP]']\n",
            "Original: ['Another', 'Italian', 'restaurant', 'in', 'Collingswood?']\n",
            "Tokenized: ['[CLS]', 'another', 'italian', 'restaurant', 'in', 'collin', '##gs', '##wood', '?', '[SEP]']\n",
            "Original: ['Do', 'we', 'need', 'another', 'one?']\n",
            "Tokenized: ['[CLS]', 'do', 'we', 'need', 'another', 'one', '?', '[SEP]']\n",
            "Original: ['Only', 'if', 'it', 'is', 'of', 'the', 'quality', 'of', \"That's\", 'Amore.']\n",
            "Tokenized: ['[CLS]', 'only', 'if', 'it', 'is', 'of', 'the', 'quality', 'of', 'that', \"'\", 's', 'amore', '.', '[SEP]']\n",
            "Original: ['The', 'menu', 'has', 'the', 'usual', 'but', 'then', 'they', 'step', 'it', 'up', 'another', 'notch.']\n",
            "Tokenized: ['[CLS]', 'the', 'menu', 'has', 'the', 'usual', 'but', 'then', 'they', 'step', 'it', 'up', 'another', 'notch', '.', '[SEP]']\n",
            "Original: ['The', 'arancini', 'di', 'riso', '(risotto', 'fritters)', 'are', 'not', 'to', 'be', 'missed.']\n",
            "Tokenized: ['[CLS]', 'the', 'ara', '##nc', '##ini', 'di', 'ri', '##so', '(', 'ri', '##so', '##tto', 'fr', '##itte', '##rs', ')', 'are', 'not', 'to', 'be', 'missed', '.', '[SEP]']\n",
            "Original: ['Chicken', 'saltimboca', 'was', 'excellent', 'and', 'then', \"there's\", 'the', 'chocolate', 'mousse', 'that', 'comes', 'straight', 'from', 'heaven.']\n",
            "Tokenized: ['[CLS]', 'chicken', 'salt', '##im', '##bo', '##ca', 'was', 'excellent', 'and', 'then', 'there', \"'\", 's', 'the', 'chocolate', 'mo', '##uss', '##e', 'that', 'comes', 'straight', 'from', 'heaven', '.', '[SEP]']\n",
            "Original: ['Pay', 'extra', 'attention', 'to', 'the', 'appetizers', '-', 'the', 'next', 'time', 'I', 'go', 'there', \"I'm\", 'planning', 'on', 'ordered', 'a', 'few', 'instead', 'of', 'an', 'entree.']\n",
            "Tokenized: ['[CLS]', 'pay', 'extra', 'attention', 'to', 'the', 'app', '##eti', '##zers', '-', 'the', 'next', 'time', 'i', 'go', 'there', 'i', \"'\", 'm', 'planning', 'on', 'ordered', 'a', 'few', 'instead', 'of', 'an', 'en', '##tree', '.', '[SEP]']\n",
            "Original: ['Liars,', 'negative', 'stars!']\n",
            "Tokenized: ['[CLS]', 'liar', '##s', ',', 'negative', 'stars', '!', '[SEP]']\n",
            "Original: ['Took', 'my', 'Cruze', 'in', 'twice', 'for', 'poor', 'fuel', 'economy.']\n",
            "Tokenized: ['[CLS]', 'took', 'my', 'cruz', '##e', 'in', 'twice', 'for', 'poor', 'fuel', 'economy', '.', '[SEP]']\n",
            "Original: ['The', 'first', 'time', 'they', 'claimed', 'to', 'get', 'reasonable', 'mpg.']\n",
            "Tokenized: ['[CLS]', 'the', 'first', 'time', 'they', 'claimed', 'to', 'get', 'reasonable', 'mp', '##g', '.', '[SEP]']\n",
            "Original: ['However,', 'they', 'would', 'never', 'drive', 'the', 'car', 'with', 'me', 'in', 'it', 'to', 'prove', 'their', 'findings.']\n",
            "Tokenized: ['[CLS]', 'however', ',', 'they', 'would', 'never', 'drive', 'the', 'car', 'with', 'me', 'in', 'it', 'to', 'prove', 'their', 'findings', '.', '[SEP]']\n",
            "Original: ['I', 'wonder', 'if', 'they', 'were', 'going', 'down', 'a', 'hill!']\n",
            "Tokenized: ['[CLS]', 'i', 'wonder', 'if', 'they', 'were', 'going', 'down', 'a', 'hill', '!', '[SEP]']\n",
            "Original: ['They', 'told', 'me', 'to', 'bring', 'it', 'back', 'after', '5000', 'miles.']\n",
            "Tokenized: ['[CLS]', 'they', 'told', 'me', 'to', 'bring', 'it', 'back', 'after', '5000', 'miles', '.', '[SEP]']\n",
            "Original: ['I', 'brought', 'it', 'back', 'with', '9000', 'miles.']\n",
            "Tokenized: ['[CLS]', 'i', 'brought', 'it', 'back', 'with', '900', '##0', 'miles', '.', '[SEP]']\n",
            "Original: ['They', '\"finished\"', 'the', 'work', 'and', 'told', 'me', 'the', 'car', 'was', 'ready.']\n",
            "Tokenized: ['[CLS]', 'they', '\"', 'finished', '\"', 'the', 'work', 'and', 'told', 'me', 'the', 'car', 'was', 'ready', '.', '[SEP]']\n",
            "Original: ['I', 'found', 'out', 'they', 'did', 'not', 'even', 'drive', 'the', 'car,', 'stated', 'they', 'looked', 'at', 'it', 'before.']\n",
            "Tokenized: ['[CLS]', 'i', 'found', 'out', 'they', 'did', 'not', 'even', 'drive', 'the', 'car', ',', 'stated', 'they', 'looked', 'at', 'it', 'before', '.', '[SEP]']\n",
            "Original: ['They', 'still', 'would', 'not', 'drive', 'the', 'car', 'with', 'me', 'to', 'show', 'their', 'mpg', 'number.']\n",
            "Tokenized: ['[CLS]', 'they', 'still', 'would', 'not', 'drive', 'the', 'car', 'with', 'me', 'to', 'show', 'their', 'mp', '##g', 'number', '.', '[SEP]']\n",
            "Original: ['They', 'also', 'claimed', 'not', 'to', 'see', 'anything', 'wrong', 'with', 'the', 'blower', 'fan', '(a', 'seperate', 'issue),', 'but', 'when', 'I', 'drove', 'the', 'car', 'home', 'I', 'had', 'the', 'same', 'symptoms.']\n",
            "Tokenized: ['[CLS]', 'they', 'also', 'claimed', 'not', 'to', 'see', 'anything', 'wrong', 'with', 'the', 'blow', '##er', 'fan', '(', 'a', 'sep', '##erate', 'issue', ')', ',', 'but', 'when', 'i', 'drove', 'the', 'car', 'home', 'i', 'had', 'the', 'same', 'symptoms', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'never', 'purchace', 'another', 'vehicle', 'from', 'Vic', 'Canever.']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'never', 'pu', '##rch', '##ace', 'another', 'vehicle', 'from', 'vic', 'cane', '##ver', '.', '[SEP]']\n",
            "Original: ['Your', 'average', 'crappy', 'chain.']\n",
            "Tokenized: ['[CLS]', 'your', 'average', 'crap', '##py', 'chain', '.', '[SEP]']\n",
            "Original: ['Food', 'is', 'awful', 'and', 'the', 'place', 'caters', 'to', 'the', 'yuppy', 'crowd.']\n",
            "Tokenized: ['[CLS]', 'food', 'is', 'awful', 'and', 'the', 'place', 'cater', '##s', 'to', 'the', 'yu', '##ppy', 'crowd', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'a', 'great', 'place', 'to', 'get', 'a', 'permit']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'a', 'great', 'place', 'to', 'get', 'a', 'permit', '[SEP]']\n",
            "Original: ['I', 'had', 'to', 'get', 'a', 'permit', 'here,', 'it', 'was', 'cool']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'to', 'get', 'a', 'permit', 'here', ',', 'it', 'was', 'cool', '[SEP]']\n",
            "Original: ['These', 'guys', 'do', 'great', 'work', 'at', 'VERY', 'reasonable', 'prices.']\n",
            "Tokenized: ['[CLS]', 'these', 'guys', 'do', 'great', 'work', 'at', 'very', 'reasonable', 'prices', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'use', 'them', 'four', 'times', 'for', 'fixing', 'items', 'from', 'pushing', 'out', 'a', 'dent', 'in', 'a', 'bumper', 'to', 'fixing', 'the', 'fender', 'on', 'my', 'beloved', 'Miata.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'use', 'them', 'four', 'times', 'for', 'fixing', 'items', 'from', 'pushing', 'out', 'a', 'dent', 'in', 'a', 'bumper', 'to', 'fixing', 'the', 'fender', 'on', 'my', 'beloved', 'mia', '##ta', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'never', 'been', 'disappointed.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'never', 'been', 'disappointed', '.', '[SEP]']\n",
            "Original: ['Great', 'service', 'and', 'awesome', 'prices.']\n",
            "Tokenized: ['[CLS]', 'great', 'service', 'and', 'awesome', 'prices', '.', '[SEP]']\n",
            "Original: ['I', 'get', 'Microdermabrasions', 'regularly', 'and', 'I', 'love', 'the', 'environment']\n",
            "Tokenized: ['[CLS]', 'i', 'get', 'micro', '##der', '##ma', '##bra', '##sions', 'regularly', 'and', 'i', 'love', 'the', 'environment', '[SEP]']\n",
            "Original: ['Hospitality.!']\n",
            "Tokenized: ['[CLS]', 'hospitality', '.', '!', '[SEP]']\n",
            "Original: ['Very', 'good', 'hospitality', 'offered.!']\n",
            "Tokenized: ['[CLS]', 'very', 'good', 'hospitality', 'offered', '.', '!', '[SEP]']\n",
            "Original: ['Keep', 'it', 'up.']\n",
            "Tokenized: ['[CLS]', 'keep', 'it', 'up', '.', '[SEP]']\n",
            "Original: ['-', 'Shree', 'Ghatkopar', 'Bhatia', 'Mitra', 'Mandal']\n",
            "Tokenized: ['[CLS]', '-', 'sh', '##ree', 'g', '##hat', '##ko', '##par', 'b', '##hat', '##ia', 'mit', '##ra', 'mandal', '[SEP]']\n",
            "Original: ['Pleasure', 'to', 'work', 'with.']\n",
            "Tokenized: ['[CLS]', 'pleasure', 'to', 'work', 'with', '.', '[SEP]']\n",
            "Original: ['The', 'experience', 'with', 'every', 'department', 'has', 'been', 'great.']\n",
            "Tokenized: ['[CLS]', 'the', 'experience', 'with', 'every', 'department', 'has', 'been', 'great', '.', '[SEP]']\n",
            "Original: ['No', 'complaints!']\n",
            "Tokenized: ['[CLS]', 'no', 'complaints', '!', '[SEP]']\n",
            "Original: ['If', 'you', 'want', 'cheaply', 'made', 'glass', 'from', 'India', 'and', 'China,', 'this', 'is', 'not', 'your', 'place.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'want', 'cheap', '##ly', 'made', 'glass', 'from', 'india', 'and', 'china', ',', 'this', 'is', 'not', 'your', 'place', '.', '[SEP]']\n",
            "Original: ['These', 'people', 'only', 'carry', 'the', 'very', 'best', 'American', 'blown', 'glass.']\n",
            "Tokenized: ['[CLS]', 'these', 'people', 'only', 'carry', 'the', 'very', 'best', 'american', 'blown', 'glass', '.', '[SEP]']\n",
            "Original: ['Their', 'selection', 'is', 'top', 'notch', 'and', 'the', 'staff', 'is', 'very', 'knowledgable.']\n",
            "Tokenized: ['[CLS]', 'their', 'selection', 'is', 'top', 'notch', 'and', 'the', 'staff', 'is', 'very', 'know', '##led', '##ga', '##ble', '.', '[SEP]']\n",
            "Original: ['Go', 'to', 'the', 'Looking', 'Glass', 'for', 'all', 'your', 'smoking', 'needs!!!!!!!!!!']\n",
            "Tokenized: ['[CLS]', 'go', 'to', 'the', 'looking', 'glass', 'for', 'all', 'your', 'smoking', 'needs', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['5', 'star', 'detail', 'job']\n",
            "Tokenized: ['[CLS]', '5', 'star', 'detail', 'job', '[SEP]']\n",
            "Original: ['I', 'took', 'my', 'Mustang', 'here', 'and', 'it', 'looked', 'amazing', 'after', 'they', 'were', 'done,', 'they', 'did', 'a', 'great', 'job,', \"I'm\", 'very', 'satisfied', 'with', 'the', 'results.']\n",
            "Tokenized: ['[CLS]', 'i', 'took', 'my', 'mustang', 'here', 'and', 'it', 'looked', 'amazing', 'after', 'they', 'were', 'done', ',', 'they', 'did', 'a', 'great', 'job', ',', 'i', \"'\", 'm', 'very', 'satisfied', 'with', 'the', 'results', '.', '[SEP]']\n",
            "Original: ['The', 'paint', 'and', 'wheels', 'looked', 'like', 'glass', 'and', 'the', 'interior', 'looked', 'new!']\n",
            "Tokenized: ['[CLS]', 'the', 'paint', 'and', 'wheels', 'looked', 'like', 'glass', 'and', 'the', 'interior', 'looked', 'new', '!', '[SEP]']\n",
            "Original: ['Also,', 'they', 'have', 'great', 'customer', 'service', 'and', 'a', 'very', 'knowledgeable', 'staff']\n",
            "Tokenized: ['[CLS]', 'also', ',', 'they', 'have', 'great', 'customer', 'service', 'and', 'a', 'very', 'knowledge', '##able', 'staff', '[SEP]']\n",
            "Original: ['Good', 'location']\n",
            "Tokenized: ['[CLS]', 'good', 'location', '[SEP]']\n",
            "Original: ['For', 'a', 'hotel', 'like', 'this', 'you', 'would', 'expect', 'some', 'form', 'of', 'free', 'internet.']\n",
            "Tokenized: ['[CLS]', 'for', 'a', 'hotel', 'like', 'this', 'you', 'would', 'expect', 'some', 'form', 'of', 'free', 'internet', '.', '[SEP]']\n",
            "Original: ['What', 'you', 'get', 'is', 'a', '$13/day', 'charge', 'to', 'access', 'the', 'internet', 'through', 'a', 'slow', '512/512kb/s', 'that', 'you', 'can', 'only', 'use', 'to', 'check', 'email', 'etc', '(Read:', 'No', 'downloading).']\n",
            "Tokenized: ['[CLS]', 'what', 'you', 'get', 'is', 'a', '$', '13', '/', 'day', 'charge', 'to', 'access', 'the', 'internet', 'through', 'a', 'slow', '512', '/', '512', '##k', '##b', '/', 's', 'that', 'you', 'can', 'only', 'use', 'to', 'check', 'email', 'etc', '(', 'read', ':', 'no', 'download', '##ing', ')', '.', '[SEP]']\n",
            "Original: ['Other', 'than', 'that', 'the', 'hotel', 'is', 'in', 'a', 'good', 'location', 'and', 'the', 'breakfast', 'is', 'great']\n",
            "Tokenized: ['[CLS]', 'other', 'than', 'that', 'the', 'hotel', 'is', 'in', 'a', 'good', 'location', 'and', 'the', 'breakfast', 'is', 'great', '[SEP]']\n",
            "Original: ['Fantastic', 'Service!']\n",
            "Tokenized: ['[CLS]', 'fantastic', 'service', '!', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'here', 'a', 'few', 'times', 'for', 'oil', 'changes', 'and', 'just', 'got', 'my', 'tires,', 'alignment', 'and', 'state', 'inspection', 'done', 'yesterday.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'here', 'a', 'few', 'times', 'for', 'oil', 'changes', 'and', 'just', 'got', 'my', 'tires', ',', 'alignment', 'and', 'state', 'inspection', 'done', 'yesterday', '.', '[SEP]']\n",
            "Original: ['I', 'shopped', 'it', 'around', 'and', 'they', 'were', 'extremely', 'competitive', 'in', 'pricing', 'and', 'also', 'added', 'nitrogen', 'to', 'my', 'tires', 'which', 'should', 'extend', 'the', 'life', 'and', 'get', 'me', 'better', 'gas', 'mileage.']\n",
            "Tokenized: ['[CLS]', 'i', 'shop', '##ped', 'it', 'around', 'and', 'they', 'were', 'extremely', 'competitive', 'in', 'pricing', 'and', 'also', 'added', 'nitrogen', 'to', 'my', 'tires', 'which', 'should', 'extend', 'the', 'life', 'and', 'get', 'me', 'better', 'gas', 'mile', '##age', '.', '[SEP]']\n",
            "Original: ['It', 'also', 'came', 'with', 'free', 'balance', 'and', 'rotation', 'for', 'the', 'life', 'of', 'the', 'tires!']\n",
            "Tokenized: ['[CLS]', 'it', 'also', 'came', 'with', 'free', 'balance', 'and', 'rotation', 'for', 'the', 'life', 'of', 'the', 'tires', '!', '[SEP]']\n",
            "Original: ['What', 'made', 'it', 'perfect', 'was', 'that', 'they', 'offered', 'transportation', 'so', 'that', 'I', 'would', 'not', 'have', 'to', 'wait', 'there', 'or', 'take', 'time', 'off', 'of', 'work', 'to', 'go', 'back', 'and', 'forth', 'or', 'try', 'to', 'find', 'a', 'ride.']\n",
            "Tokenized: ['[CLS]', 'what', 'made', 'it', 'perfect', 'was', 'that', 'they', 'offered', 'transportation', 'so', 'that', 'i', 'would', 'not', 'have', 'to', 'wait', 'there', 'or', 'take', 'time', 'off', 'of', 'work', 'to', 'go', 'back', 'and', 'forth', 'or', 'try', 'to', 'find', 'a', 'ride', '.', '[SEP]']\n",
            "Original: ['Great', 'service,', 'great', 'pricing', 'and', 'SUPER', 'CONVENIENT!']\n",
            "Tokenized: ['[CLS]', 'great', 'service', ',', 'great', 'pricing', 'and', 'super', 'convenient', '!', '[SEP]']\n",
            "Original: ['I', 'also', 'did', 'not', 'feel', 'like', 'they', 'tried', 'to', 'sell', 'me', 'a', 'bunch', 'of', 'services', 'that', 'I', 'did', 'not', 'need.']\n",
            "Tokenized: ['[CLS]', 'i', 'also', 'did', 'not', 'feel', 'like', 'they', 'tried', 'to', 'sell', 'me', 'a', 'bunch', 'of', 'services', 'that', 'i', 'did', 'not', 'need', '.', '[SEP]']\n",
            "Original: ['spoken', 'english', 'rushi']\n",
            "Tokenized: ['[CLS]', 'spoken', 'english', 'rush', '##i', '[SEP]']\n",
            "Original: ['can', 'ever', '&', 'never', 'forget', 'the', 'training', 'undergone', 'here', 'which', 'made', 'my', 'life', 'step', 'onto', 'the', 'successful', 'job', 'without', 'any', 'hurdles.']\n",
            "Tokenized: ['[CLS]', 'can', 'ever', '&', 'never', 'forget', 'the', 'training', 'undergone', 'here', 'which', 'made', 'my', 'life', 'step', 'onto', 'the', 'successful', 'job', 'without', 'any', 'hurdles', '.', '[SEP]']\n",
            "Original: ['The', 'staff,', 'material', 'provided,', 'infra', 'structure,', 'environment', '&low', 'fees', 'totally', 'above', 'the', 'satisfactory', 'mark']\n",
            "Tokenized: ['[CLS]', 'the', 'staff', ',', 'material', 'provided', ',', 'in', '##fra', 'structure', ',', 'environment', '&', 'low', 'fees', 'totally', 'above', 'the', 'satisfactory', 'mark', '[SEP]']\n",
            "Original: ['Amazing', 'service!']\n",
            "Tokenized: ['[CLS]', 'amazing', 'service', '!', '[SEP]']\n",
            "Original: ['I', 'just', 'had', 'the', 'best', 'experience', 'at', 'this', 'Kal', 'Tire', 'location.']\n",
            "Tokenized: ['[CLS]', 'i', 'just', 'had', 'the', 'best', 'experience', 'at', 'this', 'ka', '##l', 'tire', 'location', '.', '[SEP]']\n",
            "Original: ['Courteous,', 'fast', 'and', 'friendly.']\n",
            "Tokenized: ['[CLS]', 'court', '##eous', ',', 'fast', 'and', 'friendly', '.', '[SEP]']\n",
            "Original: ['Rip', 'Off!']\n",
            "Tokenized: ['[CLS]', 'rip', 'off', '!', '[SEP]']\n",
            "Original: ['45p', 'for', 'tap', 'water!']\n",
            "Tokenized: ['[CLS]', '45', '##p', 'for', 'tap', 'water', '!', '[SEP]']\n",
            "Original: ['Ridiculous!']\n",
            "Tokenized: ['[CLS]', 'ridiculous', '!', '[SEP]']\n",
            "Original: [\"Don't\", 'think', \"I've\", 'ever', 'been', 'charged', 'before.']\n",
            "Tokenized: ['[CLS]', 'don', \"'\", 't', 'think', 'i', \"'\", 've', 'ever', 'been', 'charged', 'before', '.', '[SEP]']\n",
            "Original: ['Oh,', 'and', 'salad', 'cream,', 'not', 'mayonnaise,', 'on', 'the', 'coleslaw.']\n",
            "Tokenized: ['[CLS]', 'oh', ',', 'and', 'salad', 'cream', ',', 'not', 'mayo', '##nna', '##ise', ',', 'on', 'the', 'cole', '##sl', '##aw', '.', '[SEP]']\n",
            "Original: ['Avoid!']\n",
            "Tokenized: ['[CLS]', 'avoid', '!', '[SEP]']\n",
            "Original: ['Great', 'lunch', 'specials.']\n",
            "Tokenized: ['[CLS]', 'great', 'lunch', 'specials', '.', '[SEP]']\n",
            "Original: ['Delivery', 'is', 'lightning', 'fast.']\n",
            "Tokenized: ['[CLS]', 'delivery', 'is', 'lightning', 'fast', '.', '[SEP]']\n",
            "Original: ['Perfect', 'since', \"I'm\", 'on', 'a', 'budget.']\n",
            "Tokenized: ['[CLS]', 'perfect', 'since', 'i', \"'\", 'm', 'on', 'a', 'budget', '.', '[SEP]']\n",
            "Original: ['But', 'otherwise,', 'it', 'can', 'feel', 'pricey', 'for', 'what', 'you', 'get.']\n",
            "Tokenized: ['[CLS]', 'but', 'otherwise', ',', 'it', 'can', 'feel', 'price', '##y', 'for', 'what', 'you', 'get', '.', '[SEP]']\n",
            "Original: ['Like', 'the', 'sushi,', \"don't\", 'like', 'the', 'pad', 'thai.']\n",
            "Tokenized: ['[CLS]', 'like', 'the', 'su', '##shi', ',', 'don', \"'\", 't', 'like', 'the', 'pad', 'thai', '.', '[SEP]']\n",
            "Original: ['Best', 'place', 'to', 'sleep!!!!!!!']\n",
            "Tokenized: ['[CLS]', 'best', 'place', 'to', 'sleep', '!', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['Good', 'fun', 'for', 'wing', 'night,', 'food', 'eh,', 'beer', 'list', 'eh...']\n",
            "Tokenized: ['[CLS]', 'good', 'fun', 'for', 'wing', 'night', ',', 'food', 'eh', ',', 'beer', 'list', 'eh', '.', '.', '.', '[SEP]']\n",
            "Original: ['Dr.', 'Strzalka', 'at', 'Flagship', 'CVTS', 'is', 'not', 'a', 'good', 'doctor']\n",
            "Tokenized: ['[CLS]', 'dr', '.', 'st', '##rza', '##lka', 'at', 'flagship', 'cv', '##ts', 'is', 'not', 'a', 'good', 'doctor', '[SEP]']\n",
            "Original: ['I', 'am', 'not', 'sure', 'about', 'the', 'quality', 'of', 'the', 'other', 'doctors', 'there,', 'but', 'i', 'do', 'know', 'from', 'personal', 'experience', 'that', 'Dr.', 'Christopher', 'T.', 'Strzalka', 'is', 'not', 'a', 'man', 'of', 'his', 'word,', 'and', 'is', 'also', 'very', 'CRUEL', 'AND', 'UNCARING!!']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'not', 'sure', 'about', 'the', 'quality', 'of', 'the', 'other', 'doctors', 'there', ',', 'but', 'i', 'do', 'know', 'from', 'personal', 'experience', 'that', 'dr', '.', 'christopher', 't', '.', 'st', '##rza', '##lka', 'is', 'not', 'a', 'man', 'of', 'his', 'word', ',', 'and', 'is', 'also', 'very', 'cruel', 'and', 'un', '##car', '##ing', '!', '!', '[SEP]']\n",
            "Original: ['He', 'was', 'going', 'to', 'operate', 'and', 'replace', 'my', 'bicuspid', 'aortic', 'valve', 'due', 'to', 'critical', 'aortic', 'stenosis.']\n",
            "Tokenized: ['[CLS]', 'he', 'was', 'going', 'to', 'operate', 'and', 'replace', 'my', 'bi', '##cus', '##pid', 'ao', '##rti', '##c', 'valve', 'due', 'to', 'critical', 'ao', '##rti', '##c', 'ste', '##nosis', '.', '[SEP]']\n",
            "Original: ['I', 'had', 'a', 'surgery', 'date', 'of', 'July', '17,', '2008.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'a', 'surgery', 'date', 'of', 'july', '17', ',', '2008', '.', '[SEP]']\n",
            "Original: ['Then', 'he', 'renigged', 'when', 'he', 'read', 'my', 'Health', 'Care', 'Proxy,', 'even', 'though', 'i', 'agreed', 'to', 'be', 'on', 'the', 'ventilator', 'for', '2', 'months', 'following', 'surgery', '(as', 'he', 'had', 'twice', 'stated', 'i', 'must', 'agree', 'to).']\n",
            "Tokenized: ['[CLS]', 'then', 'he', 'ren', '##ig', '##ged', 'when', 'he', 'read', 'my', 'health', 'care', 'proxy', ',', 'even', 'though', 'i', 'agreed', 'to', 'be', 'on', 'the', 'vent', '##ila', '##tor', 'for', '2', 'months', 'following', 'surgery', '(', 'as', 'he', 'had', 'twice', 'stated', 'i', 'must', 'agree', 'to', ')', '.', '[SEP]']\n",
            "Original: ['He', 'later', 'said', 'that', 'by', '2', 'months', 'he', '\"meant', 'at', 'least', 'two', 'months\".']\n",
            "Tokenized: ['[CLS]', 'he', 'later', 'said', 'that', 'by', '2', 'months', 'he', '\"', 'meant', 'at', 'least', 'two', 'months', '\"', '.', '[SEP]']\n",
            "Original: ['Two', 'months', 'and', 'at', 'least', 'two', 'months', 'are', 'totally', 'different', 'things.']\n",
            "Tokenized: ['[CLS]', 'two', 'months', 'and', 'at', 'least', 'two', 'months', 'are', 'totally', 'different', 'things', '.', '[SEP]']\n",
            "Original: ['He', 'did', 'not', 'even', 'give', 'me', 'the', 'chance', 'to', 'say', 'i', 'would', 'stay', 'on', 'the', 'ventilator', 'longer,', 'which', 'i', 'would', 'have.']\n",
            "Tokenized: ['[CLS]', 'he', 'did', 'not', 'even', 'give', 'me', 'the', 'chance', 'to', 'say', 'i', 'would', 'stay', 'on', 'the', 'vent', '##ila', '##tor', 'longer', ',', 'which', 'i', 'would', 'have', '.', '[SEP]']\n",
            "Original: ['A', 'Health', 'Care', 'Proxy', 'is', 'not', 'written', 'in', 'stone', 'and', 'can', 'be', 'changed.']\n",
            "Tokenized: ['[CLS]', 'a', 'health', 'care', 'proxy', 'is', 'not', 'written', 'in', 'stone', 'and', 'can', 'be', 'changed', '.', '[SEP]']\n",
            "Original: ['He', 'also', 'never', 'even', 'said', 'he', 'was', 'sorry.']\n",
            "Tokenized: ['[CLS]', 'he', 'also', 'never', 'even', 'said', 'he', 'was', 'sorry', '.', '[SEP]']\n",
            "Original: ['Just', 'said', 'i', 'was', 'inoperable', 'and', 'walked', 'out', 'of', 'the', 'hospital', 'room.']\n",
            "Tokenized: ['[CLS]', 'just', 'said', 'i', 'was', 'in', '##oper', '##able', 'and', 'walked', 'out', 'of', 'the', 'hospital', 'room', '.', '[SEP]']\n",
            "Original: ['So,', 'therefore,', 'now', 'he', 'says', 'i', 'am', 'inoperable', '(even', 'though', 'i', 'am', 'not', '100%', 'inoperable),', 'and', 'he', 'is', 'letting', 'me', 'die.']\n",
            "Tokenized: ['[CLS]', 'so', ',', 'therefore', ',', 'now', 'he', 'says', 'i', 'am', 'in', '##oper', '##able', '(', 'even', 'though', 'i', 'am', 'not', '100', '%', 'in', '##oper', '##able', ')', ',', 'and', 'he', 'is', 'letting', 'me', 'die', '.', '[SEP]']\n",
            "Original: ['I', 'am', 'just', 'middle', 'aged', 'and', 'do', 'not', 'want', 'to', 'die,', 'but', 'thanks', 'to', 'this', 'doctor', 'i', 'have', 'no', 'other', 'alternatives.']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'just', 'middle', 'aged', 'and', 'do', 'not', 'want', 'to', 'die', ',', 'but', 'thanks', 'to', 'this', 'doctor', 'i', 'have', 'no', 'other', 'alternatives', '.', '[SEP]']\n",
            "Original: ['I,', 'along', 'with', 'my', 'friends,', 'consider', 'this', 'doctor', 'to', 'be', 'the', 'cause', 'of', 'my', 'death', 'as', 'he', 'is', 'not', 'even', 'trying', 'to', 'save', 'my', 'life', 'by', 'operating.']\n",
            "Tokenized: ['[CLS]', 'i', ',', 'along', 'with', 'my', 'friends', ',', 'consider', 'this', 'doctor', 'to', 'be', 'the', 'cause', 'of', 'my', 'death', 'as', 'he', 'is', 'not', 'even', 'trying', 'to', 'save', 'my', 'life', 'by', 'operating', '.', '[SEP]']\n",
            "Original: ['Even', 'my', 'PA', 'i', 'went', 'to', 'the', 'other', 'day', 'said', '\"it', 'must', 'by', 'comforting', 'to', 'have', 'gone', 'to', 'a', 'heart', 'surgeon', 'like', 'him', 'who', 'will', 'do', 'nothing', 'for', 'you\".']\n",
            "Tokenized: ['[CLS]', 'even', 'my', 'pa', 'i', 'went', 'to', 'the', 'other', 'day', 'said', '\"', 'it', 'must', 'by', 'comforting', 'to', 'have', 'gone', 'to', 'a', 'heart', 'surgeon', 'like', 'him', 'who', 'will', 'do', 'nothing', 'for', 'you', '\"', '.', '[SEP]']\n",
            "Original: ['He', 'said', 'it', 'sarcastically.']\n",
            "Tokenized: ['[CLS]', 'he', 'said', 'it', 'sarcastically', '.', '[SEP]']\n",
            "Original: ['If', 'you', 'want', 'a', 'doctor', 'who', 'will', 'lie', 'to', 'you', 'and', 'say', 'he', 'will', 'operate', 'and', 'then', 'change', 'his', 'mind,', 'and', 'not', 'know', 'what', 'he', 'is', 'talking', 'about', 'when', 'he', 'recommends', 'procedures', 'at', 'other', 'hospitals', 'and', 'says', 'they', 'are', 'what', 'you', 'need,', 'when', 'they', 'will', 'not', 'work', 'for', 'you,', 'go', 'to', 'this', 'doctor...he', 'is', 'the', 'one', 'for', 'you.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'want', 'a', 'doctor', 'who', 'will', 'lie', 'to', 'you', 'and', 'say', 'he', 'will', 'operate', 'and', 'then', 'change', 'his', 'mind', ',', 'and', 'not', 'know', 'what', 'he', 'is', 'talking', 'about', 'when', 'he', 'recommends', 'procedures', 'at', 'other', 'hospitals', 'and', 'says', 'they', 'are', 'what', 'you', 'need', ',', 'when', 'they', 'will', 'not', 'work', 'for', 'you', ',', 'go', 'to', 'this', 'doctor', '.', '.', '.', 'he', 'is', 'the', 'one', 'for', 'you', '.', '[SEP]']\n",
            "Original: [\"It's\", 'helpful', 'to', 'know', 'a', 'quite', 'a', 'bit', 'about', 'bull', 'fighting.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'helpful', 'to', 'know', 'a', 'quite', 'a', 'bit', 'about', 'bull', 'fighting', '.', '[SEP]']\n",
            "Original: ['If', 'you', 'watch', 'a', 'lot', 'of', 'fights', '(youtube)', 'and', 'research', 'some', 'of', 'the', 'history', '(http://en.wikipedia.org/wiki/Bullfighting)', 'you', 'will', 'find', 'yourself', 'enjoying', 'it', 'a', 'lot', 'more!']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'watch', 'a', 'lot', 'of', 'fights', '(', 'youtube', ')', 'and', 'research', 'some', 'of', 'the', 'history', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'bull', '##fighting', ')', 'you', 'will', 'find', 'yourself', 'enjoying', 'it', 'a', 'lot', 'more', '!', '[SEP]']\n",
            "Original: ['Also', 'after', 'seeing', 'a', 'handful', 'of', 'bullfights,', 'I', 'can', 'say', 'that', \"they're\", 'a', 'lot', 'more', 'enjoyable', 'if', \"you're\", 'smashed', '(BAC>=', '.15).']\n",
            "Tokenized: ['[CLS]', 'also', 'after', 'seeing', 'a', 'handful', 'of', 'bull', '##fight', '##s', ',', 'i', 'can', 'say', 'that', 'they', \"'\", 're', 'a', 'lot', 'more', 'enjoyable', 'if', 'you', \"'\", 're', 'smashed', '(', 'ba', '##c', '>', '=', '.', '15', ')', '.', '[SEP]']\n",
            "Original: ['Clean', 'rooms,', 'great', 'for', 'the', 'price', 'and', 'cheapest', 'on', 'the', 'exit.']\n",
            "Tokenized: ['[CLS]', 'clean', 'rooms', ',', 'great', 'for', 'the', 'price', 'and', 'cheap', '##est', 'on', 'the', 'exit', '.', '[SEP]']\n",
            "Original: ['The', 'front', 'desk', 'staff', 'was', 'very', 'pleasant', 'and', 'efficient.']\n",
            "Tokenized: ['[CLS]', 'the', 'front', 'desk', 'staff', 'was', 'very', 'pleasant', 'and', 'efficient', '.', '[SEP]']\n",
            "Original: ['Serves', 'FREE', 'breakfast', '!']\n",
            "Tokenized: ['[CLS]', 'serves', 'free', 'breakfast', '!', '[SEP]']\n",
            "Original: ['There', 'are', 'so', 'many', 'wonderful', 'great', 'places', 'to', 'dine', 'in', \"houston....don't.waste\", 'your', 'time', 'here.']\n",
            "Tokenized: ['[CLS]', 'there', 'are', 'so', 'many', 'wonderful', 'great', 'places', 'to', 'din', '##e', 'in', 'houston', '.', '.', '.', '.', 'don', \"'\", 't', '.', 'waste', 'your', 'time', 'here', '.', '[SEP]']\n",
            "Original: ['I', 'had', 'the', 'morelias', 'enchiladas.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'the', 'more', '##lia', '##s', 'en', '##chi', '##lad', '##as', '.', '[SEP]']\n",
            "Original: ['The', 'sauce', 'was', 'dry', 'and', 'the', 'enchiladas', 'did', 'not', 'taste', 'good.at', 'all.']\n",
            "Tokenized: ['[CLS]', 'the', 'sauce', 'was', 'dry', 'and', 'the', 'en', '##chi', '##lad', '##as', 'did', 'not', 'taste', 'good', '.', 'at', 'all', '.', '[SEP]']\n",
            "Original: ['In', 'fact', 'my', 'friend', 'vomited', 'after', 'our', 'meal.']\n",
            "Tokenized: ['[CLS]', 'in', 'fact', 'my', 'friend', 'vomit', '##ed', 'after', 'our', 'meal', '.', '[SEP]']\n",
            "Original: ['Maybe', 'we', 'ordered', 'the', 'wrong', 'dish', 'but', 'my', 'experience', 'here', 'was', 'poor.']\n",
            "Tokenized: ['[CLS]', 'maybe', 'we', 'ordered', 'the', 'wrong', 'dish', 'but', 'my', 'experience', 'here', 'was', 'poor', '.', '[SEP]']\n",
            "Original: ['Service', 'was', 'okay', 'not', 'great,', 'we', 'came', 'for', 'a', 'late', 'lunch.']\n",
            "Tokenized: ['[CLS]', 'service', 'was', 'okay', 'not', 'great', ',', 'we', 'came', 'for', 'a', 'late', 'lunch', '.', '[SEP]']\n",
            "Original: ['I', \"don't\", 'recommend', 'this', 'place.']\n",
            "Tokenized: ['[CLS]', 'i', 'don', \"'\", 't', 'recommend', 'this', 'place', '.', '[SEP]']\n",
            "Original: ['Great', 'Service,', 'Thanks', 'Don.']\n",
            "Tokenized: ['[CLS]', 'great', 'service', ',', 'thanks', 'don', '.', '[SEP]']\n",
            "Original: ['Nice', 'Top', 'Lights.']\n",
            "Tokenized: ['[CLS]', 'nice', 'top', 'lights', '.', '[SEP]']\n",
            "Original: ['Fresh', 'and', 'Excellent', 'Quality']\n",
            "Tokenized: ['[CLS]', 'fresh', 'and', 'excellent', 'quality', '[SEP]']\n",
            "Original: ['We', 'order', 'take', 'out', 'from', 'here', 'all', 'the', 'time', 'and', 'we', 'are', 'never', 'disappointed.']\n",
            "Tokenized: ['[CLS]', 'we', 'order', 'take', 'out', 'from', 'here', 'all', 'the', 'time', 'and', 'we', 'are', 'never', 'disappointed', '.', '[SEP]']\n",
            "Original: ['The', 'food', 'is', 'always', 'fresh', 'and', 'delicious.']\n",
            "Tokenized: ['[CLS]', 'the', 'food', 'is', 'always', 'fresh', 'and', 'delicious', '.', '[SEP]']\n",
            "Original: ['It', 'can', 'be', 'a', 'little', 'on', 'the', 'spicy', 'side', 'but', 'just', 'ask', 'them', 'exactly', 'what', 'you', 'want', 'and', 'they', 'are', 'very', 'helpful.']\n",
            "Tokenized: ['[CLS]', 'it', 'can', 'be', 'a', 'little', 'on', 'the', 'spicy', 'side', 'but', 'just', 'ask', 'them', 'exactly', 'what', 'you', 'want', 'and', 'they', 'are', 'very', 'helpful', '.', '[SEP]']\n",
            "Original: ['A', 'good', 'cut!']\n",
            "Tokenized: ['[CLS]', 'a', 'good', 'cut', '!', '[SEP]']\n",
            "Original: ['Cécile', 'is', 'a', 'hairdresser', 'and', 'has', 'just', 'moved', 'into', 'the', 'neighbourhood.']\n",
            "Tokenized: ['[CLS]', 'cecil', '##e', 'is', 'a', 'hair', '##dre', '##sser', 'and', 'has', 'just', 'moved', 'into', 'the', 'neighbourhood', '.', '[SEP]']\n",
            "Original: ['I', 'go', 'to', 'see', 'her', 'to', 'have', 'my', 'hair', 'cut.']\n",
            "Tokenized: ['[CLS]', 'i', 'go', 'to', 'see', 'her', 'to', 'have', 'my', 'hair', 'cut', '.', '[SEP]']\n",
            "Original: ['FAST', 'and', 'reasonable$']\n",
            "Tokenized: ['[CLS]', 'fast', 'and', 'reasonable', '$', '[SEP]']\n",
            "Original: ['We', 'went', 'to', 'Kobeys', 'on', 'Saturday', 'and', 'had', 'our', 'whole', 'teams', 'uniforms', 'done!']\n",
            "Tokenized: ['[CLS]', 'we', 'went', 'to', 'kobe', '##ys', 'on', 'saturday', 'and', 'had', 'our', 'whole', 'teams', 'uniforms', 'done', '!', '[SEP]']\n",
            "Original: ['He', 'was', 'less', 'than', 'half', 'of', 'the', 'price', 'of', 'the', 'cheapest', 'quote', 'we', 'got,', 'and', 'his', 'work', 'was', 'top', 'notch.']\n",
            "Tokenized: ['[CLS]', 'he', 'was', 'less', 'than', 'half', 'of', 'the', 'price', 'of', 'the', 'cheap', '##est', 'quote', 'we', 'got', ',', 'and', 'his', 'work', 'was', 'top', 'notch', '.', '[SEP]']\n",
            "Original: ['Down', 'to', 'earth', 'and', 'fast', 'service.']\n",
            "Tokenized: ['[CLS]', 'down', 'to', 'earth', 'and', 'fast', 'service', '.', '[SEP]']\n",
            "Original: ['Going', 'back', 'to', 'have', 'some', 'lab', 'coats', 'done', 'this', 'weekend!']\n",
            "Tokenized: ['[CLS]', 'going', 'back', 'to', 'have', 'some', 'lab', 'coats', 'done', 'this', 'weekend', '!', '[SEP]']\n",
            "Original: ['Great', 'out', 'night!']\n",
            "Tokenized: ['[CLS]', 'great', 'out', 'night', '!', '[SEP]']\n",
            "Original: ['You', \"can't\", 'go', 'wrong', 'with', 'Tuesday', 'prices,', 'even', 'if', 'you', 'get', 'quite', 'the', 'mixed', 'bag', 'of', 'comedians!']\n",
            "Tokenized: ['[CLS]', 'you', 'can', \"'\", 't', 'go', 'wrong', 'with', 'tuesday', 'prices', ',', 'even', 'if', 'you', 'get', 'quite', 'the', 'mixed', 'bag', 'of', 'comedians', '!', '[SEP]']\n",
            "Original: ['Extensive', 'drink', 'list', 'and', 'daily', 'specials', 'but', 'wish', 'they', 'had', 'a', 'bit', 'more', 'on', 'their', 'food', 'menu,', 'although', 'popcorn', 'is', 'a', 'nice', 'touch!']\n",
            "Tokenized: ['[CLS]', 'extensive', 'drink', 'list', 'and', 'daily', 'specials', 'but', 'wish', 'they', 'had', 'a', 'bit', 'more', 'on', 'their', 'food', 'menu', ',', 'although', 'popcorn', 'is', 'a', 'nice', 'touch', '!', '[SEP]']\n",
            "Original: ['(other', 'items:', 'chicken', 'fingers,', 'wings,', 'asian', 'pizza,', 'and', 'yam', 'and', 'regular', 'fries)']\n",
            "Tokenized: ['[CLS]', '(', 'other', 'items', ':', 'chicken', 'fingers', ',', 'wings', ',', 'asian', 'pizza', ',', 'and', 'ya', '##m', 'and', 'regular', 'fries', ')', '[SEP]']\n",
            "Original: ['amazing,', 'fun,', 'great', 'beers.']\n",
            "Tokenized: ['[CLS]', 'amazing', ',', 'fun', ',', 'great', 'beers', '.', '[SEP]']\n",
            "Original: ['service', 'could', 'be', 'a', 'little', 'better', 'but', 'its', 'an', 'all', 'round', 'good', 'place']\n",
            "Tokenized: ['[CLS]', 'service', 'could', 'be', 'a', 'little', 'better', 'but', 'its', 'an', 'all', 'round', 'good', 'place', '[SEP]']\n",
            "Original: ['Unbelievably', 'huge', 'experience', 'for', 'such', 'a', 'small', 'salon!!!']\n",
            "Tokenized: ['[CLS]', 'un', '##bel', '##ie', '##va', '##bly', 'huge', 'experience', 'for', 'such', 'a', 'small', 'salon', '!', '!', '!', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'to', 'Kim', 'at', 'Cheveux', 'for', 'more', 'than', 'five', 'years.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'to', 'kim', 'at', 'che', '##ve', '##ux', 'for', 'more', 'than', 'five', 'years', '.', '[SEP]']\n",
            "Original: ['I', 'cannot', 'tell', 'you', 'how', 'often', 'I', 'am', 'complimented', 'on', 'my', 'hair', '(style', 'AND', 'color)!']\n",
            "Tokenized: ['[CLS]', 'i', 'cannot', 'tell', 'you', 'how', 'often', 'i', 'am', 'complimented', 'on', 'my', 'hair', '(', 'style', 'and', 'color', ')', '!', '[SEP]']\n",
            "Original: ['I', 'regularly', 'request', \"Kim's\", 'business', 'cards', 'as', 'I', 'am', 'often', 'stopped', 'on', 'the', 'street', 'and', 'asked', '\"Who', 'does', 'your', 'hair...I', 'LOVE', 'it!!!\"']\n",
            "Tokenized: ['[CLS]', 'i', 'regularly', 'request', 'kim', \"'\", 's', 'business', 'cards', 'as', 'i', 'am', 'often', 'stopped', 'on', 'the', 'street', 'and', 'asked', '\"', 'who', 'does', 'your', 'hair', '.', '.', '.', 'i', 'love', 'it', '!', '!', '!', '\"', '[SEP]']\n",
            "Original: ['Quaint,', 'lovely,', 'small', 'salon', 'with', 'BIG', 'personality.']\n",
            "Tokenized: ['[CLS]', 'qu', '##aint', ',', 'lovely', ',', 'small', 'salon', 'with', 'big', 'personality', '.', '[SEP]']\n",
            "Original: ['I', 'am', 'made', 'to', 'feel', 'special', 'when', 'I', 'am', 'in', 'the', 'chair', 'and', 'I', 'have', 'NEVER', 'had', 'a', 'less', 'than', 'amazing', 'cut', 'or', 'color', 'experience.']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'made', 'to', 'feel', 'special', 'when', 'i', 'am', 'in', 'the', 'chair', 'and', 'i', 'have', 'never', 'had', 'a', 'less', 'than', 'amazing', 'cut', 'or', 'color', 'experience', '.', '[SEP]']\n",
            "Original: ['My', 'hair', 'has', 'never', 'felt', 'this', 'healthy,', 'either.']\n",
            "Tokenized: ['[CLS]', 'my', 'hair', 'has', 'never', 'felt', 'this', 'healthy', ',', 'either', '.', '[SEP]']\n",
            "Original: ['I', 'cannot', 'recommend', 'this', 'salon', 'enough!!!']\n",
            "Tokenized: ['[CLS]', 'i', 'cannot', 'recommend', 'this', 'salon', 'enough', '!', '!', '!', '[SEP]']\n",
            "Original: ['Thanks', 'Cheveux!']\n",
            "Tokenized: ['[CLS]', 'thanks', 'che', '##ve', '##ux', '!', '[SEP]']\n",
            "Original: ['wonderful']\n",
            "Tokenized: ['[CLS]', 'wonderful', '[SEP]']\n",
            "Original: ['I', 'went', 'to', 'ohm', 'after', 'reading', 'some', 'of', 'the', 'reviews.']\n",
            "Tokenized: ['[CLS]', 'i', 'went', 'to', 'oh', '##m', 'after', 'reading', 'some', 'of', 'the', 'reviews', '.', '[SEP]']\n",
            "Original: ['I', 'go', 'to', 'school', 'in', 'the', 'area', 'and', 'usually', 'wait', 'until', 'I', 'go', 'home', 'to', 'get', 'my', 'hair', 'cut.']\n",
            "Tokenized: ['[CLS]', 'i', 'go', 'to', 'school', 'in', 'the', 'area', 'and', 'usually', 'wait', 'until', 'i', 'go', 'home', 'to', 'get', 'my', 'hair', 'cut', '.', '[SEP]']\n",
            "Original: ['I', 'decided', 'it', 'was', 'time', 'to', 'grow', 'up', 'and', 'made', 'an', 'appointment.']\n",
            "Tokenized: ['[CLS]', 'i', 'decided', 'it', 'was', 'time', 'to', 'grow', 'up', 'and', 'made', 'an', 'appointment', '.', '[SEP]']\n",
            "Original: ['Sierra', 'was', 'my', 'stylist', 'and', 'i', 'love', 'what', 'she', 'did.']\n",
            "Tokenized: ['[CLS]', 'sierra', 'was', 'my', 'st', '##yl', '##ist', 'and', 'i', 'love', 'what', 'she', 'did', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'wavy', 'hair', 'and', 'she', 'cut', 'to', 'my', 'hair', 'style.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'wavy', 'hair', 'and', 'she', 'cut', 'to', 'my', 'hair', 'style', '.', '[SEP]']\n",
            "Original: ['It', 'was', 'the', 'first', 'time', 'i', 'had', 'left', 'a', 'salon', 'with', 'my', 'hair', 'curly.']\n",
            "Tokenized: ['[CLS]', 'it', 'was', 'the', 'first', 'time', 'i', 'had', 'left', 'a', 'salon', 'with', 'my', 'hair', 'curly', '.', '[SEP]']\n",
            "Original: ['Usually', 'they', 'blow', 'dry', 'it', 'out', 'and', 'i', 'have', 'to', 'wait', 'until', 'i', 'wash', 'it', 'to', 'see', 'what', 'it', 'will', 'look', 'like', 'in', 'its', 'natural', 'state.']\n",
            "Tokenized: ['[CLS]', 'usually', 'they', 'blow', 'dry', 'it', 'out', 'and', 'i', 'have', 'to', 'wait', 'until', 'i', 'wash', 'it', 'to', 'see', 'what', 'it', 'will', 'look', 'like', 'in', 'its', 'natural', 'state', '.', '[SEP]']\n",
            "Original: ['But', 'she', 'did', 'a', 'fabulous', 'job', 'letting', 'me', 'know', 'what', 'she', 'was', 'doing', 'at', 'all', 'times', 'and', 'styled', 'my', 'hair', 'in', 'a', 'way', 'i', 'could', 'do', 'it', 'at', 'home.']\n",
            "Tokenized: ['[CLS]', 'but', 'she', 'did', 'a', 'fabulous', 'job', 'letting', 'me', 'know', 'what', 'she', 'was', 'doing', 'at', 'all', 'times', 'and', 'styled', 'my', 'hair', 'in', 'a', 'way', 'i', 'could', 'do', 'it', 'at', 'home', '.', '[SEP]']\n",
            "Original: ['It', \"wasn't\", 'completly', 'impossible!!!!']\n",
            "Tokenized: ['[CLS]', 'it', 'wasn', \"'\", 't', 'com', '##ple', '##tly', 'impossible', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['I', 'am', 'definitely', 'going', 'back']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'definitely', 'going', 'back', '[SEP]']\n",
            "Original: ['Restaurant', 'on', 'top', 'was', 'renovated,', 'food', 'was', 'decent,', 'price', 'was', 'way', 'to', 'high', 'for', 'Duluth', 'for', 'quality,', 'new', 'decor', 'seems', 'tacky']\n",
            "Tokenized: ['[CLS]', 'restaurant', 'on', 'top', 'was', 'renovated', ',', 'food', 'was', 'decent', ',', 'price', 'was', 'way', 'to', 'high', 'for', 'duluth', 'for', 'quality', ',', 'new', 'decor', 'seems', 'tack', '##y', '[SEP]']\n",
            "Original: ['junkie', 'lube?!']\n",
            "Tokenized: ['[CLS]', 'junk', '##ie', 'lu', '##be', '?', '!', '[SEP]']\n",
            "Original: ['really', 'weird', 'place,', 'I', 'was', 'driving', 'home', 'from', 'work', 'thought', \"I'd\", 'stop', 'in', 'for', 'an', 'oil', 'change', 'and', 'well,', 'there', 'are', 'a', 'few', 'guys', 'in', 'jiffy', 'lube', 'uniforms', 'sitting', 'at', 'the', 'table', 'drinking', 'beers', 'and', 'shooting', 'the', 'breeze.']\n",
            "Tokenized: ['[CLS]', 'really', 'weird', 'place', ',', 'i', 'was', 'driving', 'home', 'from', 'work', 'thought', 'i', \"'\", 'd', 'stop', 'in', 'for', 'an', 'oil', 'change', 'and', 'well', ',', 'there', 'are', 'a', 'few', 'guys', 'in', 'ji', '##ffy', 'lu', '##be', 'uniforms', 'sitting', 'at', 'the', 'table', 'drinking', 'beers', 'and', 'shooting', 'the', 'breeze', '.', '[SEP]']\n",
            "Original: ['The', 'neon', 'lighting', 'sign', 'still', 'said', '\"on\"', 'yet', 'some', 'guy', 'who', 'seemed', 'to', 'be', 'the', 'oldest', 'of', 'the', 'bunch,', 'more', 'like', 'the', 'drunkest', 'of', 'the', 'bunch', 'told', 'me', \"they'd\", 'be', 'open', 'tomorrow.']\n",
            "Tokenized: ['[CLS]', 'the', 'neon', 'lighting', 'sign', 'still', 'said', '\"', 'on', '\"', 'yet', 'some', 'guy', 'who', 'seemed', 'to', 'be', 'the', 'oldest', 'of', 'the', 'bunch', ',', 'more', 'like', 'the', 'drunk', '##est', 'of', 'the', 'bunch', 'told', 'me', 'they', \"'\", 'd', 'be', 'open', 'tomorrow', '.', '[SEP]']\n",
            "Original: ['I', 'asked', 'if', 'a', 'manager', 'was', 'on', 'duty', 'he', 'told', 'me', 'he', 'was.']\n",
            "Tokenized: ['[CLS]', 'i', 'asked', 'if', 'a', 'manager', 'was', 'on', 'duty', 'he', 'told', 'me', 'he', 'was', '.', '[SEP]']\n",
            "Original: ['WOW!']\n",
            "Tokenized: ['[CLS]', 'wow', '!', '[SEP]']\n",
            "Original: ['grey', 'shirt', '\"mark\"', 'of', 'course', 'with', 'this', 'type', 'behavior', 'he', 'could', 'have', 'been', 'wearing', 'someonelses', 'clothing.....']\n",
            "Tokenized: ['[CLS]', 'grey', 'shirt', '\"', 'mark', '\"', 'of', 'course', 'with', 'this', 'type', 'behavior', 'he', 'could', 'have', 'been', 'wearing', 'someone', '##ls', '##es', 'clothing', '.', '.', '.', '.', '.', '[SEP]']\n",
            "Original: ['awesome', 'place!!!']\n",
            "Tokenized: ['[CLS]', 'awesome', 'place', '!', '!', '!', '[SEP]']\n",
            "Original: ['it', 'was', 'worth', 'of', 'the', 'ride', 'more', 'than', 'an', 'hour...']\n",
            "Tokenized: ['[CLS]', 'it', 'was', 'worth', 'of', 'the', 'ride', 'more', 'than', 'an', 'hour', '.', '.', '.', '[SEP]']\n",
            "Original: ['I', 'had', 'so', 'many', 'strawberries', 'right', 'on', 'the', 'field...strongly', 'recomend...dont', 'forget', 'to', 'try', 'their', 'great', 'ice', 'cream']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'so', 'many', 'straw', '##berries', 'right', 'on', 'the', 'field', '.', '.', '.', 'strongly', 'rec', '##ome', '##nd', '.', '.', '.', 'don', '##t', 'forget', 'to', 'try', 'their', 'great', 'ice', 'cream', '[SEP]']\n",
            "Original: ['ok', 'but', 'just', 'becuse', 'we', 'where', 'on', 'a', 'tight', 'budget.']\n",
            "Tokenized: ['[CLS]', 'ok', 'but', 'just', 'be', '##cus', '##e', 'we', 'where', 'on', 'a', 'tight', 'budget', '.', '[SEP]']\n",
            "Original: ['me', 'and', 'my', 'dad', 'where', 'in', 'NJ', 'for', 'a', 'kc', 'chiefs(my', 'home', 'team)vs', 'the', 'NY', 'jets', 'and', 'for', 'game', '4', 'of', 'the', 'world', 'series.']\n",
            "Tokenized: ['[CLS]', 'me', 'and', 'my', 'dad', 'where', 'in', 'nj', 'for', 'a', 'kc', 'chiefs', '(', 'my', 'home', 'team', ')', 'vs', 'the', 'ny', 'jets', 'and', 'for', 'game', '4', 'of', 'the', 'world', 'series', '.', '[SEP]']\n",
            "Original: [\"i'ma\", 'red', 'sox', 'fan', 'so', 'i', 'was', 'glad', 'that', 'the', 'phillies', 'won.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'ma', 'red', 'sox', 'fan', 'so', 'i', 'was', 'glad', 'that', 'the', 'phillies', 'won', '.', '[SEP]']\n",
            "Original: ['the', 'knights', 'inn', 'was', 'small', 'very', 'small.i', 'mean', '1', 'room', 'in', 'every', 'room!it', 'was', 'cozy', 'a', 'little', 'and', 'a', 'small', 'tv.']\n",
            "Tokenized: ['[CLS]', 'the', 'knights', 'inn', 'was', 'small', 'very', 'small', '.', 'i', 'mean', '1', 'room', 'in', 'every', 'room', '!', 'it', 'was', 'cozy', 'a', 'little', 'and', 'a', 'small', 'tv', '.', '[SEP]']\n",
            "Original: ['i', 'mean', 'a', '2', 'day', 'stay', 'was', 'a', 'ok', 'stay', 'even', 'tho', 'the', 'manager', 'looked', 'like', 'he', 'was', 'chineze', 'and', 'that', 'we', 'only', 'slept', 'and', 'went', 'out', 'from', '8', 'to', '7', 'to', 'see', 'New', 'York.']\n",
            "Tokenized: ['[CLS]', 'i', 'mean', 'a', '2', 'day', 'stay', 'was', 'a', 'ok', 'stay', 'even', 'tho', 'the', 'manager', 'looked', 'like', 'he', 'was', 'chin', '##ez', '##e', 'and', 'that', 'we', 'only', 'slept', 'and', 'went', 'out', 'from', '8', 'to', '7', 'to', 'see', 'new', 'york', '.', '[SEP]']\n",
            "Original: ['but', 'sice', 'we', 'almost', 'just', 'slept', 'there', 'i', 'cant', 'give', 'that', 'good', 'of', 'a', 'review']\n",
            "Tokenized: ['[CLS]', 'but', 'sic', '##e', 'we', 'almost', 'just', 'slept', 'there', 'i', 'can', '##t', 'give', 'that', 'good', 'of', 'a', 'review', '[SEP]']\n",
            "Original: ['In', 'this', 'hard', 'economic', 'times', 'is', 'very', 'important', 'to', 'save', 'money', 'Very', 'reasonable', 'prices', 'top', 'quality', 'work', 'The', 'owner', 'operator', 'he', 'does', 'all', 'the', 'the', 'work', 'with', 'Helpers', 'very', 'friendly', 'I', 'definitely', 'recommend', 'this', 'this', 'guys', \"Don't\", 'get', 'jack', 'by', 'big', 'companies', 'that', 'they', 'pay', 'alot', 'of', 'money', 'to', 'be', 'on', 'top', 'of', 'the', 'list', 'Thanks']\n",
            "Tokenized: ['[CLS]', 'in', 'this', 'hard', 'economic', 'times', 'is', 'very', 'important', 'to', 'save', 'money', 'very', 'reasonable', 'prices', 'top', 'quality', 'work', 'the', 'owner', 'operator', 'he', 'does', 'all', 'the', 'the', 'work', 'with', 'help', '##ers', 'very', 'friendly', 'i', 'definitely', 'recommend', 'this', 'this', 'guys', 'don', \"'\", 't', 'get', 'jack', 'by', 'big', 'companies', 'that', 'they', 'pay', 'al', '##ot', 'of', 'money', 'to', 'be', 'on', 'top', 'of', 'the', 'list', 'thanks', '[SEP]']\n",
            "Original: ['Review', 'on', 'House', 'of', 'Joy', 'Chinese', 'Restaurant']\n",
            "Tokenized: ['[CLS]', 'review', 'on', 'house', 'of', 'joy', 'chinese', 'restaurant', '[SEP]']\n",
            "Original: ['My', 'family', 'and', 'I', 'moved', 'to', 'San', 'Antonio', 'a', 'year', 'ago', 'and', 'have', 'tried', 'almost', 'all', 'of', 'the', 'Chinese', 'Restaurants', 'because', 'we', 'love', 'Chinese', 'food.']\n",
            "Tokenized: ['[CLS]', 'my', 'family', 'and', 'i', 'moved', 'to', 'san', 'antonio', 'a', 'year', 'ago', 'and', 'have', 'tried', 'almost', 'all', 'of', 'the', 'chinese', 'restaurants', 'because', 'we', 'love', 'chinese', 'food', '.', '[SEP]']\n",
            "Original: ['Well', 'it', 'took', 'us', 'a', 'while', 'to', 'find', 'one', 'that', 'we', 'liked.']\n",
            "Tokenized: ['[CLS]', 'well', 'it', 'took', 'us', 'a', 'while', 'to', 'find', 'one', 'that', 'we', 'liked', '.', '[SEP]']\n",
            "Original: ['But', 'we', \"don't\", 'just', 'like', 'House', 'of', 'Joy', 'WE', 'LOVE', 'IT.']\n",
            "Tokenized: ['[CLS]', 'but', 'we', 'don', \"'\", 't', 'just', 'like', 'house', 'of', 'joy', 'we', 'love', 'it', '.', '[SEP]']\n",
            "Original: ['Everything', 'we', 'have', 'gotten', 'there', 'has', 'been', 'more', 'authentic', 'and', 'better', 'tasting', 'than', 'any', 'other', 'Chinese', 'restaurant', 'in', 'the', 'San', 'Antonio', 'area', 'we', 'have', 'been', 'to--and', 'trust', 'me', 'we', 'have', 'been', 'to', 'a', 'lot', 'of', 'them.']\n",
            "Tokenized: ['[CLS]', 'everything', 'we', 'have', 'gotten', 'there', 'has', 'been', 'more', 'authentic', 'and', 'better', 'tasting', 'than', 'any', 'other', 'chinese', 'restaurant', 'in', 'the', 'san', 'antonio', 'area', 'we', 'have', 'been', 'to', '-', '-', 'and', 'trust', 'me', 'we', 'have', 'been', 'to', 'a', 'lot', 'of', 'them', '.', '[SEP]']\n",
            "Original: ['We', 'just', 'happen', 'to', 'stumble', 'across', 'this', 'little', 'restaurant', 'one', 'day', 'when', 'we', 'had', 'to', 'visit', 'the', 'Bexar', 'County', 'Tax', 'Office', 'off', 'of', 'Bandera', 'Road.']\n",
            "Tokenized: ['[CLS]', 'we', 'just', 'happen', 'to', 'stumble', 'across', 'this', 'little', 'restaurant', 'one', 'day', 'when', 'we', 'had', 'to', 'visit', 'the', 'be', '##xa', '##r', 'county', 'tax', 'office', 'off', 'of', 'band', '##era', 'road', '.', '[SEP]']\n",
            "Original: ['We', 'stopped', 'in', 'and', 'got', 'some', 'take', 'out', 'and', 'cannot', 'stop', 'going', 'back.']\n",
            "Tokenized: ['[CLS]', 'we', 'stopped', 'in', 'and', 'got', 'some', 'take', 'out', 'and', 'cannot', 'stop', 'going', 'back', '.', '[SEP]']\n",
            "Original: ['They', 'have', 'the', 'best', 'Egg', 'Drop', 'Soup', 'I', 'have', 'ever', 'tasted.']\n",
            "Tokenized: ['[CLS]', 'they', 'have', 'the', 'best', 'egg', 'drop', 'soup', 'i', 'have', 'ever', 'tasted', '.', '[SEP]']\n",
            "Original: ['We', 'also', 'love', 'their', 'Egg', 'Rolls', 'and', 'Spring', 'Rolls.']\n",
            "Tokenized: ['[CLS]', 'we', 'also', 'love', 'their', 'egg', 'rolls', 'and', 'spring', 'rolls', '.', '[SEP]']\n",
            "Original: ['And', 'every', 'entree', 'we', 'have', 'ordered', 'is', 'perfect.']\n",
            "Tokenized: ['[CLS]', 'and', 'every', 'en', '##tree', 'we', 'have', 'ordered', 'is', 'perfect', '.', '[SEP]']\n",
            "Original: ['Everything', 'is', 'always', 'cooked', 'fresh', 'and', 'tastes', 'fresh.']\n",
            "Tokenized: ['[CLS]', 'everything', 'is', 'always', 'cooked', 'fresh', 'and', 'tastes', 'fresh', '.', '[SEP]']\n",
            "Original: ['Their', 'prices', 'are', 'extremely', 'reasonable', 'for', 'the', 'amount', 'of', 'food', 'you', 'receive.']\n",
            "Tokenized: ['[CLS]', 'their', 'prices', 'are', 'extremely', 'reasonable', 'for', 'the', 'amount', 'of', 'food', 'you', 'receive', '.', '[SEP]']\n",
            "Original: ['The', 'staff', 'is', 'also', 'just', 'so', 'pleasant', 'to', 'deal', 'with.']\n",
            "Tokenized: ['[CLS]', 'the', 'staff', 'is', 'also', 'just', 'so', 'pleasant', 'to', 'deal', 'with', '.', '[SEP]']\n",
            "Original: ['They', 'are', 'also', 'quick', 'at', 'getting', 'your', 'order', 'out', 'to', 'you.']\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'also', 'quick', 'at', 'getting', 'your', 'order', 'out', 'to', 'you', '.', '[SEP]']\n",
            "Original: ['You', \"don't\", 'have', 'to', 'sit', 'and', 'wait', 'around', 'forever', 'like', 'most', 'places!!']\n",
            "Tokenized: ['[CLS]', 'you', 'don', \"'\", 't', 'have', 'to', 'sit', 'and', 'wait', 'around', 'forever', 'like', 'most', 'places', '!', '!', '[SEP]']\n",
            "Original: ['So', 'anyone', 'looking', 'for', 'an', 'Excellent', 'night', 'out', 'for', 'Chinese', 'or', 'maybe', 'just', 'lunch', 'should', 'stop', 'in', 'and', 'try', 'it', 'because', 'I', 'promise', 'you', '2', 'things---You', \"won't\", 'regret', 'it!!']\n",
            "Tokenized: ['[CLS]', 'so', 'anyone', 'looking', 'for', 'an', 'excellent', 'night', 'out', 'for', 'chinese', 'or', 'maybe', 'just', 'lunch', 'should', 'stop', 'in', 'and', 'try', 'it', 'because', 'i', 'promise', 'you', '2', 'things', '-', '-', '-', 'you', 'won', \"'\", 't', 'regret', 'it', '!', '!', '[SEP]']\n",
            "Original: ['and', 'you', 'will', 'go', 'back', 'for', 'more!!!']\n",
            "Tokenized: ['[CLS]', 'and', 'you', 'will', 'go', 'back', 'for', 'more', '!', '!', '!', '[SEP]']\n",
            "Original: ['Karla', 'Ferguson-Granger']\n",
            "Tokenized: ['[CLS]', 'karl', '##a', 'ferguson', '-', 'grange', '##r', '[SEP]']\n",
            "Original: ['PS', 'I', 'have', 'noticed', 'on', 'here', 'that', 'someone', 'left', 'a', 'comment', 'that', '\"all', 'of', 'the', 'nice', 'comments', 'must', 'come', 'from', 'co', 'workers', 'or', 'friends\"', 'and', 'I', 'will', 'tell', 'you', 'that', 'I', \"don't\", 'know', 'these', 'people', 'except', 'from', 'eating', 'at', 'their', 'restaurant.']\n",
            "Tokenized: ['[CLS]', 'ps', 'i', 'have', 'noticed', 'on', 'here', 'that', 'someone', 'left', 'a', 'comment', 'that', '\"', 'all', 'of', 'the', 'nice', 'comments', 'must', 'come', 'from', 'co', 'workers', 'or', 'friends', '\"', 'and', 'i', 'will', 'tell', 'you', 'that', 'i', 'don', \"'\", 't', 'know', 'these', 'people', 'except', 'from', 'eating', 'at', 'their', 'restaurant', '.', '[SEP]']\n",
            "Original: ['We', 'are', 'from', 'Virginia', 'and', 'just', 'moved', 'here', 'a', 'year', 'ago.']\n",
            "Tokenized: ['[CLS]', 'we', 'are', 'from', 'virginia', 'and', 'just', 'moved', 'here', 'a', 'year', 'ago', '.', '[SEP]']\n",
            "Original: ['So', 'that', 'comment', 'is', 'completely', 'false.']\n",
            "Tokenized: ['[CLS]', 'so', 'that', 'comment', 'is', 'completely', 'false', '.', '[SEP]']\n",
            "Original: ['Because', 'we', 'think', \"IT'S\", 'THE', 'BEST!!!']\n",
            "Tokenized: ['[CLS]', 'because', 'we', 'think', 'it', \"'\", 's', 'the', 'best', '!', '!', '!', '[SEP]']\n",
            "Original: ['and', 'we', \"don't\", 'know', 'them', 'except', 'for', 'eating', 'there.']\n",
            "Tokenized: ['[CLS]', 'and', 'we', 'don', \"'\", 't', 'know', 'them', 'except', 'for', 'eating', 'there', '.', '[SEP]']\n",
            "Original: ['Extremely', 'bad', 'customer', 'service']\n",
            "Tokenized: ['[CLS]', 'extremely', 'bad', 'customer', 'service', '[SEP]']\n",
            "Original: ['Do', 'not', 'go', 'to', 'this', 'salon,', 'especially', 'if', 'you', 'have', 'to', 'get', 'your', 'hair', 'straightened.']\n",
            "Tokenized: ['[CLS]', 'do', 'not', 'go', 'to', 'this', 'salon', ',', 'especially', 'if', 'you', 'have', 'to', 'get', 'your', 'hair', 'straightened', '.', '[SEP]']\n",
            "Original: ['They', 'did', 'a', 'very', 'bad', 'job', 'with', 'my', 'hair', 'and', 'were', 'extremely', 'rude', 'when', 'I', 'went', 'back', 'to', 'ask', 'them', 'why', 'it', \"didn't\", 'work', 'for', 'my', 'hair.']\n",
            "Tokenized: ['[CLS]', 'they', 'did', 'a', 'very', 'bad', 'job', 'with', 'my', 'hair', 'and', 'were', 'extremely', 'rude', 'when', 'i', 'went', 'back', 'to', 'ask', 'them', 'why', 'it', 'didn', \"'\", 't', 'work', 'for', 'my', 'hair', '.', '[SEP]']\n",
            "Original: ['Rude,', 'insensitive,', 'discourteous', 'people!!!!!']\n",
            "Tokenized: ['[CLS]', 'rude', ',', 'ins', '##ens', '##itive', ',', 'disco', '##urt', '##eous', 'people', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['Over', 'charged.']\n",
            "Tokenized: ['[CLS]', 'over', 'charged', '.', '[SEP]']\n",
            "Original: ['I', 'used', 'my', 'card', 'to', 'purchase', 'a', 'meal', 'on', 'the', 'menu', 'and', 'the', 'total', 'on', 'my', 'receipt', 'was', '$8.95', 'but', 'when', 'I', 'went', 'on', 'line', 'to', 'check', 'my', 'transaction', 'it', 'show', '$10.74.']\n",
            "Tokenized: ['[CLS]', 'i', 'used', 'my', 'card', 'to', 'purchase', 'a', 'meal', 'on', 'the', 'menu', 'and', 'the', 'total', 'on', 'my', 'receipt', 'was', '$', '8', '.', '95', 'but', 'when', 'i', 'went', 'on', 'line', 'to', 'check', 'my', 'transaction', 'it', 'show', '$', '10', '.', '74', '.', '[SEP]']\n",
            "Original: ['There', 'is', 'something', 'wrong', 'or', 'maybe', 'the', 'individual', 'made', 'a', 'mistake', 'but', 'to', 'me', 'that', 'is', 'not', 'integrity.']\n",
            "Tokenized: ['[CLS]', 'there', 'is', 'something', 'wrong', 'or', 'maybe', 'the', 'individual', 'made', 'a', 'mistake', 'but', 'to', 'me', 'that', 'is', 'not', 'integrity', '.', '[SEP]']\n",
            "Original: ['Horrible.']\n",
            "Tokenized: ['[CLS]', 'horrible', '.', '[SEP]']\n",
            "Original: ['Horrible.']\n",
            "Tokenized: ['[CLS]', 'horrible', '.', '[SEP]']\n",
            "Original: ['When', 'we', 'walked', 'in,', 'the', 'person', 'behind', 'desk', 'said:', '\"oh', 'well,', 'you', 'must', 'wait,', 'I', 'am', 'in', 'the', 'middle', 'of', 'something.\"']\n",
            "Tokenized: ['[CLS]', 'when', 'we', 'walked', 'in', ',', 'the', 'person', 'behind', 'desk', 'said', ':', '\"', 'oh', 'well', ',', 'you', 'must', 'wait', ',', 'i', 'am', 'in', 'the', 'middle', 'of', 'something', '.', '\"', '[SEP]']\n",
            "Original: ['After', 'a', 'good', 'few', 'minutes,', 'he', 'asked:', '\"what', 'do', 'you', 'want?\"']\n",
            "Tokenized: ['[CLS]', 'after', 'a', 'good', 'few', 'minutes', ',', 'he', 'asked', ':', '\"', 'what', 'do', 'you', 'want', '?', '\"', '[SEP]']\n",
            "Original: ['Somewhere', 'in', 'between', 'his', 'rudeness', 'he', 'asked', 'if', 'we', 'smoked.']\n",
            "Tokenized: ['[CLS]', 'somewhere', 'in', 'between', 'his', 'rude', '##ness', 'he', 'asked', 'if', 'we', 'smoked', '.', '[SEP]']\n",
            "Original: ['I', 'asked', 'if', 'this', 'hotel', 'had', 'smoking', 'rooms.']\n",
            "Tokenized: ['[CLS]', 'i', 'asked', 'if', 'this', 'hotel', 'had', 'smoking', 'rooms', '.', '[SEP]']\n",
            "Original: ['He', 'immediately', 'said', '\"no,', 'there', 'is', 'a', '$50', 'deposit', 'now!\"']\n",
            "Tokenized: ['[CLS]', 'he', 'immediately', 'said', '\"', 'no', ',', 'there', 'is', 'a', '$', '50', 'deposit', 'now', '!', '\"', '[SEP]']\n",
            "Original: ['Sure', 'enough', 'he', 'charged', 'it', 'to', 'the', 'credit', 'card.']\n",
            "Tokenized: ['[CLS]', 'sure', 'enough', 'he', 'charged', 'it', 'to', 'the', 'credit', 'card', '.', '[SEP]']\n",
            "Original: ['When', 'I', 'inquired', 'he', 'rudely', 'replied', '\"in', 'the', 'morning', 'when', 'things', 'are', 'checked', 'out', \"you'll\", 'get', 'it', 'back.\"']\n",
            "Tokenized: ['[CLS]', 'when', 'i', 'inquired', 'he', 'rude', '##ly', 'replied', '\"', 'in', 'the', 'morning', 'when', 'things', 'are', 'checked', 'out', 'you', \"'\", 'll', 'get', 'it', 'back', '.', '\"', '[SEP]']\n",
            "Original: ['I', 'called', 'customer', 'service', 'about', 'it', 'because', 'the', 'website', 'specifically', 'states', 'that', 'there', 'are', 'no', 'other', 'charges', 'at', 'the', 'check-in.']\n",
            "Tokenized: ['[CLS]', 'i', 'called', 'customer', 'service', 'about', 'it', 'because', 'the', 'website', 'specifically', 'states', 'that', 'there', 'are', 'no', 'other', 'charges', 'at', 'the', 'check', '-', 'in', '.', '[SEP]']\n",
            "Original: ['I', 'also', 'mentioned', 'to', 'the', 'reception', 'person.']\n",
            "Tokenized: ['[CLS]', 'i', 'also', 'mentioned', 'to', 'the', 'reception', 'person', '.', '[SEP]']\n",
            "Original: ['He', 'responded', '\"we', 'have', 'problem', 'with', '\\'people\\'\".']\n",
            "Tokenized: ['[CLS]', 'he', 'responded', '\"', 'we', 'have', 'problem', 'with', \"'\", 'people', \"'\", '\"', '.', '[SEP]']\n",
            "Original: ['Imagine', 'a', 'hotel', 'having', 'problems', 'with', 'people.']\n",
            "Tokenized: ['[CLS]', 'imagine', 'a', 'hotel', 'having', 'problems', 'with', 'people', '.', '[SEP]']\n",
            "Original: ['I', 'finally', 'alerted', 'him', 'to', 'his', 'rudeness.']\n",
            "Tokenized: ['[CLS]', 'i', 'finally', 'alerted', 'him', 'to', 'his', 'rude', '##ness', '.', '[SEP]']\n",
            "Original: ['He', 'said', \"he's\", 'had', 'a', 'long', 'and', 'bad', 'day.']\n",
            "Tokenized: ['[CLS]', 'he', 'said', 'he', \"'\", 's', 'had', 'a', 'long', 'and', 'bad', 'day', '.', '[SEP]']\n",
            "Original: ['We', 'had', 'no', 'choice', 'but', 'to', 'stay', 'but', 'will', 'take', 'this', 'as', 'far', 'as', 'we', 'can.']\n",
            "Tokenized: ['[CLS]', 'we', 'had', 'no', 'choice', 'but', 'to', 'stay', 'but', 'will', 'take', 'this', 'as', 'far', 'as', 'we', 'can', '.', '[SEP]']\n",
            "Original: [\"Don't\", 'stay', 'there.']\n",
            "Tokenized: ['[CLS]', 'don', \"'\", 't', 'stay', 'there', '.', '[SEP]']\n",
            "Original: ['I', 'came', 'to', 'find', 'out', 'the', 'person', 'was', 'the', 'hotel', 'OWNER', 'also.']\n",
            "Tokenized: ['[CLS]', 'i', 'came', 'to', 'find', 'out', 'the', 'person', 'was', 'the', 'hotel', 'owner', 'also', '.', '[SEP]']\n",
            "Original: ['Highly', 'Recommend']\n",
            "Tokenized: ['[CLS]', 'highly', 'recommend', '[SEP]']\n",
            "Original: ['I', 'have', 'worked', 'with', 'Shannon', 'as', 'my', 'massage', 'therapist', 'and', 'intuitive', 'bodyworker', 'for', 'years', 'and', 'have', 'never', 'been', 'disappointed.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'worked', 'with', 'shannon', 'as', 'my', 'massage', 'therapist', 'and', 'intuitive', 'body', '##work', '##er', 'for', 'years', 'and', 'have', 'never', 'been', 'disappointed', '.', '[SEP]']\n",
            "Original: ['No', 'matter', 'what', 'state', 'I', 'am', 'in', 'when', 'I', 'arrive,', 'I', 'always', 'leave', 'feeling', 'better.']\n",
            "Tokenized: ['[CLS]', 'no', 'matter', 'what', 'state', 'i', 'am', 'in', 'when', 'i', 'arrive', ',', 'i', 'always', 'leave', 'feeling', 'better', '.', '[SEP]']\n",
            "Original: ['She', 'has', 'not', 'only', 'helped', 'me', 'through', 'some', 'challenging', 'computer-work', 'and', 'sports', 'related', 'injuries,', 'she', 'was', 'wonderful', 'to', 'work', 'with', 'throughout', 'my', 'pregnancy', 'and', 'beyond.']\n",
            "Tokenized: ['[CLS]', 'she', 'has', 'not', 'only', 'helped', 'me', 'through', 'some', 'challenging', 'computer', '-', 'work', 'and', 'sports', 'related', 'injuries', ',', 'she', 'was', 'wonderful', 'to', 'work', 'with', 'throughout', 'my', 'pregnancy', 'and', 'beyond', '.', '[SEP]']\n",
            "Original: ['I', 'highly', 'recommend', 'her.']\n",
            "Tokenized: ['[CLS]', 'i', 'highly', 'recommend', 'her', '.', '[SEP]']\n",
            "Original: ['Great', 'place', 'for', 'embroidery.']\n",
            "Tokenized: ['[CLS]', 'great', 'place', 'for', 'embroidery', '.', '[SEP]']\n",
            "Original: ['Service', 'was', 'friendly', 'and', 'VERY', 'fast.']\n",
            "Tokenized: ['[CLS]', 'service', 'was', 'friendly', 'and', 'very', 'fast', '.', '[SEP]']\n",
            "Original: ['I', 'purchased', 'four', 'gift', 'items', 'there', 'and', 'had', 'them', 'all', 'embroidered', 'within', 'a', 'week.']\n",
            "Tokenized: ['[CLS]', 'i', 'purchased', 'four', 'gift', 'items', 'there', 'and', 'had', 'them', 'all', 'embroidered', 'within', 'a', 'week', '.', '[SEP]']\n",
            "Original: ['The', 'best', 'photographer', 'in', 'Miami']\n",
            "Tokenized: ['[CLS]', 'the', 'best', 'photographer', 'in', 'miami', '[SEP]']\n",
            "Original: ['I', 'was', 'soooo', 'lucky', 'to', 'have', 'used', \"Marlon's\", 'photography', 'services....such', 'a', 'creative', 'and', 'talented', 'photographer', 'and', 'a', 'pleasure', 'to', 'work', 'with.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'soo', '##oo', 'lucky', 'to', 'have', 'used', 'marlon', \"'\", 's', 'photography', 'services', '.', '.', '.', '.', 'such', 'a', 'creative', 'and', 'talented', 'photographer', 'and', 'a', 'pleasure', 'to', 'work', 'with', '.', '[SEP]']\n",
            "Original: ['The', 'images', 'turned', 'out', 'amazing.']\n",
            "Tokenized: ['[CLS]', 'the', 'images', 'turned', 'out', 'amazing', '.', '[SEP]']\n",
            "Original: ['I', 'definitely', 'recommend', 'him', ':)']\n",
            "Tokenized: ['[CLS]', 'i', 'definitely', 'recommend', 'him', ':', ')', '[SEP]']\n",
            "Original: ['Great', 'Service', 'and', 'hairstyles', 'that', 'last.']\n",
            "Tokenized: ['[CLS]', 'great', 'service', 'and', 'hairs', '##tyle', '##s', 'that', 'last', '.', '[SEP]']\n",
            "Original: ['I', 'am', 'pleased', 'with', 'the', 'service', 'that', 'i', 'get', 'at', 'Luxe.']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'pleased', 'with', 'the', 'service', 'that', 'i', 'get', 'at', 'lux', '##e', '.', '[SEP]']\n",
            "Original: ['The', 'staff', 'is', 'very', 'pleasant', 'and', 'my', 'hair', 'is', 'always', 'fresh.']\n",
            "Tokenized: ['[CLS]', 'the', 'staff', 'is', 'very', 'pleasant', 'and', 'my', 'hair', 'is', 'always', 'fresh', '.', '[SEP]']\n",
            "Original: ['Great', 'service']\n",
            "Tokenized: ['[CLS]', 'great', 'service', '[SEP]']\n",
            "Original: ['These', 'people', 'were', 'so', 'helpful', 'this', 'week', 'and', 'did', 'everything', 'to', 'sort', 'out', 'my', 'windscreen', 'and', 'insurance.']\n",
            "Tokenized: ['[CLS]', 'these', 'people', 'were', 'so', 'helpful', 'this', 'week', 'and', 'did', 'everything', 'to', 'sort', 'out', 'my', 'winds', '##creen', 'and', 'insurance', '.', '[SEP]']\n",
            "Original: ['It', 'was', 'all', 'sorted', 'with', 'no', 'hassle', 'at', 'all', 'and', \"I'm\", 'really', 'grateful', '-', 'they', 'were', 'fab.']\n",
            "Tokenized: ['[CLS]', 'it', 'was', 'all', 'sorted', 'with', 'no', 'has', '##sle', 'at', 'all', 'and', 'i', \"'\", 'm', 'really', 'grateful', '-', 'they', 'were', 'fa', '##b', '.', '[SEP]']\n",
            "Original: ['The', 'best', 'customer', 'service', \"I've\", 'come', 'across', 'for', 'long', 'time.']\n",
            "Tokenized: ['[CLS]', 'the', 'best', 'customer', 'service', 'i', \"'\", 've', 'come', 'across', 'for', 'long', 'time', '.', '[SEP]']\n",
            "Original: ['Cheap', 'Hotel', 'Rome', '-', 'Thanks', 'for', 'all', 'your', 'help!']\n",
            "Tokenized: ['[CLS]', 'cheap', 'hotel', 'rome', '-', 'thanks', 'for', 'all', 'your', 'help', '!', '[SEP]']\n",
            "Original: ['Cheap', 'Hotel', 'Rome', '-', 'thanks', 'for', 'finding', 'us', 'a', 'hotel', 'at', 'the', 'last', 'minute.']\n",
            "Tokenized: ['[CLS]', 'cheap', 'hotel', 'rome', '-', 'thanks', 'for', 'finding', 'us', 'a', 'hotel', 'at', 'the', 'last', 'minute', '.', '[SEP]']\n",
            "Original: ['We', 'had', 'a', 'great', 'stay,', 'your', 'service', 'was', 'excellent', 'and', 'we', 'will', 'use', 'you', 'again!']\n",
            "Tokenized: ['[CLS]', 'we', 'had', 'a', 'great', 'stay', ',', 'your', 'service', 'was', 'excellent', 'and', 'we', 'will', 'use', 'you', 'again', '!', '[SEP]']\n",
            "Original: ['For', 'cheap', 'Chinese', 'food,', 'this', 'is', 'the', 'place', 'to', 'go.']\n",
            "Tokenized: ['[CLS]', 'for', 'cheap', 'chinese', 'food', ',', 'this', 'is', 'the', 'place', 'to', 'go', '.', '[SEP]']\n",
            "Original: ['I', 'used', 'to', 'eat', 'at', 'places', 'like', 'New', 'China', 'or', 'Green', 'Buffet', 'in', 'Troy,', 'MO', '-', 'nothing', 'terrible', 'but', 'not', 'that', 'great.']\n",
            "Tokenized: ['[CLS]', 'i', 'used', 'to', 'eat', 'at', 'places', 'like', 'new', 'china', 'or', 'green', 'buffet', 'in', 'troy', ',', 'mo', '-', 'nothing', 'terrible', 'but', 'not', 'that', 'great', '.', '[SEP]']\n",
            "Original: ['Now,', 'I', \"won't\", 'eat', 'fast', 'food', 'Chinese', 'unless', \"it's\", 'from', 'this', 'place.']\n",
            "Tokenized: ['[CLS]', 'now', ',', 'i', 'won', \"'\", 't', 'eat', 'fast', 'food', 'chinese', 'unless', 'it', \"'\", 's', 'from', 'this', 'place', '.', '[SEP]']\n",
            "Original: ['The', 'best', 'value', \"I've\", 'found', 'from', 'a', 'Chinese', 'restaurant.']\n",
            "Tokenized: ['[CLS]', 'the', 'best', 'value', 'i', \"'\", 've', 'found', 'from', 'a', 'chinese', 'restaurant', '.', '[SEP]']\n",
            "Original: ['I', \"don't\", 'live', 'in', 'Lake', 'St.', 'Louis', 'anymore,', 'but', 'deliveries', 'were', 'always', 'correct', 'and', 'the', 'service', 'courteous.']\n",
            "Tokenized: ['[CLS]', 'i', 'don', \"'\", 't', 'live', 'in', 'lake', 'st', '.', 'louis', 'anymore', ',', 'but', 'deliveries', 'were', 'always', 'correct', 'and', 'the', 'service', 'court', '##eous', '.', '[SEP]']\n",
            "Original: ['Now', 'I', 'have', 'to', 'be', 'in', 'the', 'area', 'to', 'get', 'some', \"lovin'\", ':sadface:']\n",
            "Tokenized: ['[CLS]', 'now', 'i', 'have', 'to', 'be', 'in', 'the', 'area', 'to', 'get', 'some', 'lo', '##vin', \"'\", ':', 'sad', '##face', ':', '[SEP]']\n",
            "Original: ['My', '2004', 'x-type', 'was', 'getting', 'close', 'to', '100,000', 'miles', 'so', 'it', 'was', 'time', 'for', 'an', 'upgrade.']\n",
            "Tokenized: ['[CLS]', 'my', '2004', 'x', '-', 'type', 'was', 'getting', 'close', 'to', '100', ',', '000', 'miles', 'so', 'it', 'was', 'time', 'for', 'an', 'upgrade', '.', '[SEP]']\n",
            "Original: ['I', 'sent', 'my', 'wife', 'and', 'daughter', 'over', 'to', 'check', 'out', 'a', 'pre-owned', '2009', 'XF.']\n",
            "Tokenized: ['[CLS]', 'i', 'sent', 'my', 'wife', 'and', 'daughter', 'over', 'to', 'check', 'out', 'a', 'pre', '-', 'owned', '2009', 'x', '##f', '.', '[SEP]']\n",
            "Original: ['Michael', 'Chestney', 'was', 'very', 'pleasant', 'and', 'patient', 'with', 'my', 'wife', 'and', 'she', 'suggested', 'I', 'go', 'check', 'it', 'out.']\n",
            "Tokenized: ['[CLS]', 'michael', 'chest', '##ney', 'was', 'very', 'pleasant', 'and', 'patient', 'with', 'my', 'wife', 'and', 'she', 'suggested', 'i', 'go', 'check', 'it', 'out', '.', '[SEP]']\n",
            "Original: ['I', 'went', 'in', 'later', 'that', 'afternoon', 'and', 'met', 'with', 'Michael.']\n",
            "Tokenized: ['[CLS]', 'i', 'went', 'in', 'later', 'that', 'afternoon', 'and', 'met', 'with', 'michael', '.', '[SEP]']\n",
            "Original: ['He', 'showed', 'me', 'the', 'car', 'I', 'was', 'interested', 'in', 'and', 'we', 'took', 'a', 'test', 'drive.']\n",
            "Tokenized: ['[CLS]', 'he', 'showed', 'me', 'the', 'car', 'i', 'was', 'interested', 'in', 'and', 'we', 'took', 'a', 'test', 'drive', '.', '[SEP]']\n",
            "Original: ['I', 'loved', 'the', 'car', 'so', 'we', 'began', 'negotiating', 'my', 'trade-in', 'and', 'the', 'price', 'of', 'the', '09', 'XF.']\n",
            "Tokenized: ['[CLS]', 'i', 'loved', 'the', 'car', 'so', 'we', 'began', 'negotiating', 'my', 'trade', '-', 'in', 'and', 'the', 'price', 'of', 'the', '09', 'x', '##f', '.', '[SEP]']\n",
            "Original: ['The', 'entire', 'negotiation', 'took', 'about', '20', 'minutes.']\n",
            "Tokenized: ['[CLS]', 'the', 'entire', 'negotiation', 'took', 'about', '20', 'minutes', '.', '[SEP]']\n",
            "Original: ['Fair', 'give', 'and', 'take', 'on', 'both', 'sides', 'until', 'we', 'agreed', 'on', 'a', 'deal', 'that', 'was', 'within', 'my', 'parameters', 'and', 'was', 'fair', 'to', 'both', 'sides.']\n",
            "Tokenized: ['[CLS]', 'fair', 'give', 'and', 'take', 'on', 'both', 'sides', 'until', 'we', 'agreed', 'on', 'a', 'deal', 'that', 'was', 'within', 'my', 'parameters', 'and', 'was', 'fair', 'to', 'both', 'sides', '.', '[SEP]']\n",
            "Original: ['10', 'minutes', 'of', 'paperwork', 'and', 'I', 'was', 'the', 'owner', 'of', 'a', 'beautiful', 'pre-owned', '09', 'XF.']\n",
            "Tokenized: ['[CLS]', '10', 'minutes', 'of', 'paperwork', 'and', 'i', 'was', 'the', 'owner', 'of', 'a', 'beautiful', 'pre', '-', 'owned', '09', 'x', '##f', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'purchased', 'over', '15', 'vehicles', '(cars,', 'rvs,', 'and', 'boats)', 'in', 'my', 'lifetime', 'and', 'I', 'have', 'to', 'say', 'the', 'experience', 'with', 'Michael', 'and', 'Barrett', 'Motor', 'Cars', 'of', 'San', 'Antonio', 'was', 'one', 'of', 'the', 'best.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'purchased', 'over', '15', 'vehicles', '(', 'cars', ',', 'rv', '##s', ',', 'and', 'boats', ')', 'in', 'my', 'lifetime', 'and', 'i', 'have', 'to', 'say', 'the', 'experience', 'with', 'michael', 'and', 'barrett', 'motor', 'cars', 'of', 'san', 'antonio', 'was', 'one', 'of', 'the', 'best', '.', '[SEP]']\n",
            "Original: ['Friendly,', 'knowledgeable,', 'and', 'above', 'all', 'fair.']\n",
            "Tokenized: ['[CLS]', 'friendly', ',', 'knowledge', '##able', ',', 'and', 'above', 'all', 'fair', '.', '[SEP]']\n",
            "Original: [\"That's\", 'all', 'you', 'can', 'really', 'ask', 'from', 'a', 'car', 'dealer', 'and', 'Michael', 'and', 'Barrett', 'hit', 'all', '3.']\n",
            "Tokenized: ['[CLS]', 'that', \"'\", 's', 'all', 'you', 'can', 'really', 'ask', 'from', 'a', 'car', 'dealer', 'and', 'michael', 'and', 'barrett', 'hit', 'all', '3', '.', '[SEP]']\n",
            "Original: ['Thanks', 'for', 'the', 'great', 'deal', 'and', 'the', 'great', 'car!']\n",
            "Tokenized: ['[CLS]', 'thanks', 'for', 'the', 'great', 'deal', 'and', 'the', 'great', 'car', '!', '[SEP]']\n",
            "Original: ['Super', 'nice', 'people,', 'really', 'good', 'food']\n",
            "Tokenized: ['[CLS]', 'super', 'nice', 'people', ',', 'really', 'good', 'food', '[SEP]']\n",
            "Original: ['What', 'I', 'love', 'most', 'about', 'this', 'place,', 'other', 'than', 'the', 'food,', 'is', 'that', 'eating', 'here', 'makes', 'you', 'feel', 'like', \"you're\", 'in', 'a', 'small', 'town', 'rather', 'than', 'Baltimore.']\n",
            "Tokenized: ['[CLS]', 'what', 'i', 'love', 'most', 'about', 'this', 'place', ',', 'other', 'than', 'the', 'food', ',', 'is', 'that', 'eating', 'here', 'makes', 'you', 'feel', 'like', 'you', \"'\", 're', 'in', 'a', 'small', 'town', 'rather', 'than', 'baltimore', '.', '[SEP]']\n",
            "Original: ['The', 'owners', 'are', 'really', 'nice,', 'they', 'serve', 'good', 'food', 'at', 'a', 'good', 'price,', 'and', 'the', 'option', 'to', 'eat', 'outside', 'on', 'the', 'deck', '(esp', 'on', 'the', 'weekend', 'whether', 'there', 'is', 'hardly', 'any', 'traffic)', 'is', 'great.']\n",
            "Tokenized: ['[CLS]', 'the', 'owners', 'are', 'really', 'nice', ',', 'they', 'serve', 'good', 'food', 'at', 'a', 'good', 'price', ',', 'and', 'the', 'option', 'to', 'eat', 'outside', 'on', 'the', 'deck', '(', 'es', '##p', 'on', 'the', 'weekend', 'whether', 'there', 'is', 'hardly', 'any', 'traffic', ')', 'is', 'great', '.', '[SEP]']\n",
            "Original: ['One', 'of', 'my', 'top', '5', 'places', 'to', 'eat', 'in', 'Baltimore.']\n",
            "Tokenized: ['[CLS]', 'one', 'of', 'my', 'top', '5', 'places', 'to', 'eat', 'in', 'baltimore', '.', '[SEP]']\n",
            "Original: ['Run', 'for', 'the', 'hills', '...', \"you'll\", 'be', 'much', 'better', 'off!']\n",
            "Tokenized: ['[CLS]', 'run', 'for', 'the', 'hills', '.', '.', '.', 'you', \"'\", 'll', 'be', 'much', 'better', 'off', '!', '[SEP]']\n",
            "Original: ['One', 'night', 'was', 'too', 'much.']\n",
            "Tokenized: ['[CLS]', 'one', 'night', 'was', 'too', 'much', '.', '[SEP]']\n",
            "Original: ['First', 'room', 'had', 'used', 'tissues', 'next', 'to', 'the', 'bed', 'and', 'I', 'requested', 'it', 'be', 'rectified.']\n",
            "Tokenized: ['[CLS]', 'first', 'room', 'had', 'used', 'tissues', 'next', 'to', 'the', 'bed', 'and', 'i', 'requested', 'it', 'be', 'rec', '##ti', '##fied', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'then', 'moved', 'to', 'another', 'room', 'around', 'the', 'back', 'where', 'the', 'room', 'was', 'dirty,', 'the', 'shower', 'was', 'dirty', 'with', 'other', 'peoples', 'hair', 'in', 'it,', 'the', 'toilet', 'seat', 'was', 'peeling', 'and', 'rough', 'and', 'the', 'bathroom', 'was', 'full', 'of', 'mould.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'then', 'moved', 'to', 'another', 'room', 'around', 'the', 'back', 'where', 'the', 'room', 'was', 'dirty', ',', 'the', 'shower', 'was', 'dirty', 'with', 'other', 'peoples', 'hair', 'in', 'it', ',', 'the', 'toilet', 'seat', 'was', 'peeling', 'and', 'rough', 'and', 'the', 'bathroom', 'was', 'full', 'of', 'mo', '##uld', '.', '[SEP]']\n",
            "Original: ['I', 'called', 'reception', 'to', 'ask', 'if', 'they', 'knew', 'the', 'state', 'the', 'room', 'was', 'in', 'and', 'was', 'told', '\"This', 'is', 'a', 'Days', 'Inn,', 'not', 'the', 'Hilton\"', 'and', 'the', 'receptionist', 'then', 'hung', 'up', 'on', 'me.']\n",
            "Tokenized: ['[CLS]', 'i', 'called', 'reception', 'to', 'ask', 'if', 'they', 'knew', 'the', 'state', 'the', 'room', 'was', 'in', 'and', 'was', 'told', '\"', 'this', 'is', 'a', 'days', 'inn', ',', 'not', 'the', 'hilton', '\"', 'and', 'the', 'receptionist', 'then', 'hung', 'up', 'on', 'me', '.', '[SEP]']\n",
            "Original: ['To', 'warn', 'you', 'to', 'stay', 'away', 'from', 'this', 'place', 'just', \"isn't\", 'enough.']\n",
            "Tokenized: ['[CLS]', 'to', 'warn', 'you', 'to', 'stay', 'away', 'from', 'this', 'place', 'just', 'isn', \"'\", 't', 'enough', '.', '[SEP]']\n",
            "Original: ['There', 'was', 'not', 'one', 'ounce', 'of', 'caring', 'involved', 'and', 'anyone', 'I', 'can', 'warn', 'about', 'the', 'complete', 'lack', 'of', 'service', 'will', 'be', 'warned.']\n",
            "Tokenized: ['[CLS]', 'there', 'was', 'not', 'one', 'ounce', 'of', 'caring', 'involved', 'and', 'anyone', 'i', 'can', 'warn', 'about', 'the', 'complete', 'lack', 'of', 'service', 'will', 'be', 'warned', '.', '[SEP]']\n",
            "Original: ['Michael', 'helped', 'shoot', 'the', 'majority', 'of', 'my', \"firm's\", 'website', 'and', 'we', 'could', 'not', 'have', 'been', 'happier.']\n",
            "Tokenized: ['[CLS]', 'michael', 'helped', 'shoot', 'the', 'majority', 'of', 'my', 'firm', \"'\", 's', 'website', 'and', 'we', 'could', 'not', 'have', 'been', 'happier', '.', '[SEP]']\n",
            "Original: ['We', 'went', 'through', 'six', 'photographers', 'to', 'find', 'the', 'right', 'photographers', 'that', 'would', 'represent', 'our', 'firm', 'in', 'the', 'light', 'we', 'wished', 'to', 'and', 'Michael', 'and', 'his', 'team', 'made', 'that', 'happen.']\n",
            "Tokenized: ['[CLS]', 'we', 'went', 'through', 'six', 'photographers', 'to', 'find', 'the', 'right', 'photographers', 'that', 'would', 'represent', 'our', 'firm', 'in', 'the', 'light', 'we', 'wished', 'to', 'and', 'michael', 'and', 'his', 'team', 'made', 'that', 'happen', '.', '[SEP]']\n",
            "Original: [\"He's\", 'worth', 'every', 'penny.']\n",
            "Tokenized: ['[CLS]', 'he', \"'\", 's', 'worth', 'every', 'penny', '.', '[SEP]']\n",
            "Original: ['They', \"won't\", 'have', 'a', 'second', 'chance', 'from', 'me.']\n",
            "Tokenized: ['[CLS]', 'they', 'won', \"'\", 't', 'have', 'a', 'second', 'chance', 'from', 'me', '.', '[SEP]']\n",
            "Original: ['First,', 'let', 'me', 'state', 'that', 'although', 'I', 'live', 'in', 'NYC,', 'I', 'am', 'not', 'from', 'NYC,', 'I', \"don't\", 'care', 'about', 'baseball', 'and', 'I', 'absolutely', 'love', 'Boston', 'to', 'death', '(so', 'beautiful,', 'so', 'clean,', 'so', 'awesome).']\n",
            "Tokenized: ['[CLS]', 'first', ',', 'let', 'me', 'state', 'that', 'although', 'i', 'live', 'in', 'nyc', ',', 'i', 'am', 'not', 'from', 'nyc', ',', 'i', 'don', \"'\", 't', 'care', 'about', 'baseball', 'and', 'i', 'absolutely', 'love', 'boston', 'to', 'death', '(', 'so', 'beautiful', ',', 'so', 'clean', ',', 'so', 'awesome', ')', '.', '[SEP]']\n",
            "Original: ['That', 'said,', 'I', 'hated', 'this', 'restaurant.']\n",
            "Tokenized: ['[CLS]', 'that', 'said', ',', 'i', 'hated', 'this', 'restaurant', '.', '[SEP]']\n",
            "Original: ['The', 'service', 'was', 'just', 'about', 'as', 'good', 'as', \"I'd\", 'get', 'in', 'NYC', '(that', 'means', 'it', 'was', 'poor)', 'and', 'the', 'food', 'was', 'almost', 'mediocre.']\n",
            "Tokenized: ['[CLS]', 'the', 'service', 'was', 'just', 'about', 'as', 'good', 'as', 'i', \"'\", 'd', 'get', 'in', 'nyc', '(', 'that', 'means', 'it', 'was', 'poor', ')', 'and', 'the', 'food', 'was', 'almost', 'med', '##io', '##cre', '.', '[SEP]']\n",
            "Original: ['The', 'decor', 'left', 'a', 'lot', 'to', 'be', 'desired', 'and', 'the', 'posters', 'telling', 'me', 'all', 'the', 'reasons', '99', 'was', 'great', 'just', 'served', 'as', 'an', 'ironic', 'contrast', 'against', 'the', 'reality.']\n",
            "Tokenized: ['[CLS]', 'the', 'decor', 'left', 'a', 'lot', 'to', 'be', 'desired', 'and', 'the', 'posters', 'telling', 'me', 'all', 'the', 'reasons', '99', 'was', 'great', 'just', 'served', 'as', 'an', 'ironic', 'contrast', 'against', 'the', 'reality', '.', '[SEP]']\n",
            "Original: ['Personally', 'I', 'recommend', 'you', 'take', 'your', 'money', 'elsewhere']\n",
            "Tokenized: ['[CLS]', 'personally', 'i', 'recommend', 'you', 'take', 'your', 'money', 'elsewhere', '[SEP]']\n",
            "Original: ['The', 'pancakes', 'are', 'to', 'die', 'for.']\n",
            "Tokenized: ['[CLS]', 'the', 'pancakes', 'are', 'to', 'die', 'for', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'a', 'patient', 'a', 'NW', 'hospital', 'and', 'it', 'was', 'great.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'a', 'patient', 'a', 'nw', 'hospital', 'and', 'it', 'was', 'great', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'also', 'had', 'an', '80', 'yr', 'old', 'that', 'I', 'take', 'of', 'sent', 'to', 'the', 'ER', 'and', 'long', 'stays', 'in', 'the', 'hospital.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'also', 'had', 'an', '80', 'y', '##r', 'old', 'that', 'i', 'take', 'of', 'sent', 'to', 'the', 'er', 'and', 'long', 'stays', 'in', 'the', 'hospital', '.', '[SEP]']\n",
            "Original: ['Yes,', 'we', 'had', 'to', 'wait,', 'but', 'ER', 'is', 'a', 'triage', 'system', 'that', 'takes', 'the', 'most', 'life', 'threatening', 'cases', 'first.']\n",
            "Tokenized: ['[CLS]', 'yes', ',', 'we', 'had', 'to', 'wait', ',', 'but', 'er', 'is', 'a', 'tri', '##age', 'system', 'that', 'takes', 'the', 'most', 'life', 'threatening', 'cases', 'first', '.', '[SEP]']\n",
            "Original: ['Any', 'ER', 'would', 'be', 'the', 'same.']\n",
            "Tokenized: ['[CLS]', 'any', 'er', 'would', 'be', 'the', 'same', '.', '[SEP]']\n",
            "Original: ['As', 'far', 'as', 'being', 'treated', 'like', 'a', 'drug', 'seeker,', 'that', 'has', 'not', 'been', 'my', 'experience.']\n",
            "Tokenized: ['[CLS]', 'as', 'far', 'as', 'being', 'treated', 'like', 'a', 'drug', 'seeker', ',', 'that', 'has', 'not', 'been', 'my', 'experience', '.', '[SEP]']\n",
            "Original: ['As', 'a', 'nurse', 'I', 'know', 'about', 'drug', 'seekers.']\n",
            "Tokenized: ['[CLS]', 'as', 'a', 'nurse', 'i', 'know', 'about', 'drug', 'seekers', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'a', 'great', 'hospital', 'and', 'even', 'better', 'since', 'it', 'became', 'part', 'of', 'the', 'UW', 'system.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'a', 'great', 'hospital', 'and', 'even', 'better', 'since', 'it', 'became', 'part', 'of', 'the', 'u', '##w', 'system', '.', '[SEP]']\n",
            "Original: ['house', 'closing']\n",
            "Tokenized: ['[CLS]', 'house', 'closing', '[SEP]']\n",
            "Original: ['Mrs.', 'Tolchin', 'provided', 'us', 'with', 'excellent', 'service', 'and', 'came', 'with', 'a', 'great', 'deal', 'of', 'knowledge', 'and', 'professionalism!']\n",
            "Tokenized: ['[CLS]', 'mrs', '.', 'to', '##lch', '##in', 'provided', 'us', 'with', 'excellent', 'service', 'and', 'came', 'with', 'a', 'great', 'deal', 'of', 'knowledge', 'and', 'professional', '##ism', '!', '[SEP]']\n",
            "Original: ['Her', 'flexibility', 'and', 'accessibility', 'made', 'for', 'an', 'easy', 'closing.']\n",
            "Tokenized: ['[CLS]', 'her', 'flexibility', 'and', 'accessibility', 'made', 'for', 'an', 'easy', 'closing', '.', '[SEP]']\n",
            "Original: ['NEVER', 'fear', 'going', 'to', 'the', 'dentist', 'again!']\n",
            "Tokenized: ['[CLS]', 'never', 'fear', 'going', 'to', 'the', 'dentist', 'again', '!', '[SEP]']\n",
            "Original: [\"I'm\", '61', 'years', 'old', 'and', 'have', 'dental', 'problems', 'my', 'entire', 'life.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'm', '61', 'years', 'old', 'and', 'have', 'dental', 'problems', 'my', 'entire', 'life', '.', '[SEP]']\n",
            "Original: ['By', 'the', 'age', 'of', '24', 'I', 'stopped', 'going', 'to', 'the', 'dentist.']\n",
            "Tokenized: ['[CLS]', 'by', 'the', 'age', 'of', '24', 'i', 'stopped', 'going', 'to', 'the', 'dentist', '.', '[SEP]']\n",
            "Original: ['My', 'fear', 'and', 'discomfort', 'from', 'dental', 'work', 'scared', 'me!']\n",
            "Tokenized: ['[CLS]', 'my', 'fear', 'and', 'discomfort', 'from', 'dental', 'work', 'scared', 'me', '!', '[SEP]']\n",
            "Original: ['It', 'took', 'all', 'the', 'courage', 'I', 'could', 'muster', 'to', 'make', 'an', 'appointment.']\n",
            "Tokenized: ['[CLS]', 'it', 'took', 'all', 'the', 'courage', 'i', 'could', 'muster', 'to', 'make', 'an', 'appointment', '.', '[SEP]']\n",
            "Original: ['At', 'the', 'front', 'door', 'of', 'his', 'office,', 'I', 'nearly', 'turned', 'around.']\n",
            "Tokenized: ['[CLS]', 'at', 'the', 'front', 'door', 'of', 'his', 'office', ',', 'i', 'nearly', 'turned', 'around', '.', '[SEP]']\n",
            "Original: ['I', 'considered', 'just', 'leaving', 'after', 'going', 'inside', 'and', 'nearly', 'did.']\n",
            "Tokenized: ['[CLS]', 'i', 'considered', 'just', 'leaving', 'after', 'going', 'inside', 'and', 'nearly', 'did', '.', '[SEP]']\n",
            "Original: ['Doctor', 'Gonzales', 'and', 'his', 'entire', 'staff', 'are', 'the', 'most', 'professional', 'people', 'I', 'have', 'ever', 'dealt', 'with.']\n",
            "Tokenized: ['[CLS]', 'doctor', 'gonzales', 'and', 'his', 'entire', 'staff', 'are', 'the', 'most', 'professional', 'people', 'i', 'have', 'ever', 'dealt', 'with', '.', '[SEP]']\n",
            "Original: ['They', 'made', 'me', 'feel', 'confident', 'in', 'what', 'they', 'would', 'do,', 'and', 'treated', 'me', 'like', 'a', 'member', 'of', 'their', 'own', 'family.']\n",
            "Tokenized: ['[CLS]', 'they', 'made', 'me', 'feel', 'confident', 'in', 'what', 'they', 'would', 'do', ',', 'and', 'treated', 'me', 'like', 'a', 'member', 'of', 'their', 'own', 'family', '.', '[SEP]']\n",
            "Original: ['I', 'never', 'felt', 'pain', 'or', 'discomfort.']\n",
            "Tokenized: ['[CLS]', 'i', 'never', 'felt', 'pain', 'or', 'discomfort', '.', '[SEP]']\n",
            "Original: ['The', 'ability', 'to', 'smile', 'and', 'eat', 'again', 'can', 'only', 'be', 'described', 'as', 'a', 'whole', 'new', 'lease', 'on', 'life.']\n",
            "Tokenized: ['[CLS]', 'the', 'ability', 'to', 'smile', 'and', 'eat', 'again', 'can', 'only', 'be', 'described', 'as', 'a', 'whole', 'new', 'lease', 'on', 'life', '.', '[SEP]']\n",
            "Original: ['Thank', 'you', 'Doctor', 'Gonzales,', 'Doctor', 'Stout,', 'Eva', 'Marie', 'and', 'the', 'entire', 'staff!']\n",
            "Tokenized: ['[CLS]', 'thank', 'you', 'doctor', 'gonzales', ',', 'doctor', 'stout', ',', 'eva', 'marie', 'and', 'the', 'entire', 'staff', '!', '[SEP]']\n",
            "Original: ['Rocky', 'M.', 'Lange', 'Retired', 'Coordinator,', 'Clark', 'County', 'School', 'District']\n",
            "Tokenized: ['[CLS]', 'rocky', 'm', '.', 'lange', 'retired', 'coordinator', ',', 'clark', 'county', 'school', 'district', '[SEP]']\n",
            "Original: ['this', 'is', 'the', 'worst', 'Sams', 'club', \"I've\", 'ever', 'been', 'to']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'the', 'worst', 'sam', '##s', 'club', 'i', \"'\", 've', 'ever', 'been', 'to', '[SEP]']\n",
            "Original: ['what', 'a', 'mindblowing', 'servicing']\n",
            "Tokenized: ['[CLS]', 'what', 'a', 'mind', '##bl', '##owing', 'servicing', '[SEP]']\n",
            "Original: ['They', 'treat', 'there', 'employees', 'with', 'respect', 'and', 'concern', 'and', 'expect', 'that', 'they', 'will', 'extend', 'the', 'same', 'politeness', 'to', 'there', 'customers.']\n",
            "Tokenized: ['[CLS]', 'they', 'treat', 'there', 'employees', 'with', 'respect', 'and', 'concern', 'and', 'expect', 'that', 'they', 'will', 'extend', 'the', 'same', 'polite', '##ness', 'to', 'there', 'customers', '.', '[SEP]']\n",
            "Original: ['ipad', 'reiew']\n",
            "Tokenized: ['[CLS]', 'ipad', 'rei', '##ew', '[SEP]']\n",
            "Original: ['they', 'are', 'the', 'best', 'orthodontics', 'in', 'the', 'world.']\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'the', 'best', 'or', '##th', '##odon', '##tics', 'in', 'the', 'world', '.', '[SEP]']\n",
            "Original: ['i', 'went', 'there', 'since', 'i', 'was', 'four.']\n",
            "Tokenized: ['[CLS]', 'i', 'went', 'there', 'since', 'i', 'was', 'four', '.', '[SEP]']\n",
            "Original: ['now', 'i', 'will', 'have', 'really', 'straight', 'teeth.']\n",
            "Tokenized: ['[CLS]', 'now', 'i', 'will', 'have', 'really', 'straight', 'teeth', '.', '[SEP]']\n",
            "Original: ['Great', 'store', 'great', 'products']\n",
            "Tokenized: ['[CLS]', 'great', 'store', 'great', 'products', '[SEP]']\n",
            "Original: ['best', 'place', 'for', 'snowboard', 'eva.']\n",
            "Tokenized: ['[CLS]', 'best', 'place', 'for', 'snow', '##board', 'eva', '.', '[SEP]']\n",
            "Original: ['Great', 'Service']\n",
            "Tokenized: ['[CLS]', 'great', 'service', '[SEP]']\n",
            "Original: ['Dr', 'Mcdonald', 'is', 'wonderful.']\n",
            "Tokenized: ['[CLS]', 'dr', 'mcdonald', 'is', 'wonderful', '.', '[SEP]']\n",
            "Original: ['She', 'answers', 'all', 'questions', 'asked', 'and', 'provides', 'the', 'best', 'service', 'i', 'have', 'ever', 'seen.']\n",
            "Tokenized: ['[CLS]', 'she', 'answers', 'all', 'questions', 'asked', 'and', 'provides', 'the', 'best', 'service', 'i', 'have', 'ever', 'seen', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'a', 'new', 'born', 'daughter', 'and', 'she', 'helped', 'me', 'with', 'a', 'lot.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'a', 'new', 'born', 'daughter', 'and', 'she', 'helped', 'me', 'with', 'a', 'lot', '.', '[SEP]']\n",
            "Original: ['Good', 'Job', 'DR.']\n",
            "Tokenized: ['[CLS]', 'good', 'job', 'dr', '.', '[SEP]']\n",
            "Original: ['Yeah', 'they', 'ruined', 'some', 'shirts', 'I', 'had', 'too.']\n",
            "Tokenized: ['[CLS]', 'yeah', 'they', 'ruined', 'some', 'shirts', 'i', 'had', 'too', '.', '[SEP]']\n",
            "Original: ['Horrible!']\n",
            "Tokenized: ['[CLS]', 'horrible', '!', '[SEP]']\n",
            "Original: ['This', 'is', 'a', \"Ralph's\"]\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'a', 'ralph', \"'\", 's', '[SEP]']\n",
            "Original: ['I', 'just', 'called', 'this', 'number', 'and', 'it', 'is', 'a', \"Ralph's\", 'Market.']\n",
            "Tokenized: ['[CLS]', 'i', 'just', 'called', 'this', 'number', 'and', 'it', 'is', 'a', 'ralph', \"'\", 's', 'market', '.', '[SEP]']\n",
            "Original: ['Brain', 'Dead']\n",
            "Tokenized: ['[CLS]', 'brain', 'dead', '[SEP]']\n",
            "Original: ['Not', 'only', 'are', 'these', 'people', 'completely', 'inefficient', 'and', 'ineffective,', 'but', 'they', 'just', \"don't\", 'give', 'a', 'darn.']\n",
            "Tokenized: ['[CLS]', 'not', 'only', 'are', 'these', 'people', 'completely', 'in', '##ef', '##fi', '##cie', '##nt', 'and', 'ineffective', ',', 'but', 'they', 'just', 'don', \"'\", 't', 'give', 'a', 'dar', '##n', '.', '[SEP]']\n",
            "Original: ['This', 'place', 'is', 'a', 'complete', 'embarrassment.']\n",
            "Tokenized: ['[CLS]', 'this', 'place', 'is', 'a', 'complete', 'embarrassment', '.', '[SEP]']\n",
            "Original: ['Excellent', 'medical', 'care!!!!!!']\n",
            "Tokenized: ['[CLS]', 'excellent', 'medical', 'care', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['Highly', 'recommend']\n",
            "Tokenized: ['[CLS]', 'highly', 'recommend', '[SEP]']\n",
            "Original: ['I', 'went', 'to', 'this', 'urgent', 'care', 'center', 'and', 'was', 'blown', 'away', 'with', 'their', 'service.']\n",
            "Tokenized: ['[CLS]', 'i', 'went', 'to', 'this', 'urgent', 'care', 'center', 'and', 'was', 'blown', 'away', 'with', 'their', 'service', '.', '[SEP]']\n",
            "Original: ['Finally', 'a', 'convenient', 'place', 'close', 'to', 'home.']\n",
            "Tokenized: ['[CLS]', 'finally', 'a', 'convenient', 'place', 'close', 'to', 'home', '.', '[SEP]']\n",
            "Original: ['Wonderful', 'staff', 'and', 'physician.']\n",
            "Tokenized: ['[CLS]', 'wonderful', 'staff', 'and', 'physician', '.', '[SEP]']\n",
            "Original: ['Clean', 'and', 'superb.']\n",
            "Tokenized: ['[CLS]', 'clean', 'and', 'superb', '.', '[SEP]']\n",
            "Original: ['Thanks', 'for', 'the', 'great', 'care!!!!']\n",
            "Tokenized: ['[CLS]', 'thanks', 'for', 'the', 'great', 'care', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['Will', 'definitely', 'go', 'back', 'when', 'I', 'need', 'medical', 'care.']\n",
            "Tokenized: ['[CLS]', 'will', 'definitely', 'go', 'back', 'when', 'i', 'need', 'medical', 'care', '.', '[SEP]']\n",
            "Original: ['Wonderful', 'Experience']\n",
            "Tokenized: ['[CLS]', 'wonderful', 'experience', '[SEP]']\n",
            "Original: ['I', 'had', 'immense', 'pain', 'on', 'a', 'Sunday', 'morning,', 'with', 'friends', 'and', 'family', 'telling', 'me', 'that', 'I', 'would', 'never', 'find', 'a', 'Dentist', 'on', 'a', 'Sunday.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'immense', 'pain', 'on', 'a', 'sunday', 'morning', ',', 'with', 'friends', 'and', 'family', 'telling', 'me', 'that', 'i', 'would', 'never', 'find', 'a', 'dentist', 'on', 'a', 'sunday', '.', '[SEP]']\n",
            "Original: ['Dr.', 'Taylor', 'was', 'not', 'only', 'available', 'on', 'a', 'Sunday,', 'but', 'also', 'was', 'able', 'to', 'immediately', 'take', 'care', 'of', 'me.']\n",
            "Tokenized: ['[CLS]', 'dr', '.', 'taylor', 'was', 'not', 'only', 'available', 'on', 'a', 'sunday', ',', 'but', 'also', 'was', 'able', 'to', 'immediately', 'take', 'care', 'of', 'me', '.', '[SEP]']\n",
            "Original: ['He', 'was', 'incredibly', 'informative', 'about', 'the', 'options', 'I', 'had,', 'giving', 'me', 'opinions', 'on', 'different', 'treatments', 'to', 'choose', 'from.']\n",
            "Tokenized: ['[CLS]', 'he', 'was', 'incredibly', 'inform', '##ative', 'about', 'the', 'options', 'i', 'had', ',', 'giving', 'me', 'opinions', 'on', 'different', 'treatments', 'to', 'choose', 'from', '.', '[SEP]']\n",
            "Original: ['Coming', 'from', 'a', 'person', 'who', 'hates', 'the', 'dentist', 'in', 'general,', 'Dr.', 'Taylor', 'was', 'the', 'best!']\n",
            "Tokenized: ['[CLS]', 'coming', 'from', 'a', 'person', 'who', 'hates', 'the', 'dentist', 'in', 'general', ',', 'dr', '.', 'taylor', 'was', 'the', 'best', '!', '[SEP]']\n",
            "Original: ['He', 'really', 'made', 'the', 'visit', 'a', 'pain', 'free', 'one', 'with', 'excellent', 'service!']\n",
            "Tokenized: ['[CLS]', 'he', 'really', 'made', 'the', 'visit', 'a', 'pain', 'free', 'one', 'with', 'excellent', 'service', '!', '[SEP]']\n",
            "Original: ['I', 'would', 'recommend', 'him', 'to', 'everyone!']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'recommend', 'him', 'to', 'everyone', '!', '[SEP]']\n",
            "Original: ['Winning', 'Attorney!']\n",
            "Tokenized: ['[CLS]', 'winning', 'attorney', '!', '[SEP]']\n",
            "Original: ['The', 'only', '10.0', '\"Perfect', 'Score\"', 'AVVO', 'Rated', 'Attorney', 'I', 'Have', 'Ever', 'Met.']\n",
            "Tokenized: ['[CLS]', 'the', 'only', '10', '.', '0', '\"', 'perfect', 'score', '\"', 'av', '##vo', 'rated', 'attorney', 'i', 'have', 'ever', 'met', '.', '[SEP]']\n",
            "Original: ['I', 'Highly', 'Recommend,', 'The', 'Law', 'Offices', 'Of', 'Dale', 'Gribow!!']\n",
            "Tokenized: ['[CLS]', 'i', 'highly', 'recommend', ',', 'the', 'law', 'offices', 'of', 'dale', 'gr', '##ib', '##ow', '!', '!', '[SEP]']\n",
            "Original: ['Be', 'Careful', 'Of', 'Who', 'Your', 'Sales', 'Guy', 'Is']\n",
            "Tokenized: ['[CLS]', 'be', 'careful', 'of', 'who', 'your', 'sales', 'guy', 'is', '[SEP]']\n",
            "Original: ['I', 'think', 'this', 'place', 'is', 'probably', 'really', 'great', 'especially', 'judging', 'by', 'the', 'reviews', 'on', 'here.']\n",
            "Tokenized: ['[CLS]', 'i', 'think', 'this', 'place', 'is', 'probably', 'really', 'great', 'especially', 'judging', 'by', 'the', 'reviews', 'on', 'here', '.', '[SEP]']\n",
            "Original: ['My', 'experience', 'was', 'awful', 'though.']\n",
            "Tokenized: ['[CLS]', 'my', 'experience', 'was', 'awful', 'though', '.', '[SEP]']\n",
            "Original: ['It', 'ALL', 'had', 'to', 'do', 'with', 'the', 'sales', 'guy', 'which', 'was', 'a', 'young', '22', 'year', 'old', 'who', 'had', 'admittedly', 'only', 'been', 'working', 'for', '2', 'weeks.']\n",
            "Tokenized: ['[CLS]', 'it', 'all', 'had', 'to', 'do', 'with', 'the', 'sales', 'guy', 'which', 'was', 'a', 'young', '22', 'year', 'old', 'who', 'had', 'admitted', '##ly', 'only', 'been', 'working', 'for', '2', 'weeks', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'extremely', 'interested', 'in', 'the', 'car', 'and', 'very', 'likely', 'would', 'have', 'bought', 'it,', 'but', 'the', 'sales', 'guy', 'I', 'dealt', 'with', 'ruined', 'the', 'deal.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'extremely', 'interested', 'in', 'the', 'car', 'and', 'very', 'likely', 'would', 'have', 'bought', 'it', ',', 'but', 'the', 'sales', 'guy', 'i', 'dealt', 'with', 'ruined', 'the', 'deal', '.', '[SEP]']\n",
            "Original: ['Essentially,', 'I', 'told', 'him', 'I', \"didn't\", 'trust', 'him', 'cause', 'he', 'was', 'a', 'car', 'salesman,', 'but', 'he', 'got', 'so', 'incredibly', 'offended', 'at', 'that', 'statement', 'that', 'he', 'had', 'to', 'go', 'cry', 'to', 'another', 'salesman', 'and', 'compose', 'himself', 'before', 'coming', 'back.']\n",
            "Tokenized: ['[CLS]', 'essentially', ',', 'i', 'told', 'him', 'i', 'didn', \"'\", 't', 'trust', 'him', 'cause', 'he', 'was', 'a', 'car', 'salesman', ',', 'but', 'he', 'got', 'so', 'incredibly', 'offended', 'at', 'that', 'statement', 'that', 'he', 'had', 'to', 'go', 'cry', 'to', 'another', 'salesman', 'and', 'compose', 'himself', 'before', 'coming', 'back', '.', '[SEP]']\n",
            "Original: ['I', \"don't\", 'know', 'if', 'the', 'kid', 'had', 'a', 'bad', 'day', 'or', 'what,', 'but', 'I', 'had', 'to', 'sit', 'and', 'apologize', 'about', 'nothing', 'for', '10', 'minutes', 'until', 'he', 'dropped', 'the', 'issue.']\n",
            "Tokenized: ['[CLS]', 'i', 'don', \"'\", 't', 'know', 'if', 'the', 'kid', 'had', 'a', 'bad', 'day', 'or', 'what', ',', 'but', 'i', 'had', 'to', 'sit', 'and', 'apologize', 'about', 'nothing', 'for', '10', 'minutes', 'until', 'he', 'dropped', 'the', 'issue', '.', '[SEP]']\n",
            "Original: ['After', 'that,', 'I', 'just', 'tried', 'to', 'ignore', 'his', 'lack', 'of', 'professionalism', 'and', 'test', 'drive', 'the', 'car.']\n",
            "Tokenized: ['[CLS]', 'after', 'that', ',', 'i', 'just', 'tried', 'to', 'ignore', 'his', 'lack', 'of', 'professional', '##ism', 'and', 'test', 'drive', 'the', 'car', '.', '[SEP]']\n",
            "Original: ['I', 'played', 'dumb', 'and', 'asked', 'him', 'questions', 'that', 'I', 'already', 'knew', 'the', 'answers', 'to', 'and', 'he', 'responded', 'with', 'half', 'truths', 'and', 'a', 'few', 'falsehoods.']\n",
            "Tokenized: ['[CLS]', 'i', 'played', 'dumb', 'and', 'asked', 'him', 'questions', 'that', 'i', 'already', 'knew', 'the', 'answers', 'to', 'and', 'he', 'responded', 'with', 'half', 'truths', 'and', 'a', 'few', 'false', '##hood', '##s', '.', '[SEP]']\n",
            "Original: ['For', 'instance', 'I', 'asked', 'who', 'owned', 'Mazda', 'and', 'he', 'said', 'with', 'confidence', 'that', 'was', 'GM,', 'which', \"isn't\", 'true.']\n",
            "Tokenized: ['[CLS]', 'for', 'instance', 'i', 'asked', 'who', 'owned', 'mazda', 'and', 'he', 'said', 'with', 'confidence', 'that', 'was', 'gm', ',', 'which', 'isn', \"'\", 't', 'true', '.', '[SEP]']\n",
            "Original: ['I', 'mean,', 'I', \"don't\", 'care', 'if', 'he', \"doesn't\", 'know,', 'but', 'if', 'he', 'pretends', 'to', 'know', 'and', 'tells', 'me', 'BS', 'to', 'my', 'face,', \"there's\", 'no', 'way', \"I'm\", 'going', 'to', 'trust', 'him', 'when', 'matters', 'turn', 'to', 'the', 'price', 'of', 'the', 'car', 'and', 'financing.']\n",
            "Tokenized: ['[CLS]', 'i', 'mean', ',', 'i', 'don', \"'\", 't', 'care', 'if', 'he', 'doesn', \"'\", 't', 'know', ',', 'but', 'if', 'he', 'pretend', '##s', 'to', 'know', 'and', 'tells', 'me', 'bs', 'to', 'my', 'face', ',', 'there', \"'\", 's', 'no', 'way', 'i', \"'\", 'm', 'going', 'to', 'trust', 'him', 'when', 'matters', 'turn', 'to', 'the', 'price', 'of', 'the', 'car', 'and', 'financing', '.', '[SEP]']\n",
            "Original: ['the', '2010', 'Genesis', 'is', 'grrrrrrrreeeaaat!']\n",
            "Tokenized: ['[CLS]', 'the', '2010', 'genesis', 'is', 'gr', '##rr', '##rr', '##rr', '##ree', '##ea', '##aa', '##t', '!', '[SEP]']\n",
            "Original: ['I', 'am', 'a', 'proud', 'owner', 'of', 'a', 'brand', 'new', '2010', 'Hyundai', 'Genesis.']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'a', 'proud', 'owner', 'of', 'a', 'brand', 'new', '2010', 'hyundai', 'genesis', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'never', 'seen', 'this', 'car', 'before', 'until', 'this', 'lady', 'at', 'wal', 'mart', 'had', 'it', 'and', 'she', 'told', 'me', 'she', 'got', 'it', 'here', 'and', 'that', 'everyone', 'was', 'so', 'nice', 'to', 'her.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'never', 'seen', 'this', 'car', 'before', 'until', 'this', 'lady', 'at', 'wal', 'mart', 'had', 'it', 'and', 'she', 'told', 'me', 'she', 'got', 'it', 'here', 'and', 'that', 'everyone', 'was', 'so', 'nice', 'to', 'her', '.', '[SEP]']\n",
            "Original: ['I', 'asked', 'her', 'who', 'she', 'worked', 'with', 'and', 'she', 'just', 'told', 'me', 'ti', 'was', 'the', 'sales', 'manager.']\n",
            "Tokenized: ['[CLS]', 'i', 'asked', 'her', 'who', 'she', 'worked', 'with', 'and', 'she', 'just', 'told', 'me', 'ti', 'was', 'the', 'sales', 'manager', '.', '[SEP]']\n",
            "Original: ['I', 'took', 'the', 'weekend', 'off', 'and', 'came', 'in', 'and', 'asked', 'for', 'the', 'manager', 'who', 'is', 'Jeff', 'and', 'he', 'remembered', 'her', 'right', 'away', 'even', 'remembered', 'her', 'dog,', 'I', 'was', 'a', 'bit', 'shocked', 'that', 'someone', 'would', 'pay', 'that', 'close', 'attention.']\n",
            "Tokenized: ['[CLS]', 'i', 'took', 'the', 'weekend', 'off', 'and', 'came', 'in', 'and', 'asked', 'for', 'the', 'manager', 'who', 'is', 'jeff', 'and', 'he', 'remembered', 'her', 'right', 'away', 'even', 'remembered', 'her', 'dog', ',', 'i', 'was', 'a', 'bit', 'shocked', 'that', 'someone', 'would', 'pay', 'that', 'close', 'attention', '.', '[SEP]']\n",
            "Original: ['We', 'got', 'to', 'talking', 'and', 'he', 'got', 'me', 'set', 'up', 'and', 'I', 'test', 'drove', 'with', 'Craig', 'and', 'I', 'fell', 'head', 'over', 'heels', 'for', 'this', 'car', 'all', 'I', 'kept', 'saying,', '\"', 'was', 'I', 'gotta', 'have', 'it!\"']\n",
            "Tokenized: ['[CLS]', 'we', 'got', 'to', 'talking', 'and', 'he', 'got', 'me', 'set', 'up', 'and', 'i', 'test', 'drove', 'with', 'craig', 'and', 'i', 'fell', 'head', 'over', 'heels', 'for', 'this', 'car', 'all', 'i', 'kept', 'saying', ',', '\"', 'was', 'i', 'gotta', 'have', 'it', '!', '\"', '[SEP]']\n",
            "Original: ['I', 'thouhgt', 'it', 'would', 'be', 'out', 'of', 'my', 'price', 'range', 'but', 'they', 'really', 'worked', 'with', 'me', 'and', 'now', 'I', 'couldnt', 'be', 'happier.']\n",
            "Tokenized: ['[CLS]', 'i', 'thou', '##hg', '##t', 'it', 'would', 'be', 'out', 'of', 'my', 'price', 'range', 'but', 'they', 'really', 'worked', 'with', 'me', 'and', 'now', 'i', 'couldn', '##t', 'be', 'happier', '.', '[SEP]']\n",
            "Original: ['Jeff', 'and', 'Craig', 'are', 'really', 'good', 'at', 'what', 'they', 'do', 'and', 'know', 'exactly', 'how', 'to', 'treat', 'a', 'customer.']\n",
            "Tokenized: ['[CLS]', 'jeff', 'and', 'craig', 'are', 'really', 'good', 'at', 'what', 'they', 'do', 'and', 'know', 'exactly', 'how', 'to', 'treat', 'a', 'customer', '.', '[SEP]']\n",
            "Original: ['Definitely', 'go', 'see', 'them!']\n",
            "Tokenized: ['[CLS]', 'definitely', 'go', 'see', 'them', '!', '[SEP]']\n",
            "Original: ['Prominent', 'Builders', 'NJ']\n",
            "Tokenized: ['[CLS]', 'prominent', 'builders', 'nj', '[SEP]']\n",
            "Original: ['Prominent', 'Builders', 'in', 'New', 'Jersey', 'are', 'one', 'the', 'best', 'building', 'contractors,', 'I', 'was', 'referred', 'to', 'them', 'by', 'my', 'friend,', 'I', 'am', 'so', 'glad', 'I', 'used', 'them', 'for', 'my', 'Home', 'renovation,', 'and', 'addition.']\n",
            "Tokenized: ['[CLS]', 'prominent', 'builders', 'in', 'new', 'jersey', 'are', 'one', 'the', 'best', 'building', 'contractors', ',', 'i', 'was', 'referred', 'to', 'them', 'by', 'my', 'friend', ',', 'i', 'am', 'so', 'glad', 'i', 'used', 'them', 'for', 'my', 'home', 'renovation', ',', 'and', 'addition', '.', '[SEP]']\n",
            "Original: ['They', 'were', 'very', 'professional,', 'respectful,', 'completed', 'the', 'Job', 'on', 'time,', 'and', 'well', 'below', 'my', 'budget.']\n",
            "Tokenized: ['[CLS]', 'they', 'were', 'very', 'professional', ',', 'respectful', ',', 'completed', 'the', 'job', 'on', 'time', ',', 'and', 'well', 'below', 'my', 'budget', '.', '[SEP]']\n",
            "Original: ['Mike', 'one', 'of', 'owners', 'was', 'awesome,', 'he', 'explained', 'the', 'detailed', 'plan,', 'and', 'executed', 'on', 'time,', 'I', 'am', 'always', 'going', 'use', 'them', 'and', 'refer', 'them', 'to', 'many', 'friends', 'I', 'can', 'because', 'of', 'the', 'great', 'job', 'they', 'did', 'me.']\n",
            "Tokenized: ['[CLS]', 'mike', 'one', 'of', 'owners', 'was', 'awesome', ',', 'he', 'explained', 'the', 'detailed', 'plan', ',', 'and', 'executed', 'on', 'time', ',', 'i', 'am', 'always', 'going', 'use', 'them', 'and', 'refer', 'them', 'to', 'many', 'friends', 'i', 'can', 'because', 'of', 'the', 'great', 'job', 'they', 'did', 'me', '.', '[SEP]']\n",
            "Original: ['DO', 'NOT', 'GO', 'HERE!!!']\n",
            "Tokenized: ['[CLS]', 'do', 'not', 'go', 'here', '!', '!', '!', '[SEP]']\n",
            "Original: ['I', 'went', 'to', 'get', 'my', 'nails', 'filled', 'Friday,', 'by', 'Monday', '2', 'were', 'broken.']\n",
            "Tokenized: ['[CLS]', 'i', 'went', 'to', 'get', 'my', 'nails', 'filled', 'friday', ',', 'by', 'monday', '2', 'were', 'broken', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'not', 'happy', 'with', 'the', 'way', 'they', 'looked,', 'very', 'wavy,', 'uneven', 'edges,', 'and', 'with', 'the', 'exception', 'of', '1,', 'there', 'is', 'a', 'dip', 'in', 'the', 'center', 'of', 'each', 'nail.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'not', 'happy', 'with', 'the', 'way', 'they', 'looked', ',', 'very', 'wavy', ',', 'uneven', 'edges', ',', 'and', 'with', 'the', 'exception', 'of', '1', ',', 'there', 'is', 'a', 'dip', 'in', 'the', 'center', 'of', 'each', 'nail', '.', '[SEP]']\n",
            "Original: ['I', 'also', 'had', 'a', 'pedicure,', 'and', 'they', 'cut', 'my', 'nails', 'too', 'short,', 'one', 'of', 'my', 'big', 'toes', 'looks', 'like', 'its', 'getting', 'infected.']\n",
            "Tokenized: ['[CLS]', 'i', 'also', 'had', 'a', 'pe', '##dic', '##ure', ',', 'and', 'they', 'cut', 'my', 'nails', 'too', 'short', ',', 'one', 'of', 'my', 'big', 'toes', 'looks', 'like', 'its', 'getting', 'infected', '.', '[SEP]']\n",
            "Original: ['An', 'Asset', 'to', 'Richmond']\n",
            "Tokenized: ['[CLS]', 'an', 'asset', 'to', 'richmond', '[SEP]']\n",
            "Original: ['For', 'a', 'long', 'time,', 'one', 'big', 'source', 'of', 'entertainment', 'missing', 'from', 'the', 'Richmond', 'City', 'Limits', 'was', 'a', 'first-run', 'multiplex.']\n",
            "Tokenized: ['[CLS]', 'for', 'a', 'long', 'time', ',', 'one', 'big', 'source', 'of', 'entertainment', 'missing', 'from', 'the', 'richmond', 'city', 'limits', 'was', 'a', 'first', '-', 'run', 'multiple', '##x', '.', '[SEP]']\n",
            "Original: ['Bowtie', 'has', 'filled', 'that', 'role', 'nicely.']\n",
            "Tokenized: ['[CLS]', 'bow', '##tie', 'has', 'filled', 'that', 'role', 'nicely', '.', '[SEP]']\n",
            "Original: ['The', 'theatre', 'is', 'in', 'an', 'old', 'brick', 'warehouse,', 'which', 'is', 'a', 'block', 'away', 'from', 'the', 'Flying', 'Squirrels', 'stadium.']\n",
            "Tokenized: ['[CLS]', 'the', 'theatre', 'is', 'in', 'an', 'old', 'brick', 'warehouse', ',', 'which', 'is', 'a', 'block', 'away', 'from', 'the', 'flying', 'squirrels', 'stadium', '.', '[SEP]']\n",
            "Original: ['The', 'owners', 'of', 'Bowtie', 'manged', 'to', 'add', 'something', 'new', 'while', 'preserving', 'what', 'was', 'already', 'there.']\n",
            "Tokenized: ['[CLS]', 'the', 'owners', 'of', 'bow', '##tie', 'man', '##ged', 'to', 'add', 'something', 'new', 'while', 'preserving', 'what', 'was', 'already', 'there', '.', '[SEP]']\n",
            "Original: ['The', 'theatre', 'has', 'greatly', 'improved', 'the', 'surrounding', 'neighborhood.']\n",
            "Tokenized: ['[CLS]', 'the', 'theatre', 'has', 'greatly', 'improved', 'the', 'surrounding', 'neighborhood', '.', '[SEP]']\n",
            "Original: [\"There's\", 'plenty', 'of', 'parking,', 'and', \"I've\", 'never', 'had', 'an', 'issue', 'with', 'audience', 'members', 'who', \"won't\", 'stop', 'talking', 'or', 'answering', 'their', 'cellphones.']\n",
            "Tokenized: ['[CLS]', 'there', \"'\", 's', 'plenty', 'of', 'parking', ',', 'and', 'i', \"'\", 've', 'never', 'had', 'an', 'issue', 'with', 'audience', 'members', 'who', 'won', \"'\", 't', 'stop', 'talking', 'or', 'answering', 'their', 'cell', '##phones', '.', '[SEP]']\n",
            "Original: ['Best', 'of', 'all,', 'there', 'are', 'no', 'ads!!!!!']\n",
            "Tokenized: ['[CLS]', 'best', 'of', 'all', ',', 'there', 'are', 'no', 'ads', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['Just', 'previews', 'and', 'the', 'main', 'feature.']\n",
            "Tokenized: ['[CLS]', 'just', 'preview', '##s', 'and', 'the', 'main', 'feature', '.', '[SEP]']\n",
            "Original: ['Expect', 'to', 'pay', 'full', 'price,', 'and', 'the', 'theatres', 'themselves', 'are', 'on', 'the', 'small', 'side.']\n",
            "Tokenized: ['[CLS]', 'expect', 'to', 'pay', 'full', 'price', ',', 'and', 'the', 'theatres', 'themselves', 'are', 'on', 'the', 'small', 'side', '.', '[SEP]']\n",
            "Original: ['But', 'if', 'you', 'like', 'going', 'to', 'the', 'movies,', 'you', 'should', 'love', 'Bowtie.']\n",
            "Tokenized: ['[CLS]', 'but', 'if', 'you', 'like', 'going', 'to', 'the', 'movies', ',', 'you', 'should', 'love', 'bow', '##tie', '.', '[SEP]']\n",
            "Original: ['The', 'mangers', 'also', 'play', 'a', 'classic', 'movies', 'on', 'Sundays,', 'you', 'can', 'get', 'a', 'beer', 'in', 'the', 'lobby,', 'and', 'repeat', 'customers', 'can', 'get', 'discounts', 'if', 'they', 'have', 'a', 'member', 'card.']\n",
            "Tokenized: ['[CLS]', 'the', 'man', '##gers', 'also', 'play', 'a', 'classic', 'movies', 'on', 'sundays', ',', 'you', 'can', 'get', 'a', 'beer', 'in', 'the', 'lobby', ',', 'and', 'repeat', 'customers', 'can', 'get', 'discount', '##s', 'if', 'they', 'have', 'a', 'member', 'card', '.', '[SEP]']\n",
            "Original: ['A', 'great', 'cinema', 'in', 'a', 'great', 'location.']\n",
            "Tokenized: ['[CLS]', 'a', 'great', 'cinema', 'in', 'a', 'great', 'location', '.', '[SEP]']\n",
            "Original: ['Thank', 'you,', 'Bowtie!']\n",
            "Tokenized: ['[CLS]', 'thank', 'you', ',', 'bow', '##tie', '!', '[SEP]']\n",
            "Original: ['wow', 'wow', 'wow.']\n",
            "Tokenized: ['[CLS]', 'wow', 'wow', 'wow', '.', '[SEP]']\n",
            "Original: ['the', 'bast', 'cab', 'in', 'minneapolis']\n",
            "Tokenized: ['[CLS]', 'the', 'bas', '##t', 'cab', 'in', 'minneapolis', '[SEP]']\n",
            "Original: ['The', \"World's\", 'Fair', 'museum', 'was', 'pretty', 'cool.']\n",
            "Tokenized: ['[CLS]', 'the', 'world', \"'\", 's', 'fair', 'museum', 'was', 'pretty', 'cool', '.', '[SEP]']\n",
            "Original: ['Rcommended', 'by', 'bees,', 'too!']\n",
            "Tokenized: ['[CLS]', 'rc', '##om', '##men', '##ded', 'by', 'bees', ',', 'too', '!', '[SEP]']\n",
            "Original: ['Highly', 'recommended.']\n",
            "Tokenized: ['[CLS]', 'highly', 'recommended', '.', '[SEP]']\n",
            "Original: ['Joe', 'removed', 'a', 'wasp', 'nest', 'for', 'our', 'condominium', 'building', 'and', 'we', 'appreciated', 'the', 'environmentally', 'friendly', 'method', 'and', 'prompt,', 'friendly', 'and', 'informative', 'service.']\n",
            "Tokenized: ['[CLS]', 'joe', 'removed', 'a', 'wasp', 'nest', 'for', 'our', 'condom', '##inium', 'building', 'and', 'we', 'appreciated', 'the', 'environmentally', 'friendly', 'method', 'and', 'prompt', ',', 'friendly', 'and', 'inform', '##ative', 'service', '.', '[SEP]']\n",
            "Original: ['No', 'spraying', 'of', 'pesticides!']\n",
            "Tokenized: ['[CLS]', 'no', 'spraying', 'of', 'pest', '##icides', '!', '[SEP]']\n",
            "Original: ['Very', 'professional.']\n",
            "Tokenized: ['[CLS]', 'very', 'professional', '.', '[SEP]']\n",
            "Original: ['Reasonable', 'rate.']\n",
            "Tokenized: ['[CLS]', 'reasonable', 'rate', '.', '[SEP]']\n",
            "Original: ['We', 'highly', 'recommend', 'Joe', 'and', 'his', 'wasp', 'removal', 'service', 'to', 'individual', 'home', 'owners', 'and', 'condos.']\n",
            "Tokenized: ['[CLS]', 'we', 'highly', 'recommend', 'joe', 'and', 'his', 'wasp', 'removal', 'service', 'to', 'individual', 'home', 'owners', 'and', 'condo', '##s', '.', '[SEP]']\n",
            "Original: ['He', 'knows', 'his', 'bees!']\n",
            "Tokenized: ['[CLS]', 'he', 'knows', 'his', 'bees', '!', '[SEP]']\n",
            "Original: ['Suzanne,', 'Vancouver']\n",
            "Tokenized: ['[CLS]', 'suzanne', ',', 'vancouver', '[SEP]']\n",
            "Original: ['Too', 'Expensive']\n",
            "Tokenized: ['[CLS]', 'too', 'expensive', '[SEP]']\n",
            "Original: ['The', \"food's\", 'okay,', 'but', 'the', 'price', 'is', 'outrageous.']\n",
            "Tokenized: ['[CLS]', 'the', 'food', \"'\", 's', 'okay', ',', 'but', 'the', 'price', 'is', 'outrageous', '.', '[SEP]']\n",
            "Original: ['wow,', 'the', 'representative', 'went', 'way', 'above', 'and', 'beyond', 'in', 'helping', 'me', 'with', 'my', 'account', 'set', 'up.']\n",
            "Tokenized: ['[CLS]', 'wow', ',', 'the', 'representative', 'went', 'way', 'above', 'and', 'beyond', 'in', 'helping', 'me', 'with', 'my', 'account', 'set', 'up', '.', '[SEP]']\n",
            "Original: ['i', 'wish', 'the', 'other', 'utilities', 'i', 'had', 'to', 'set', 'up', 'had', 'people', 'to', 'work', 'with', 'like', 'this..']\n",
            "Tokenized: ['[CLS]', 'i', 'wish', 'the', 'other', 'utilities', 'i', 'had', 'to', 'set', 'up', 'had', 'people', 'to', 'work', 'with', 'like', 'this', '.', '.', '[SEP]']\n",
            "Original: [':)']\n",
            "Tokenized: ['[CLS]', ':', ')', '[SEP]']\n",
            "Original: ['I', 'will', 'gladly', 'recommend', 'you', 'to', 'anyone', 'in', 'need', 'of', 'or', 'looking', 'for', 'a', 'good', 'florist.']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'gladly', 'recommend', 'you', 'to', 'anyone', 'in', 'need', 'of', 'or', 'looking', 'for', 'a', 'good', 'fl', '##oris', '##t', '.', '[SEP]']\n",
            "Original: ['Superb', 'Arrangements']\n",
            "Tokenized: ['[CLS]', 'superb', 'arrangements', '[SEP]']\n",
            "Original: ['I', 'used', 'Fancy', 'Flowers', 'for', 'my', 'late', \"husband's\", 'funeral', 'flowers', 'as', 'they', 'had', 'been', 'recommended', 'to', 'me.']\n",
            "Tokenized: ['[CLS]', 'i', 'used', 'fancy', 'flowers', 'for', 'my', 'late', 'husband', \"'\", 's', 'funeral', 'flowers', 'as', 'they', 'had', 'been', 'recommended', 'to', 'me', '.', '[SEP]']\n",
            "Original: ['I', 'am', 'so', 'glad', 'that', 'I', 'called', 'in', 'to', 'see', 'Ana,', 'she', 'is', 'a', 'lovely', 'girl', 'who', 'showed', 'nothing', 'but', 'care', 'and', 'compassion', 'towards', 'me.']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'so', 'glad', 'that', 'i', 'called', 'in', 'to', 'see', 'ana', ',', 'she', 'is', 'a', 'lovely', 'girl', 'who', 'showed', 'nothing', 'but', 'care', 'and', 'compassion', 'towards', 'me', '.', '[SEP]']\n",
            "Original: ['The', 'flowers', 'were', 'all', 'that', 'I', 'hoped', 'they', 'would', 'be,', 'I', 'have', 'attended', 'several', 'funerals', 'of', 'late', 'unfortunately', 'and', 'the', 'flowers', 'I', 'recieved', 'from', 'Ana', 'outshone', 'them', 'all.']\n",
            "Tokenized: ['[CLS]', 'the', 'flowers', 'were', 'all', 'that', 'i', 'hoped', 'they', 'would', 'be', ',', 'i', 'have', 'attended', 'several', 'funeral', '##s', 'of', 'late', 'unfortunately', 'and', 'the', 'flowers', 'i', 'rec', '##ie', '##ved', 'from', 'ana', 'outs', '##hone', 'them', 'all', '.', '[SEP]']\n",
            "Original: ['She', 'is', 'so', 'talented,', 'the', 'flowers', 'were', 'arranged', 'superbly', 'and', 'delicately,', 'it', 'is', 'so', 'obvious', 'to', 'see', 'the', 'difference', 'between', 'someone', 'fully', 'trained', 'and', 'skilled', 'compared', 'to', 'others.']\n",
            "Tokenized: ['[CLS]', 'she', 'is', 'so', 'talented', ',', 'the', 'flowers', 'were', 'arranged', 'superb', '##ly', 'and', 'delicately', ',', 'it', 'is', 'so', 'obvious', 'to', 'see', 'the', 'difference', 'between', 'someone', 'fully', 'trained', 'and', 'skilled', 'compared', 'to', 'others', '.', '[SEP]']\n",
            "Original: ['Thank', 'you', 'Ana', 'I', 'hope', 'to', 'see', 'you', 'in', 'the', 'future', 'under', 'better', 'circumstances.']\n",
            "Tokenized: ['[CLS]', 'thank', 'you', 'ana', 'i', 'hope', 'to', 'see', 'you', 'in', 'the', 'future', 'under', 'better', 'circumstances', '.', '[SEP]']\n",
            "Original: ['Linda']\n",
            "Tokenized: ['[CLS]', 'linda', '[SEP]']\n",
            "Original: ['I', 'would', 'highly', 'recommend', 'Landscape', 'by', 'Hiro.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'highly', 'recommend', 'landscape', 'by', 'hi', '##ro', '.', '[SEP]']\n",
            "Original: ['Excellent', 'customer', 'service', 'and', 'quality', 'work.']\n",
            "Tokenized: ['[CLS]', 'excellent', 'customer', 'service', 'and', 'quality', 'work', '.', '[SEP]']\n",
            "Original: ['Wish', 'this', 'was', 'in', 'Saratoga--']\n",
            "Tokenized: ['[CLS]', 'wish', 'this', 'was', 'in', 'saratoga', '-', '-', '[SEP]']\n",
            "Original: ['We', 'were', 'introduced', 'to', 'Bistro', 'Tallulah', 'by', 'traveler-professional', 'diner', 'who', 'happens', 'to', 'own', 'the', 'Adelphi', 'Hotel', 'and', 'travels', 'the', 'world--', 'and', 'residing', 'in', 'Paris,London,', 'New', 'York', 'the', 'rest', 'of', 'the', 'year.']\n",
            "Tokenized: ['[CLS]', 'we', 'were', 'introduced', 'to', 'bis', '##tro', 'tall', '##ula', '##h', 'by', 'traveler', '-', 'professional', 'diner', 'who', 'happens', 'to', 'own', 'the', 'ad', '##el', '##phi', 'hotel', 'and', 'travels', 'the', 'world', '-', '-', 'and', 'residing', 'in', 'paris', ',', 'london', ',', 'new', 'york', 'the', 'rest', 'of', 'the', 'year', '.', '[SEP]']\n",
            "Original: ['SHE', 'KNOWS', 'GREAT', 'FOOD', 'AND', 'DINING', 'EXPERIENCES.']\n",
            "Tokenized: ['[CLS]', 'she', 'knows', 'great', 'food', 'and', 'dining', 'experiences', '.', '[SEP]']\n",
            "Original: ['She', 'was', 'dead', 'on--', 'this', 'restaurant', 'was', 'wonderful---', 'do', 'not', 'be', 'put', 'off', 'by', 'the', 'one', 'negative', 'review', 'on', 'this', 'page.']\n",
            "Tokenized: ['[CLS]', 'she', 'was', 'dead', 'on', '-', '-', 'this', 'restaurant', 'was', 'wonderful', '-', '-', '-', 'do', 'not', 'be', 'put', 'off', 'by', 'the', 'one', 'negative', 'review', 'on', 'this', 'page', '.', '[SEP]']\n",
            "Original: ['I', 'can', 'tell', 'you', 'we', 'were', 'pleasantly', 'surprised', '--', 'BT', 'has', 'taste', 'memory', 'and', 'each', 'time', 'back', 'the', 'food', 'was', 'consistently', 'delicious', '--', 'same', 'dishes', '--', 'were', 'consistent.']\n",
            "Tokenized: ['[CLS]', 'i', 'can', 'tell', 'you', 'we', 'were', 'pleasantly', 'surprised', '-', '-', 'bt', 'has', 'taste', 'memory', 'and', 'each', 'time', 'back', 'the', 'food', 'was', 'consistently', 'delicious', '-', '-', 'same', 'dishes', '-', '-', 'were', 'consistent', '.', '[SEP]']\n",
            "Original: ['Our', 'friend', 'is', 'a', 'world', 'traveler', 'and', 'loves', 'unpretentious', 'dining', 'experiences', 'and', 'inspired', 'food.']\n",
            "Tokenized: ['[CLS]', 'our', 'friend', 'is', 'a', 'world', 'traveler', 'and', 'loves', 'un', '##pre', '##ten', '##tious', 'dining', 'experiences', 'and', 'inspired', 'food', '.', '[SEP]']\n",
            "Original: ['This', 'chef', 'knows', 'what', 'he', 'is', 'doing.']\n",
            "Tokenized: ['[CLS]', 'this', 'chef', 'knows', 'what', 'he', 'is', 'doing', '.', '[SEP]']\n",
            "Original: ['Treat', 'yourself', 'and', 'go--', 'you', 'will', 'go', 'back!']\n",
            "Tokenized: ['[CLS]', 'treat', 'yourself', 'and', 'go', '-', '-', 'you', 'will', 'go', 'back', '!', '[SEP]']\n",
            "Original: ['I', 'am', 'a', 'business', 'owner', 'in', 'downtown', 'Saratoga', 'Springs', 'and', 'wish', 'this', 'restaurant', 'was', 'in', 'our', 'town!']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'a', 'business', 'owner', 'in', 'downtown', 'saratoga', 'springs', 'and', 'wish', 'this', 'restaurant', 'was', 'in', 'our', 'town', '!', '[SEP]']\n",
            "Original: ['The', 'affordable', 'rent', 'makes', 'it', 'possible', 'for', 'an', 'inspired', 'chef', 'to', 'serve', 'his', 'high', 'quality', 'fare', 'at', 'affordable', 'prices!']\n",
            "Tokenized: ['[CLS]', 'the', 'affordable', 'rent', 'makes', 'it', 'possible', 'for', 'an', 'inspired', 'chef', 'to', 'serve', 'his', 'high', 'quality', 'fare', 'at', 'affordable', 'prices', '!', '[SEP]']\n",
            "Original: ['Unlike', 'Saratoga.']\n",
            "Tokenized: ['[CLS]', 'unlike', 'saratoga', '.', '[SEP]']\n",
            "Original: ['I', 'rrly', 'seek', 'the', 'cehf', 'out', 'to', 'introduce', 'myself--', 'but', 'the', 'second', 'time', 'we', 'went--', 'i', 'made', 'a', 'point', 'of', 'asking', 'our', 'wait', 'person', 'to', 'introduce', 'my', 'friend', 'and', 'myself', 'to', 'the', 'chef', 'to', 'tell', 'him', 'just', 'how', 'good', 'our', 'meals', 'were.']\n",
            "Tokenized: ['[CLS]', 'i', 'rr', '##ly', 'seek', 'the', 'ce', '##h', '##f', 'out', 'to', 'introduce', 'myself', '-', '-', 'but', 'the', 'second', 'time', 'we', 'went', '-', '-', 'i', 'made', 'a', 'point', 'of', 'asking', 'our', 'wait', 'person', 'to', 'introduce', 'my', 'friend', 'and', 'myself', 'to', 'the', 'chef', 'to', 'tell', 'him', 'just', 'how', 'good', 'our', 'meals', 'were', '.', '[SEP]']\n",
            "Original: ['Cannot', 'wait', 'to', 'go', 'gain.']\n",
            "Tokenized: ['[CLS]', 'cannot', 'wait', 'to', 'go', 'gain', '.', '[SEP]']\n",
            "Original: ['-R.', 'Morris', '.']\n",
            "Tokenized: ['[CLS]', '-', 'r', '.', 'morris', '.', '[SEP]']\n",
            "Original: ['BAD', 'COFFEE,', 'DONT', 'BOTHER!']\n",
            "Tokenized: ['[CLS]', 'bad', 'coffee', ',', 'don', '##t', 'bother', '!', '[SEP]']\n",
            "Original: [\"Can't\", 'you', 'make', 'a', 'decent', 'cup', 'of', 'coffee?']\n",
            "Tokenized: ['[CLS]', 'can', \"'\", 't', 'you', 'make', 'a', 'decent', 'cup', 'of', 'coffee', '?', '[SEP]']\n",
            "Original: ['You', 'charge', 'SO', 'MUCH,', 'yet', 'you', 'use', 'the', 'same', 'grounds', 'over', 'and', 'over', 'again.']\n",
            "Tokenized: ['[CLS]', 'you', 'charge', 'so', 'much', ',', 'yet', 'you', 'use', 'the', 'same', 'grounds', 'over', 'and', 'over', 'again', '.', '[SEP]']\n",
            "Original: ['The', 'coffee', 'taste', 'BURNT', 'and', 'very', 'bitter.']\n",
            "Tokenized: ['[CLS]', 'the', 'coffee', 'taste', 'burnt', 'and', 'very', 'bitter', '.', '[SEP]']\n",
            "Original: ['No', 'amount', 'of', 'sugar', 'and', 'milk', 'can', 'mask', 'it.']\n",
            "Tokenized: ['[CLS]', 'no', 'amount', 'of', 'sugar', 'and', 'milk', 'can', 'mask', 'it', '.', '[SEP]']\n",
            "Original: ['CHANGE', 'THE', 'PROCESS,', 'PPL!']\n",
            "Tokenized: ['[CLS]', 'change', 'the', 'process', ',', 'pp', '##l', '!', '[SEP]']\n",
            "Original: ['Westfield', 'and', 'Rt', '1', 'do', 'it', 'well,', 'WHY', 'CANT', 'U????']\n",
            "Tokenized: ['[CLS]', 'westfield', 'and', 'rt', '1', 'do', 'it', 'well', ',', 'why', 'can', '##t', 'u', '?', '?', '?', '?', '[SEP]']\n",
            "Original: ['This', 'is', 'a', 'great', 'place', 'to', 'shop.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'a', 'great', 'place', 'to', 'shop', '.', '[SEP]']\n",
            "Original: [\"I've\", 'been', 'a', 'regular', 'customer', 'at', 'this', 'store', 'since', 'it', 'opened,', 'and', 'love', 'the', 'fact', 'that', 'all', 'of', 'the', 'employees', 'are', 'friendly', 'locals.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 've', 'been', 'a', 'regular', 'customer', 'at', 'this', 'store', 'since', 'it', 'opened', ',', 'and', 'love', 'the', 'fact', 'that', 'all', 'of', 'the', 'employees', 'are', 'friendly', 'locals', '.', '[SEP]']\n",
            "Original: ['Particularly', 'the', 'lady', 'who', 'operates', 'the', 'front', 'register,', \"she's\", 'very', 'kind!']\n",
            "Tokenized: ['[CLS]', 'particularly', 'the', 'lady', 'who', 'operates', 'the', 'front', 'register', ',', 'she', \"'\", 's', 'very', 'kind', '!', '[SEP]']\n",
            "Original: ['Totally', 'flavored']\n",
            "Tokenized: ['[CLS]', 'totally', 'flavor', '##ed', '[SEP]']\n",
            "Original: ['The', 'statement', 'about', '\"best', 'hamburguers', 'in', 'town\"', 'can', 'be', 'even', 'amplifiaed', 'to', '\"best', 'hamburguers', 'in', 'world\"', 'Totally', 'worth,', 'juicy,', 'big,', 'fresh,', 'and', 'excellent', 'customer', 'service!']\n",
            "Tokenized: ['[CLS]', 'the', 'statement', 'about', '\"', 'best', 'hamburg', '##uer', '##s', 'in', 'town', '\"', 'can', 'be', 'even', 'amp', '##li', '##fia', '##ed', 'to', '\"', 'best', 'hamburg', '##uer', '##s', 'in', 'world', '\"', 'totally', 'worth', ',', 'juicy', ',', 'big', ',', 'fresh', ',', 'and', 'excellent', 'customer', 'service', '!', '[SEP]']\n",
            "Original: ['Good', 'food,', 'good', 'wait', 'staff,', 'poor', 'management']\n",
            "Tokenized: ['[CLS]', 'good', 'food', ',', 'good', 'wait', 'staff', ',', 'poor', 'management', '[SEP]']\n",
            "Original: ['We', 'visited', 'on', '7/26/08', 'for', 'dinner', 'We', 'received', 'a', 'gift', 'certificate', 'for', 'the', 'Mama', \"Mia's\", 'on', 'Greenfield', 'Ave.']\n",
            "Tokenized: ['[CLS]', 'we', 'visited', 'on', '7', '/', '26', '/', '08', 'for', 'dinner', 'we', 'received', 'a', 'gift', 'certificate', 'for', 'the', 'mama', 'mia', \"'\", 's', 'on', 'greenfield', 'ave', '.', '[SEP]']\n",
            "Original: ['The', 'food', 'was', 'good,', 'and', 'so', 'was', 'our', 'waitress.']\n",
            "Tokenized: ['[CLS]', 'the', 'food', 'was', 'good', ',', 'and', 'so', 'was', 'our', 'waitress', '.', '[SEP]']\n",
            "Original: ['When', 'it', 'came', 'time', 'to', 'pay', 'the', 'bill', 'up', 'front,', 'they', 'would', 'not', 'let', 'me', 'use', 'any', 'of', 'the', 'certificate', 'for', 'a', 'tip', '(which', 'I', 'have', 'done', 'with', 'any', 'other', 'restaurant', \"I've\", 'gotten', 'a', 'gift', 'certificate', 'for.)']\n",
            "Tokenized: ['[CLS]', 'when', 'it', 'came', 'time', 'to', 'pay', 'the', 'bill', 'up', 'front', ',', 'they', 'would', 'not', 'let', 'me', 'use', 'any', 'of', 'the', 'certificate', 'for', 'a', 'tip', '(', 'which', 'i', 'have', 'done', 'with', 'any', 'other', 'restaurant', 'i', \"'\", 've', 'gotten', 'a', 'gift', 'certificate', 'for', '.', ')', '[SEP]']\n",
            "Original: ['I', 'then', 'asked', 'if', 'I', 'could', 'have', 'money', 'back', 'in', 'cash.']\n",
            "Tokenized: ['[CLS]', 'i', 'then', 'asked', 'if', 'i', 'could', 'have', 'money', 'back', 'in', 'cash', '.', '[SEP]']\n",
            "Original: ['The', 'person', 'went', 'to', 'go', 'check', 'with', 'the', 'manager,', 'who', 'was', 'sitting', 'at', 'a', 'table', 'chatting', 'with', 'her', 'friends', 'who', 'were', 'eating', 'there.']\n",
            "Tokenized: ['[CLS]', 'the', 'person', 'went', 'to', 'go', 'check', 'with', 'the', 'manager', ',', 'who', 'was', 'sitting', 'at', 'a', 'table', 'chatting', 'with', 'her', 'friends', 'who', 'were', 'eating', 'there', '.', '[SEP]']\n",
            "Original: ['Her', 'answer', 'was', 'short', 'and', 'manner', 'rather', 'rude.']\n",
            "Tokenized: ['[CLS]', 'her', 'answer', 'was', 'short', 'and', 'manner', 'rather', 'rude', '.', '[SEP]']\n",
            "Original: ['Sorry', 'for', 'interrupting', 'I', 'guess.']\n",
            "Tokenized: ['[CLS]', 'sorry', 'for', 'interrupting', 'i', 'guess', '.', '[SEP]']\n",
            "Original: ['The', 'cashier', 'was', 'also', 'short,', 'unapologetic', 'and', 'made', 'me', 'feel', 'as', 'I', 'was', 'wasting', 'her', 'time.']\n",
            "Tokenized: ['[CLS]', 'the', 'cash', '##ier', 'was', 'also', 'short', ',', 'una', '##pol', '##oge', '##tic', 'and', 'made', 'me', 'feel', 'as', 'i', 'was', 'wasting', 'her', 'time', '.', '[SEP]']\n",
            "Original: ['There', 'were', 'no', 'other', 'options', 'available', '(I', 'only', 'brought', 'my', 'check', 'card', 'to', 'cover', 'any', 'overage', 'cost)', 'and', 'she', 'rang', 'it', 'up', 'and', 'applied', 'it', 'to', 'the', 'gift', 'card', 'before', 'telling', 'me', 'about', 'the', 'tip', 'policy.']\n",
            "Tokenized: ['[CLS]', 'there', 'were', 'no', 'other', 'options', 'available', '(', 'i', 'only', 'brought', 'my', 'check', 'card', 'to', 'cover', 'any', 'over', '##age', 'cost', ')', 'and', 'she', 'rang', 'it', 'up', 'and', 'applied', 'it', 'to', 'the', 'gift', 'card', 'before', 'telling', 'me', 'about', 'the', 'tip', 'policy', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'not', 'be', 'visiting', 'Mama', \"Mia's\", 'again.']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'not', 'be', 'visiting', 'mama', 'mia', \"'\", 's', 'again', '.', '[SEP]']\n",
            "Original: ['There', 'are', 'other', 'places', 'with', 'food', 'just', 'as', 'good', 'with', 'management', 'that', 'values', 'customers', 'and', 'employees', 'much', 'more.']\n",
            "Tokenized: ['[CLS]', 'there', 'are', 'other', 'places', 'with', 'food', 'just', 'as', 'good', 'with', 'management', 'that', 'values', 'customers', 'and', 'employees', 'much', 'more', '.', '[SEP]']\n",
            "Original: ['A', 'most', 'outstanding,', 'professional', 'firm.']\n",
            "Tokenized: ['[CLS]', 'a', 'most', 'outstanding', ',', 'professional', 'firm', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'thoroughly', 'impressed!']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'thoroughly', 'impressed', '!', '[SEP]']\n",
            "Original: ['I', 'had', 'a', 'problem', 'with', 'the', 'tile', 'in', 'my', 'bathroom', 'coming', 'apart.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'a', 'problem', 'with', 'the', 'tile', 'in', 'my', 'bathroom', 'coming', 'apart', '.', '[SEP]']\n",
            "Original: ['I', 'called', 'a', 'few', 'different', 'businesses', 'in', 'the', 'area', 'to', 'get', 'estimates', ',', 'they', \"weren't\", 'the', 'cheapest', 'I', 'found', 'but', 'very', 'reasonable.']\n",
            "Tokenized: ['[CLS]', 'i', 'called', 'a', 'few', 'different', 'businesses', 'in', 'the', 'area', 'to', 'get', 'estimates', ',', 'they', 'weren', \"'\", 't', 'the', 'cheap', '##est', 'i', 'found', 'but', 'very', 'reasonable', '.', '[SEP]']\n",
            "Original: ['The', 'best', 'part', 'is', 'I', 'got', 'my', 'whole', 'bathroom', 'remodeled', 'for', 'about', 'the', 'same', 'price', 'the', 'other', \"company's\", 'were', 'quoting', 'just', 'to', 'fix', 'the', 'shower', 'tile', 'and', 'fixtures.']\n",
            "Tokenized: ['[CLS]', 'the', 'best', 'part', 'is', 'i', 'got', 'my', 'whole', 'bathroom', 'remodeled', 'for', 'about', 'the', 'same', 'price', 'the', 'other', 'company', \"'\", 's', 'were', 'quoting', 'just', 'to', 'fix', 'the', 'shower', 'tile', 'and', 'fixtures', '.', '[SEP]']\n",
            "Original: ['They', 'had', 'the', 'work', 'done', 'in', 'about', 'half', 'the', 'time', 'quoted', 'which', 'made', 'me', 'and', 'my', 'wife', 'extremely', 'happy.']\n",
            "Tokenized: ['[CLS]', 'they', 'had', 'the', 'work', 'done', 'in', 'about', 'half', 'the', 'time', 'quoted', 'which', 'made', 'me', 'and', 'my', 'wife', 'extremely', 'happy', '.', '[SEP]']\n",
            "Original: ['We', 'have', 'had', 'nothing', 'but', 'compliments', 'on', 'our', 'bathroom', 'when', 'guest', 'come', 'over-', 'who', 'would', 'have', 'guessed', 'that', 'one?']\n",
            "Tokenized: ['[CLS]', 'we', 'have', 'had', 'nothing', 'but', 'compliment', '##s', 'on', 'our', 'bathroom', 'when', 'guest', 'come', 'over', '-', 'who', 'would', 'have', 'guessed', 'that', 'one', '?', '[SEP]']\n",
            "Original: ['Very', 'nice', 'work', 'and', 'friendly', 'guys', 'too.']\n",
            "Tokenized: ['[CLS]', 'very', 'nice', 'work', 'and', 'friendly', 'guys', 'too', '.', '[SEP]']\n",
            "Original: ['Best', 'money', \"I've\", 'spent', 'on', 'remodeling', 'ever.']\n",
            "Tokenized: ['[CLS]', 'best', 'money', 'i', \"'\", 've', 'spent', 'on', 're', '##mo', '##del', '##ing', 'ever', '.', '[SEP]']\n",
            "Original: ['I', 'highly', 'recommend', 'any', 'one', 'considering', 'home', 'repair', 'to', 'give', 'these', 'guys', 'a', 'call.']\n",
            "Tokenized: ['[CLS]', 'i', 'highly', 'recommend', 'any', 'one', 'considering', 'home', 'repair', 'to', 'give', 'these', 'guys', 'a', 'call', '.', '[SEP]']\n",
            "Original: ['very', 'miss', 'informed', 'people!!']\n",
            "Tokenized: ['[CLS]', 'very', 'miss', 'informed', 'people', '!', '!', '[SEP]']\n",
            "Original: ['Awsome!']\n",
            "Tokenized: ['[CLS]', 'aw', '##some', '!', '[SEP]']\n",
            "Original: ['Great', 'food', 'cheap']\n",
            "Tokenized: ['[CLS]', 'great', 'food', 'cheap', '[SEP]']\n",
            "Original: ['Every', 'thing', 'here', 'is', 'good.']\n",
            "Tokenized: ['[CLS]', 'every', 'thing', 'here', 'is', 'good', '.', '[SEP]']\n",
            "Original: ['Fish', 'tacos', 'are', 'my', 'fave', 'simple', 'and', 'filling', 'Highly', 'recommend', 'Mi', 'Pueblo.']\n",
            "Tokenized: ['[CLS]', 'fish', 'ta', '##cos', 'are', 'my', 'fa', '##ve', 'simple', 'and', 'filling', 'highly', 'recommend', 'mi', 'pueblo', '.', '[SEP]']\n",
            "Original: ['Gets', 'busy', 'so', 'come', 'early']\n",
            "Tokenized: ['[CLS]', 'gets', 'busy', 'so', 'come', 'early', '[SEP]']\n",
            "Original: ['Worst', 'Service', \"I've\", 'Ever', 'Experienced']\n",
            "Tokenized: ['[CLS]', 'worst', 'service', 'i', \"'\", 've', 'ever', 'experienced', '[SEP]']\n",
            "Original: ['I', 'wish', 'there', 'was', 'something', 'good', 'to', 'say', 'about', 'the', 'business,', 'but', 'unfortunately,', 'there', \"isn't.\"]\n",
            "Tokenized: ['[CLS]', 'i', 'wish', 'there', 'was', 'something', 'good', 'to', 'say', 'about', 'the', 'business', ',', 'but', 'unfortunately', ',', 'there', 'isn', \"'\", 't', '.', '[SEP]']\n",
            "Original: ['1)', 'Service', 'and', 'manners', 'were', 'nonexistent.']\n",
            "Tokenized: ['[CLS]', '1', ')', 'service', 'and', 'manners', 'were', 'none', '##xi', '##sten', '##t', '.', '[SEP]']\n",
            "Original: ['2)', 'The', 'employees', 'constantly', 'talk', 'down', 'to', 'customers', 'and', 'are', 'very', 'argumentative', 'for', 'the', 'sake', 'of', 'being', 'argumentative.']\n",
            "Tokenized: ['[CLS]', '2', ')', 'the', 'employees', 'constantly', 'talk', 'down', 'to', 'customers', 'and', 'are', 'very', 'argument', '##ative', 'for', 'the', 'sake', 'of', 'being', 'argument', '##ative', '.', '[SEP]']\n",
            "Original: ['(to', 'both', 'myself', 'and', 'customers)', '3)', 'I', 'have', 'never', 'experienced', 'so', 'much', 'rudeness', 'coming', 'from', 'a', 'business.']\n",
            "Tokenized: ['[CLS]', '(', 'to', 'both', 'myself', 'and', 'customers', ')', '3', ')', 'i', 'have', 'never', 'experienced', 'so', 'much', 'rude', '##ness', 'coming', 'from', 'a', 'business', '.', '[SEP]']\n",
            "Original: ['4)', 'The', 'business', 'is', 'very', 'unorganized.']\n",
            "Tokenized: ['[CLS]', '4', ')', 'the', 'business', 'is', 'very', 'uno', '##rgan', '##ized', '.', '[SEP]']\n",
            "Original: ['The', 'invoice', 'is', 'not', 'detailed,', 'so', 'it', 'is', 'difficult', 'to', 'see', 'what', 'you', 'are', 'paying', 'for.']\n",
            "Tokenized: ['[CLS]', 'the', 'in', '##vo', '##ice', 'is', 'not', 'detailed', ',', 'so', 'it', 'is', 'difficult', 'to', 'see', 'what', 'you', 'are', 'paying', 'for', '.', '[SEP]']\n",
            "Original: [\"I'd\", 'recommend', 'to', 'save', 'your', 'time', 'and', 'energy', 'and', 'find', 'another', 'greek', 'store.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'd', 'recommend', 'to', 'save', 'your', 'time', 'and', 'energy', 'and', 'find', 'another', 'greek', 'store', '.', '[SEP]']\n",
            "Original: ['Kyle', 'with', 'Bullwark']\n",
            "Tokenized: ['[CLS]', 'kyle', 'with', 'bull', '##wark', '[SEP]']\n",
            "Original: ['Great', 'job!']\n",
            "Tokenized: ['[CLS]', 'great', 'job', '!', '[SEP]']\n",
            "Original: ['Listened', 'to', 'my', 'problem', 'and', 'took', 'care', 'of', 'it.']\n",
            "Tokenized: ['[CLS]', 'listened', 'to', 'my', 'problem', 'and', 'took', 'care', 'of', 'it', '.', '[SEP]']\n",
            "Original: ['Thanks!']\n",
            "Tokenized: ['[CLS]', 'thanks', '!', '[SEP]']\n",
            "Original: ['The', 'finest', 'Christmas', 'Trees', \"i've\", 'ever', 'seen.']\n",
            "Tokenized: ['[CLS]', 'the', 'finest', 'christmas', 'trees', 'i', \"'\", 've', 'ever', 'seen', '.', '[SEP]']\n",
            "Original: ['I', 'felt', 'like', 'I', 'was', 'in', 'heaven', 'when', 'I', 'walked', 'through', 'the', 'majestic', 'fields', 'of', 'this', 'particular', 'farm.']\n",
            "Tokenized: ['[CLS]', 'i', 'felt', 'like', 'i', 'was', 'in', 'heaven', 'when', 'i', 'walked', 'through', 'the', 'majestic', 'fields', 'of', 'this', 'particular', 'farm', '.', '[SEP]']\n",
            "Original: ['The', 'trees', 'were', 'in', 'magnificent', 'shape', 'and', 'the', 'variety', 'was', 'astounding.']\n",
            "Tokenized: ['[CLS]', 'the', 'trees', 'were', 'in', 'magnificent', 'shape', 'and', 'the', 'variety', 'was', 'as', '##tou', '##nding', '.', '[SEP]']\n",
            "Original: ['The', 'owners', 'were', 'entertaining', 'and', 'gracious.']\n",
            "Tokenized: ['[CLS]', 'the', 'owners', 'were', 'entertaining', 'and', 'gr', '##acious', '.', '[SEP]']\n",
            "Original: ['I', 'especially', 'liked', 'the', 'Eco', 'friendly', 'atmosphere', 'and', 'the', 'owners', 'love', 'of', 'all', 'animals.']\n",
            "Tokenized: ['[CLS]', 'i', 'especially', 'liked', 'the', 'eco', 'friendly', 'atmosphere', 'and', 'the', 'owners', 'love', 'of', 'all', 'animals', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'one', 'of', 'the', 'best', 'farms', 'I', 'have', 'ever', 'been', 'too.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'one', 'of', 'the', 'best', 'farms', 'i', 'have', 'ever', 'been', 'too', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'call', 'it', 'the', 'Taj', 'Mahal', 'of', 'the', 'east', 'coast!']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'call', 'it', 'the', 'ta', '##j', 'mahal', 'of', 'the', 'east', 'coast', '!', '[SEP]']\n",
            "Original: ['It', 'put', 'hair', 'on', 'my', 'chest', 'and', 'thanks', 'to', 'the', 'owners', 'advice', 'I', 'invested', 'vanguard,', 'got', 'myself', 'a', 'woman', 'like', 'Jerry,', 'and', 'became', 'a', 'republican.']\n",
            "Tokenized: ['[CLS]', 'it', 'put', 'hair', 'on', 'my', 'chest', 'and', 'thanks', 'to', 'the', 'owners', 'advice', 'i', 'invested', 'vanguard', ',', 'got', 'myself', 'a', 'woman', 'like', 'jerry', ',', 'and', 'became', 'a', 'republican', '.', '[SEP]']\n",
            "Original: ['Thanks', 'Tussey', 'Mountain', 'Tree', 'Plantation!']\n",
            "Tokenized: ['[CLS]', 'thanks', 'tu', '##sse', '##y', 'mountain', 'tree', 'plantation', '!', '[SEP]']\n",
            "Original: ['Professional', 'and', 'inspiring']\n",
            "Tokenized: ['[CLS]', 'professional', 'and', 'inspiring', '[SEP]']\n",
            "Original: ['Nigel', 'from', 'Nidd', 'Design', 'has', 'always', 'provided', 'a', 'first', 'class', 'service,', 'from', 'his', 'advice', 'and', 'professionalism', 'to', 'the', 'quality', 'of', 'his', 'design', 'drawings', 'and', 'planning', 'applications.']\n",
            "Tokenized: ['[CLS]', 'nigel', 'from', 'ni', '##dd', 'design', 'has', 'always', 'provided', 'a', 'first', 'class', 'service', ',', 'from', 'his', 'advice', 'and', 'professional', '##ism', 'to', 'the', 'quality', 'of', 'his', 'design', 'drawings', 'and', 'planning', 'applications', '.', '[SEP]']\n",
            "Original: ['His', 'knowledge', 'and', 'expertise', 'help', 'smooth', 'the', 'way', 'with', 'any', 'planning', 'application,', 'ensuring', 'compliance', 'with', 'the', 'building', 'regulations.']\n",
            "Tokenized: ['[CLS]', 'his', 'knowledge', 'and', 'expertise', 'help', 'smooth', 'the', 'way', 'with', 'any', 'planning', 'application', ',', 'ensuring', 'compliance', 'with', 'the', 'building', 'regulations', '.', '[SEP]']\n",
            "Original: ['Once', 'you', 'have', 'met', 'Nigel', 'you', 'will', 'not', 'want', 'to', 'work', 'with', 'anyone', 'else.']\n",
            "Tokenized: ['[CLS]', 'once', 'you', 'have', 'met', 'nigel', 'you', 'will', 'not', 'want', 'to', 'work', 'with', 'anyone', 'else', '.', '[SEP]']\n",
            "Original: ['He', 'really', 'does', 'turn', 'your', 'dreams', 'into', 'reality', 'for', 'your', 'home!']\n",
            "Tokenized: ['[CLS]', 'he', 'really', 'does', 'turn', 'your', 'dreams', 'into', 'reality', 'for', 'your', 'home', '!', '[SEP]']\n",
            "Original: ['Gets', 'the', 'Job', 'Done']\n",
            "Tokenized: ['[CLS]', 'gets', 'the', 'job', 'done', '[SEP]']\n",
            "Original: ['We', 'have', 'utilized', 'Mr.', 'Pozza', 'and', 'his', 'firm', 'twice', 'now', 'in', 'our', 'family', 'and', 'both', 'times', 'have', 'been', 'very', 'pleased.']\n",
            "Tokenized: ['[CLS]', 'we', 'have', 'utilized', 'mr', '.', 'po', '##zza', 'and', 'his', 'firm', 'twice', 'now', 'in', 'our', 'family', 'and', 'both', 'times', 'have', 'been', 'very', 'pleased', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'not', 'hesitate', 'to', 'use', 'him', 'again', 'or', 'refer', 'him', 'to', 'my', 'family', 'or', 'friends.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'not', 'hesitate', 'to', 'use', 'him', 'again', 'or', 'refer', 'him', 'to', 'my', 'family', 'or', 'friends', '.', '[SEP]']\n",
            "Original: ['Barb', 'does', 'an', 'AMAZING', 'JOB,', 'she', 'is', 'always', 'learning', 'new', 'things', 'on', 'how', 'to', 'use', 'her', 'hands', 'and', 'body,', 'to', 'give', 'every', 'person', 'an', 'AWESOME', 'MASSAGE,', 'CALL', 'TODAY', 'AND', 'SCHEDULE', 'YOU', 'MUST', 'SEE', 'HER,', 'YOU', 'WILL', 'FALL', 'IN', 'LOVE', 'SHE', 'IS', 'THE', 'BEST', 'OF', 'THE', 'BEST', '!']\n",
            "Tokenized: ['[CLS]', 'bar', '##b', 'does', 'an', 'amazing', 'job', ',', 'she', 'is', 'always', 'learning', 'new', 'things', 'on', 'how', 'to', 'use', 'her', 'hands', 'and', 'body', ',', 'to', 'give', 'every', 'person', 'an', 'awesome', 'massage', ',', 'call', 'today', 'and', 'schedule', 'you', 'must', 'see', 'her', ',', 'you', 'will', 'fall', 'in', 'love', 'she', 'is', 'the', 'best', 'of', 'the', 'best', '!', '[SEP]']\n",
            "Original: ['Excellent', 'Pizza!!']\n",
            "Tokenized: ['[CLS]', 'excellent', 'pizza', '!', '!', '[SEP]']\n",
            "Original: ['Its', 'the', 'only', 'pizza', 'place', 'I', 'recommend', 'in', 'Woodland', 'Hills.']\n",
            "Tokenized: ['[CLS]', 'its', 'the', 'only', 'pizza', 'place', 'i', 'recommend', 'in', 'woodland', 'hills', '.', '[SEP]']\n",
            "Original: ['Yum.']\n",
            "Tokenized: ['[CLS]', 'yu', '##m', '.', '[SEP]']\n",
            "Original: ['My', 'wife', 'and', 'kids', \"can't\", 'get', 'enough.']\n",
            "Tokenized: ['[CLS]', 'my', 'wife', 'and', 'kids', 'can', \"'\", 't', 'get', 'enough', '.', '[SEP]']\n",
            "Original: ['Highly', 'recommended.']\n",
            "Tokenized: ['[CLS]', 'highly', 'recommended', '.', '[SEP]']\n",
            "Original: ['OMG']\n",
            "Tokenized: ['[CLS]', 'om', '##g', '[SEP]']\n",
            "Original: ['OMG..', 'make', 'sure', 'to', 'book', 'a', 'reservation,', 'as', 'this', 'magical', 'place', 'is', 'packed', '(in', 'a', 'nice', 'way)', 'I', 'love', 'that', 'the', 'owner', 'walks', 'around', 'and', 'cares', 'how', 'his', 'customers', 'feel', 'about', 'their', 'food.']\n",
            "Tokenized: ['[CLS]', 'om', '##g', '.', '.', 'make', 'sure', 'to', 'book', 'a', 'reservation', ',', 'as', 'this', 'magical', 'place', 'is', 'packed', '(', 'in', 'a', 'nice', 'way', ')', 'i', 'love', 'that', 'the', 'owner', 'walks', 'around', 'and', 'cares', 'how', 'his', 'customers', 'feel', 'about', 'their', 'food', '.', '[SEP]']\n",
            "Original: ['The', 'waiters', 'are', 'like', 'no', 'other...']\n",
            "Tokenized: ['[CLS]', 'the', 'waiter', '##s', 'are', 'like', 'no', 'other', '.', '.', '.', '[SEP]']\n",
            "Original: ['My', 'waiter', 'was', 'so', 'excellent', 'I', 'gave', 'him', 'a', '75%', 'tip,', 'and', 'it', 'was', 'worht', 'every', 'penny..']\n",
            "Tokenized: ['[CLS]', 'my', 'waiter', 'was', 'so', 'excellent', 'i', 'gave', 'him', 'a', '75', '%', 'tip', ',', 'and', 'it', 'was', 'wo', '##rh', '##t', 'every', 'penny', '.', '.', '[SEP]']\n",
            "Original: ['The', 'food', 'was', 'finger', 'licking', 'the', 'bowel', 'fantastic..']\n",
            "Tokenized: ['[CLS]', 'the', 'food', 'was', 'finger', 'licking', 'the', 'bow', '##el', 'fantastic', '.', '.', '[SEP]']\n",
            "Original: ['I', 'just', 'discovered', 'her', 'has', 'a', 'place', 'right', 'near', 'my', 'work', '(', 'Color', 'me', 'Phat)', 'If', 'you', 'are', 'looking', 'for', 'a', 'romatic', 'place', 'with', 'the', 'best', 'food', 'and', 'service', 'in', 'the', 'valley', 'Giovanni', 'Ristorante', 'should', 'be', 'your', 'number', '1', '+', '2', 'choice.']\n",
            "Tokenized: ['[CLS]', 'i', 'just', 'discovered', 'her', 'has', 'a', 'place', 'right', 'near', 'my', 'work', '(', 'color', 'me', 'ph', '##at', ')', 'if', 'you', 'are', 'looking', 'for', 'a', 'roma', '##tic', 'place', 'with', 'the', 'best', 'food', 'and', 'service', 'in', 'the', 'valley', 'giovanni', 'ri', '##stor', '##ante', 'should', 'be', 'your', 'number', '1', '+', '2', 'choice', '.', '[SEP]']\n",
            "Original: ['impressive', 'truly', 'impressive']\n",
            "Tokenized: ['[CLS]', 'impressive', 'truly', 'impressive', '[SEP]']\n",
            "Original: ['The', 'First', 'time', 'I', 'walked', 'in', 'there', 'with', 'my', 'teacup', 'chihuahua', 'puppy', 'I', 'knew', \"I'd\", 'be', 'here', 'a', 'lot.']\n",
            "Tokenized: ['[CLS]', 'the', 'first', 'time', 'i', 'walked', 'in', 'there', 'with', 'my', 'tea', '##cup', 'chihuahua', 'puppy', 'i', 'knew', 'i', \"'\", 'd', 'be', 'here', 'a', 'lot', '.', '[SEP]']\n",
            "Original: ['Pets', 'Discount', 'has', 'lovely', 'employees,', 'a', 'wonderful', 'grooming', 'service,', 'and', 'everything', 'I', 'need', 'to', 'keep', 'my', 'dog', 'in', 'tip', 'top', 'condition!']\n",
            "Tokenized: ['[CLS]', 'pets', 'discount', 'has', 'lovely', 'employees', ',', 'a', 'wonderful', 'groom', '##ing', 'service', ',', 'and', 'everything', 'i', 'need', 'to', 'keep', 'my', 'dog', 'in', 'tip', 'top', 'condition', '!', '[SEP]']\n",
            "Original: ['ATE', 'HERE', 'A', 'COUPLE', 'TIMES.']\n",
            "Tokenized: ['[CLS]', 'ate', 'here', 'a', 'couple', 'times', '.', '[SEP]']\n",
            "Original: ['IT', 'IS', 'NOT', 'A', 'HIGH', 'END', 'STEAK', 'HOUSE,', 'MORE', 'OF', 'THE', 'CUISINE', 'BRETT', 'ENJOYS', 'IN', 'MISSISSIPPI.']\n",
            "Tokenized: ['[CLS]', 'it', 'is', 'not', 'a', 'high', 'end', 'steak', 'house', ',', 'more', 'of', 'the', 'cuisine', 'brett', 'enjoys', 'in', 'mississippi', '.', '[SEP]']\n",
            "Original: ['SO,', 'IF', 'YOU', 'WANT', 'A', 'BURGER', 'AND', 'FRIES,', 'WELL,', 'IT', 'IS', 'OK.']\n",
            "Tokenized: ['[CLS]', 'so', ',', 'if', 'you', 'want', 'a', 'burger', 'and', 'fries', ',', 'well', ',', 'it', 'is', 'ok', '.', '[SEP]']\n",
            "Original: ['IF', 'YOU', 'WANT', 'A', 'LITTLE', 'CAJUNISH', 'FOOD', '-', 'IT', 'IS', 'GOOD.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'want', 'a', 'little', 'ca', '##jun', '##ish', 'food', '-', 'it', 'is', 'good', '.', '[SEP]']\n",
            "Original: ['IF', 'YOU', 'WANT', 'A', 'STEAK,', 'WELL,', 'THIS', 'IS', 'NOT', 'THE', 'BEST', 'IN', 'GREEN', 'BAY.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'want', 'a', 'steak', ',', 'well', ',', 'this', 'is', 'not', 'the', 'best', 'in', 'green', 'bay', '.', '[SEP]']\n",
            "Original: ['OVERALL', 'DECENT', 'BUT', 'IF', 'YOU', 'ARE', 'EXPECTING', 'A', 'RUTH', 'CHRIS', 'TYPE', 'STEAK,', 'THIS', 'IS', 'NOT', 'IT.']\n",
            "Tokenized: ['[CLS]', 'overall', 'decent', 'but', 'if', 'you', 'are', 'expecting', 'a', 'ruth', 'chris', 'type', 'steak', ',', 'this', 'is', 'not', 'it', '.', '[SEP]']\n",
            "Original: ['Great', 'service']\n",
            "Tokenized: ['[CLS]', 'great', 'service', '[SEP]']\n",
            "Original: ['I', 'had', 'a', 'dead', 'battery', 'last', 'week', 'and', 'called', 'this', 'company', 'since', 'they', 'were', 'the', 'closest', 'they', 'had', 'very', 'quick', 'service', 'for', 'a', 'Monday', 'morning,', 'thanks', 'again', 'guys.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'a', 'dead', 'battery', 'last', 'week', 'and', 'called', 'this', 'company', 'since', 'they', 'were', 'the', 'closest', 'they', 'had', 'very', 'quick', 'service', 'for', 'a', 'monday', 'morning', ',', 'thanks', 'again', 'guys', '.', '[SEP]']\n",
            "Original: ['green', 'curry', 'and', 'red', 'curry', 'is', 'awesome!']\n",
            "Tokenized: ['[CLS]', 'green', 'curry', 'and', 'red', 'curry', 'is', 'awesome', '!', '[SEP]']\n",
            "Original: ['remember', 'to', 'ask', 'for', 'extra', 'vege']\n",
            "Tokenized: ['[CLS]', 'remember', 'to', 'ask', 'for', 'extra', 've', '##ge', '[SEP]']\n",
            "Original: ['Horrible', 'Service!']\n",
            "Tokenized: ['[CLS]', 'horrible', 'service', '!', '[SEP]']\n",
            "Original: ['I', 'got', 'yelled', 'at,', 'literally', 'yelled', 'at', 'because', 'i', 'asked', 'if', 'i', 'could', 'pick', 'up', 'my', 'car', '5-10', 'minutes', 'late.']\n",
            "Tokenized: ['[CLS]', 'i', 'got', 'yelled', 'at', ',', 'literally', 'yelled', 'at', 'because', 'i', 'asked', 'if', 'i', 'could', 'pick', 'up', 'my', 'car', '5', '-', '10', 'minutes', 'late', '.', '[SEP]']\n",
            "Original: ['I', 'explained', 'that', 'i', 'was', 'already', 'on', 'my', 'way', 'and', 'i', 'would', 'rush', 'to', 'get', 'there', 'as', 'soon', 'as', 'i', 'could', 'because', 'i', 'needed', 'my', 'car', 'for', 'work', 'at', '5am,', 'but', 'the', 'guy', 'was', 'arguing', 'with', 'me', 'saying', 'he', 'was', 'gonna', 'lock', 'the', 'doors', 'right', 'at', '5:30.']\n",
            "Tokenized: ['[CLS]', 'i', 'explained', 'that', 'i', 'was', 'already', 'on', 'my', 'way', 'and', 'i', 'would', 'rush', 'to', 'get', 'there', 'as', 'soon', 'as', 'i', 'could', 'because', 'i', 'needed', 'my', 'car', 'for', 'work', 'at', '5', '##am', ',', 'but', 'the', 'guy', 'was', 'arguing', 'with', 'me', 'saying', 'he', 'was', 'gonna', 'lock', 'the', 'doors', 'right', 'at', '5', ':', '30', '.', '[SEP]']\n",
            "Original: ['Rude,', 'unprofessional,', 'just', 'jerks.']\n",
            "Tokenized: ['[CLS]', 'rude', ',', 'un', '##pro', '##fe', '##ssion', '##al', ',', 'just', 'jerk', '##s', '.', '[SEP]']\n",
            "Original: ['Seems', 'like', 'all', 'they', 'care', 'about', 'is', 'the', 'money', 'and', 'getting', 'home', 'on', 'time,', 'NO', 'care', 'for', 'the', 'customers', 'AT', 'ALL!!!']\n",
            "Tokenized: ['[CLS]', 'seems', 'like', 'all', 'they', 'care', 'about', 'is', 'the', 'money', 'and', 'getting', 'home', 'on', 'time', ',', 'no', 'care', 'for', 'the', 'customers', 'at', 'all', '!', '!', '!', '[SEP]']\n",
            "Original: ['Missed', 'a', 'whole', 'day', 'of', 'work', 'because', 'i', 'am', 'now', 'carless.']\n",
            "Tokenized: ['[CLS]', 'missed', 'a', 'whole', 'day', 'of', 'work', 'because', 'i', 'am', 'now', 'carl', '##ess', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'NEEEEEEEEEVERRRR', 'go', 'to', 'this', 'place', 'again.']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'nee', '##ee', '##ee', '##ee', '##ever', '##rr', '##r', 'go', 'to', 'this', 'place', 'again', '.', '[SEP]']\n",
            "Original: ['THE', 'TEACHING', 'THERE', 'SUCKS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!']\n",
            "Tokenized: ['[CLS]', 'the', 'teaching', 'there', 'sucks', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['ALL', 'OF', 'THE', 'TEACHERS', 'THERE', 'ARE', 'SO', 'MEAN', 'THEY', 'GET', 'MAD', 'AT', 'YOU', 'FOR', 'NOTHING!!!!!!!!!!!!!!!!!!!']\n",
            "Tokenized: ['[CLS]', 'all', 'of', 'the', 'teachers', 'there', 'are', 'so', 'mean', 'they', 'get', 'mad', 'at', 'you', 'for', 'nothing', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['THIS', 'IS', 'THE', 'WORST', 'SCHOOL', 'IVE', 'BEEN', 'TO!!!!!!']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'the', 'worst', 'school', 'iv', '##e', 'been', 'to', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['kudos', 'to', 'Allentown', 'Post', 'Office', 'staff']\n",
            "Tokenized: ['[CLS]', 'ku', '##dos', 'to', 'allen', '##town', 'post', 'office', 'staff', '[SEP]']\n",
            "Original: ['The', 'staff', 'in', 'Allentown', 'are', 'friendly,', 'helpful', 'and', 'a', 'delight', 'to', 'know..']\n",
            "Tokenized: ['[CLS]', 'the', 'staff', 'in', 'allen', '##town', 'are', 'friendly', ',', 'helpful', 'and', 'a', 'delight', 'to', 'know', '.', '.', '[SEP]']\n",
            "Original: ['Horrible']\n",
            "Tokenized: ['[CLS]', 'horrible', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'growing', 'my', 'hair', 'out', 'for', '1', 'year', 'plus', 'and', 'went', 'in', 'to', 'get', '1', 'inch', 'taken', 'off.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'growing', 'my', 'hair', 'out', 'for', '1', 'year', 'plus', 'and', 'went', 'in', 'to', 'get', '1', 'inch', 'taken', 'off', '.', '[SEP]']\n",
            "Original: ['I', 'walked', 'out', 'with', '5', 'inch', 'long', 'hair', 'on', 'the', 'top,', '2', 'inch', 'long', 'hair', 'on', 'the', 'sides,', 'and', '1.5', 'in', 'the', 'back.']\n",
            "Tokenized: ['[CLS]', 'i', 'walked', 'out', 'with', '5', 'inch', 'long', 'hair', 'on', 'the', 'top', ',', '2', 'inch', 'long', 'hair', 'on', 'the', 'sides', ',', 'and', '1', '.', '5', 'in', 'the', 'back', '.', '[SEP]']\n",
            "Original: ['My', 'hair', 'is', 'uneven', 'and', 'it', 'looks', 'rediculous.']\n",
            "Tokenized: ['[CLS]', 'my', 'hair', 'is', 'uneven', 'and', 'it', 'looks', 'red', '##ic', '##ulous', '.', '[SEP]']\n",
            "Original: ['This', 'woman', 'should', 'be', 'working', 'in', 'supercuts...if', 'that.']\n",
            "Tokenized: ['[CLS]', 'this', 'woman', 'should', 'be', 'working', 'in', 'super', '##cut', '##s', '.', '.', '.', 'if', 'that', '.', '[SEP]']\n",
            "Original: ['This', 'was', 'a', 'terrible', 'experience', 'and', 'I', 'hope', 'that', 'no', 'one', 'else', 'goes', 'through', 'that.']\n",
            "Tokenized: ['[CLS]', 'this', 'was', 'a', 'terrible', 'experience', 'and', 'i', 'hope', 'that', 'no', 'one', 'else', 'goes', 'through', 'that', '.', '[SEP]']\n",
            "Original: ['Do', 'your', 'self', 'a', 'favor', 'and', 'do', 'not', 'go', 'to', 'this', 'establishment.']\n",
            "Tokenized: ['[CLS]', 'do', 'your', 'self', 'a', 'favor', 'and', 'do', 'not', 'go', 'to', 'this', 'establishment', '.', '[SEP]']\n",
            "Original: ['PHOTOS', 'DONE', 'WELL']\n",
            "Tokenized: ['[CLS]', 'photos', 'done', 'well', '[SEP]']\n",
            "Original: ['I', 'Love', 'Hellada', 'Gallery!']\n",
            "Tokenized: ['[CLS]', 'i', 'love', 'hell', '##ada', 'gallery', '!', '[SEP]']\n",
            "Original: ['Marek', 'Dzida', 'the', 'owner', 'and', 'photographer', 'puts', 'whole', 'heart', 'in', 'his', 'business', '-', 'If', 'you', 'are', 'into', 'old', 'fashion', '(Not', 'Digital)', 'quality', 'photography', 'this', 'is', 'best', 'place', 'in', 'Long', 'Beach', 'as', 'I', 'think', 'not', 'many', 'folks', 'can', 'do', 'affordable', 'traditional', 'photos', 'anymore', 'I', 'know', 'Marek', 'personaly', 'and', 'I', 'will', 'always', 'recommend', 'him']\n",
            "Tokenized: ['[CLS]', 'marek', 'd', '##zi', '##da', 'the', 'owner', 'and', 'photographer', 'puts', 'whole', 'heart', 'in', 'his', 'business', '-', 'if', 'you', 'are', 'into', 'old', 'fashion', '(', 'not', 'digital', ')', 'quality', 'photography', 'this', 'is', 'best', 'place', 'in', 'long', 'beach', 'as', 'i', 'think', 'not', 'many', 'folks', 'can', 'do', 'affordable', 'traditional', 'photos', 'anymore', 'i', 'know', 'marek', 'personal', '##y', 'and', 'i', 'will', 'always', 'recommend', 'him', '[SEP]']\n",
            "Original: ['Great', 'help', 'even', 'near', 'closing', 'time!']\n",
            "Tokenized: ['[CLS]', 'great', 'help', 'even', 'near', 'closing', 'time', '!', '[SEP]']\n",
            "Original: ['I', 'came', 'in', 'to', 'town', 'for', 'a', 'week', 'and', 'forgot', 'my', 'trainers!']\n",
            "Tokenized: ['[CLS]', 'i', 'came', 'in', 'to', 'town', 'for', 'a', 'week', 'and', 'forgot', 'my', 'trainers', '!', '[SEP]']\n",
            "Original: ['Oh', 'no!!']\n",
            "Tokenized: ['[CLS]', 'oh', 'no', '!', '!', '[SEP]']\n",
            "Original: ['I', 'came', 'in', '30', 'min', 'before', 'close', 'and', 'the', 'staff', 'was', 'super', 'helpful.']\n",
            "Tokenized: ['[CLS]', 'i', 'came', 'in', '30', 'min', 'before', 'close', 'and', 'the', 'staff', 'was', 'super', 'helpful', '.', '[SEP]']\n",
            "Original: ['They', 'spent', 'a', 'lot', 'of', 'time', 'with', 'me', 'and', 'got', 'me', 'into', 'a', 'great', 'pair', 'of', 'shoes.']\n",
            "Tokenized: ['[CLS]', 'they', 'spent', 'a', 'lot', 'of', 'time', 'with', 'me', 'and', 'got', 'me', 'into', 'a', 'great', 'pair', 'of', 'shoes', '.', '[SEP]']\n",
            "Original: ['I', 'think', 'they', 'may', 'even', 'be', 'better', 'than', 'the', 'pair', 'I', 'have', 'been', 'using', 'this', 'past', 'year!']\n",
            "Tokenized: ['[CLS]', 'i', 'think', 'they', 'may', 'even', 'be', 'better', 'than', 'the', 'pair', 'i', 'have', 'been', 'using', 'this', 'past', 'year', '!', '[SEP]']\n",
            "Original: ['Thanks', 'Run', 'on!']\n",
            "Tokenized: ['[CLS]', 'thanks', 'run', 'on', '!', '[SEP]']\n",
            "Original: ['SERVERS']\n",
            "Tokenized: ['[CLS]', 'servers', '[SEP]']\n",
            "Original: ['When', 'my', 'server', 'crashed,', 'Greg', 'worked', 'from', '7', 'PM', 'until', '4', 'AM', 'and', 'had', 'my', 'company', 'up', 'and', 'running', 'the', 'next', 'morning.']\n",
            "Tokenized: ['[CLS]', 'when', 'my', 'server', 'crashed', ',', 'greg', 'worked', 'from', '7', 'pm', 'until', '4', 'am', 'and', 'had', 'my', 'company', 'up', 'and', 'running', 'the', 'next', 'morning', '.', '[SEP]']\n",
            "Original: [\"That's\", 'what', 'I', 'call', 'customer', 'service!']\n",
            "Tokenized: ['[CLS]', 'that', \"'\", 's', 'what', 'i', 'call', 'customer', 'service', '!', '[SEP]']\n",
            "Original: ['Elmira,', 'your', 'the', 'best!']\n",
            "Tokenized: ['[CLS]', 'elm', '##ira', ',', 'your', 'the', 'best', '!', '[SEP]']\n",
            "Original: ['All', 'I', 'can', 'say', 'is', 'that', 'Elmira', 'you', 'are', 'the', 'best', 'Ive', 'experienced,', 'never', 'before', 'has', 'the', 'seamstress', 'done', 'a', 'perfect', 'job', 'until', 'i', 'met', 'you.']\n",
            "Tokenized: ['[CLS]', 'all', 'i', 'can', 'say', 'is', 'that', 'elm', '##ira', 'you', 'are', 'the', 'best', 'iv', '##e', 'experienced', ',', 'never', 'before', 'has', 'the', 'seam', '##st', '##ress', 'done', 'a', 'perfect', 'job', 'until', 'i', 'met', 'you', '.', '[SEP]']\n",
            "Original: ['I', 'recommend', 'you', 'to', 'everyone', 'in', 'Calgary,', 'as', 'she', 'is', 'a', 'professional', 'and', 'the', 'cost', 'for', 'her', 'was', 'low.']\n",
            "Tokenized: ['[CLS]', 'i', 'recommend', 'you', 'to', 'everyone', 'in', 'calgary', ',', 'as', 'she', 'is', 'a', 'professional', 'and', 'the', 'cost', 'for', 'her', 'was', 'low', '.', '[SEP]']\n",
            "Original: ['VINGAS']\n",
            "Tokenized: ['[CLS]', 'vin', '##gas', '[SEP]']\n",
            "Original: ['VISAKHA', 'INDUSTRIAL', 'GASES', 'PVT.', 'LTD.,', 'location', 'at', 'google', 'maps.']\n",
            "Tokenized: ['[CLS]', 'visa', '##kha', 'industrial', 'gases', 'pv', '##t', '.', 'ltd', '.', ',', 'location', 'at', 'google', 'maps', '.', '[SEP]']\n",
            "Original: ['A', 'Great', 'Help!']\n",
            "Tokenized: ['[CLS]', 'a', 'great', 'help', '!', '[SEP]']\n",
            "Original: ['Ashdown', 'Horse', 'Transport', 'were', 'fantastic!']\n",
            "Tokenized: ['[CLS]', 'ash', '##down', 'horse', 'transport', 'were', 'fantastic', '!', '[SEP]']\n",
            "Original: ['Very', 'friendly', 'and', 'ALWAY', 'contactable', 'even', 'at', 'weekends.']\n",
            "Tokenized: ['[CLS]', 'very', 'friendly', 'and', 'al', '##way', 'contact', '##able', 'even', 'at', 'weekends', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'hesitate', 'to', 'recommend', 'anyone.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'hesitate', 'to', 'recommend', 'anyone', '.', '[SEP]']\n",
            "Original: ['Thanks', 'again', 'Nina.']\n",
            "Tokenized: ['[CLS]', 'thanks', 'again', 'nina', '.', '[SEP]']\n",
            "Original: ['PS.', 'Love', 'the', 'new', 'website!']\n",
            "Tokenized: ['[CLS]', 'ps', '.', 'love', 'the', 'new', 'website', '!', '[SEP]']\n",
            "Original: ['Natasha']\n",
            "Tokenized: ['[CLS]', 'natasha', '[SEP]']\n",
            "Original: ['A', 'real', 'pleasure', 'training', 'with', 'Natasha.']\n",
            "Tokenized: ['[CLS]', 'a', 'real', 'pleasure', 'training', 'with', 'natasha', '.', '[SEP]']\n",
            "Original: ['Always', 'professional', 'and', 'reliable,', 'sessions', 'are', 'good', 'fun', 'and', 'suitably', 'challenging.']\n",
            "Tokenized: ['[CLS]', 'always', 'professional', 'and', 'reliable', ',', 'sessions', 'are', 'good', 'fun', 'and', 'suit', '##ably', 'challenging', '.', '[SEP]']\n",
            "Original: ['She', 'really', 'listens', 'to', 'what', 'it', 'is', 'you', 'would', 'like', 'to', 'achieve,', 'and', 'I', 'am', 'very', 'happy', 'with', 'my', 'results.']\n",
            "Tokenized: ['[CLS]', 'she', 'really', 'listen', '##s', 'to', 'what', 'it', 'is', 'you', 'would', 'like', 'to', 'achieve', ',', 'and', 'i', 'am', 'very', 'happy', 'with', 'my', 'results', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'highly', 'recommend', 'her', 'services.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'highly', 'recommend', 'her', 'services', '.', '[SEP]']\n",
            "Original: ['Feels', 'like', 'you', 'are', 'in', 'Brooklyn,', 'but', 'people', 'watching', 'is', 'entertaining.']\n",
            "Tokenized: ['[CLS]', 'feels', 'like', 'you', 'are', 'in', 'brooklyn', ',', 'but', 'people', 'watching', 'is', 'entertaining', '.', '[SEP]']\n",
            "Original: ['highly', 'recommended.']\n",
            "Tokenized: ['[CLS]', 'highly', 'recommended', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'going', 'to', 'Warner', 'Family', 'for', 'a', 'number', 'of', 'years', 'and', 'would', 'highly', 'recommend', 'it', 'to', 'anyone.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'going', 'to', 'warner', 'family', 'for', 'a', 'number', 'of', 'years', 'and', 'would', 'highly', 'recommend', 'it', 'to', 'anyone', '.', '[SEP]']\n",
            "Original: [\"I've\", 'read', 'some', 'of', 'the', 'reviews', 'below', 'and', 'would', 'like', 'to', 'state', 'that', 'yes,', 'like', 'any', 'other', 'doctors', 'office', 'there', 'is', 'sometimes', 'a', 'wait', '(depending', 'on', 'what', 'other', 'patients', 'are', 'being', 'seen', 'for)', 'and', 'some', 'of', 'the', 'tests', 'and', 'procedures', 'that', 'are', 'ran', 'can', 'be', 'costly', '(just', 'like', 'they', 'would', 'be', 'for', 'any', 'other', 'medical', 'tests', 'elsewhere', 'if', 'you', 'do', 'not', 'have', 'insurance)...']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 've', 'read', 'some', 'of', 'the', 'reviews', 'below', 'and', 'would', 'like', 'to', 'state', 'that', 'yes', ',', 'like', 'any', 'other', 'doctors', 'office', 'there', 'is', 'sometimes', 'a', 'wait', '(', 'depending', 'on', 'what', 'other', 'patients', 'are', 'being', 'seen', 'for', ')', 'and', 'some', 'of', 'the', 'tests', 'and', 'procedures', 'that', 'are', 'ran', 'can', 'be', 'costly', '(', 'just', 'like', 'they', 'would', 'be', 'for', 'any', 'other', 'medical', 'tests', 'elsewhere', 'if', 'you', 'do', 'not', 'have', 'insurance', ')', '.', '.', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'seen', 'several', 'of', 'the', 'providers', 'from', 'the', 'office', 'and', 'have', 'not', 'once', 'been', 'shown', 'anything', 'but', 'care', 'and', 'consideration.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'seen', 'several', 'of', 'the', 'providers', 'from', 'the', 'office', 'and', 'have', 'not', 'once', 'been', 'shown', 'anything', 'but', 'care', 'and', 'consideration', '.', '[SEP]']\n",
            "Original: ['I', 'cant', 'speak', 'for', 'them', 'but', 'any', 'tests', 'or', 'appointments', 'they', 'recommend', 'are', 'probably', 'in', 'the', 'best', 'interests', 'of', 'us', '(the', 'patient)', 'and', 'you', 'have', 'the', 'ability', 'to', 'decline', 'anything', 'that', 'they', 'suggest', 'to', 'you.']\n",
            "Tokenized: ['[CLS]', 'i', 'can', '##t', 'speak', 'for', 'them', 'but', 'any', 'tests', 'or', 'appointments', 'they', 'recommend', 'are', 'probably', 'in', 'the', 'best', 'interests', 'of', 'us', '(', 'the', 'patient', ')', 'and', 'you', 'have', 'the', 'ability', 'to', 'decline', 'anything', 'that', 'they', 'suggest', 'to', 'you', '.', '[SEP]']\n",
            "Original: ['I', 'personally', 'have', 'had', 'wonderful', 'service', 'and', 'if', 'youre', 'truely', 'looking', 'for', 'a', 'FAMILY', 'practice...Warner', 'Family', 'is', 'the', 'place', 'for', 'you.']\n",
            "Tokenized: ['[CLS]', 'i', 'personally', 'have', 'had', 'wonderful', 'service', 'and', 'if', 'your', '##e', 'true', '##ly', 'looking', 'for', 'a', 'family', 'practice', '.', '.', '.', 'warner', 'family', 'is', 'the', 'place', 'for', 'you', '.', '[SEP]']\n",
            "Original: ['Not', 'a', 'clothing', 'store']\n",
            "Tokenized: ['[CLS]', 'not', 'a', 'clothing', 'store', '[SEP]']\n",
            "Original: ['Be', 'more', 'careful', 'when', 'you', 'write', 'reviews-', 'this', 'is', 'an', 'accounting', 'group,', 'not', 'Hollister', 'the', 'clothing', 'store.']\n",
            "Tokenized: ['[CLS]', 'be', 'more', 'careful', 'when', 'you', 'write', 'reviews', '-', 'this', 'is', 'an', 'accounting', 'group', ',', 'not', 'hollis', '##ter', 'the', 'clothing', 'store', '.', '[SEP]']\n",
            "Original: ['Good', 'quality', 'Indian', 'food', 'in', 'a', 'pleasant', 'environment']\n",
            "Tokenized: ['[CLS]', 'good', 'quality', 'indian', 'food', 'in', 'a', 'pleasant', 'environment', '[SEP]']\n",
            "Original: ['Whatever', 'you', 'order,', 'you', 'will', 'LOVE!']\n",
            "Tokenized: ['[CLS]', 'whatever', 'you', 'order', ',', 'you', 'will', 'love', '!', '[SEP]']\n",
            "Original: ['Pho-nomenal!!']\n",
            "Tokenized: ['[CLS]', 'ph', '##o', '-', 'no', '##men', '##al', '!', '!', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'eating', 'Pho', 'for', 'almost', 'my', 'entire', 'life', 'and', \"I've\", 'always', 'gone', 'to', 'the', 'Pho', 'places', 'in', 'south', 'philly', 'and', 'off', 'the', 'boulevard', 'and', 'even', 'the', 'other', 'one', 'in', 'chinatown,', 'but', 'when', 'i', 'tried', 'this', 'pho', 'place,', 'it', 'blew', 'the', 'other', 'pho', 'houses', 'away!!']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'eating', 'ph', '##o', 'for', 'almost', 'my', 'entire', 'life', 'and', 'i', \"'\", 've', 'always', 'gone', 'to', 'the', 'ph', '##o', 'places', 'in', 'south', 'phil', '##ly', 'and', 'off', 'the', 'boulevard', 'and', 'even', 'the', 'other', 'one', 'in', 'chinatown', ',', 'but', 'when', 'i', 'tried', 'this', 'ph', '##o', 'place', ',', 'it', 'blew', 'the', 'other', 'ph', '##o', 'houses', 'away', '!', '!', '[SEP]']\n",
            "Original: ['all', 'of', 'the', 'pho', 'places', 'taste', 'the', 'same', 'to', 'me,', 'so', 'what', 'seperates', 'one', 'from', 'the', 'other', 'is', 'the', 'service', 'and', 'the', 'price.']\n",
            "Tokenized: ['[CLS]', 'all', 'of', 'the', 'ph', '##o', 'places', 'taste', 'the', 'same', 'to', 'me', ',', 'so', 'what', 'sep', '##erate', '##s', 'one', 'from', 'the', 'other', 'is', 'the', 'service', 'and', 'the', 'price', '.', '[SEP]']\n",
            "Original: ['The', 'service', 'here', 'is', 'incredible', 'compared', 'to', 'the', 'other', 'places.']\n",
            "Tokenized: ['[CLS]', 'the', 'service', 'here', 'is', 'incredible', 'compared', 'to', 'the', 'other', 'places', '.', '[SEP]']\n",
            "Original: ['the', 'only', 'down', 'fall', 'of', 'this', 'pho', 'house', 'is', 'the', 'difficulty', 'in', 'finding', 'parking', 'in', 'chinatown.']\n",
            "Tokenized: ['[CLS]', 'the', 'only', 'down', 'fall', 'of', 'this', 'ph', '##o', 'house', 'is', 'the', 'difficulty', 'in', 'finding', 'parking', 'in', 'chinatown', '.', '[SEP]']\n",
            "Original: ['remember', 'to', 'bring', 'cash', 'since', 'they', \"don't\", 'take', 'debit', 'or', 'credit.']\n",
            "Tokenized: ['[CLS]', 'remember', 'to', 'bring', 'cash', 'since', 'they', 'don', \"'\", 't', 'take', 'de', '##bit', 'or', 'credit', '.', '[SEP]']\n",
            "Original: ['hope', 'this', 'helps!!']\n",
            "Tokenized: ['[CLS]', 'hope', 'this', 'helps', '!', '!', '[SEP]']\n",
            "Original: ['We', 'have', 'stayed', 'at', 'Tanglewood', 'for', 'many', 'years', 'now.']\n",
            "Tokenized: ['[CLS]', 'we', 'have', 'stayed', 'at', 'tangle', '##wood', 'for', 'many', 'years', 'now', '.', '[SEP]']\n",
            "Original: ['We', 'go', 'over', 'about', '5', 'times', 'a', 'year.']\n",
            "Tokenized: ['[CLS]', 'we', 'go', 'over', 'about', '5', 'times', 'a', 'year', '.', '[SEP]']\n",
            "Original: ['We', 'have', 'never', 'had', 'a', 'problem', 'with', 'the', 'cabins.']\n",
            "Tokenized: ['[CLS]', 'we', 'have', 'never', 'had', 'a', 'problem', 'with', 'the', 'cabins', '.', '[SEP]']\n",
            "Original: ['They', 'are', 'always', 'so', 'helpful.']\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'always', 'so', 'helpful', '.', '[SEP]']\n",
            "Original: ['The', 'cabins', 'have', 'always', 'been', 'clean.']\n",
            "Tokenized: ['[CLS]', 'the', 'cabins', 'have', 'always', 'been', 'clean', '.', '[SEP]']\n",
            "Original: ['Helen', 'is', 'a', 'wonderful', 'place', 'to', 'take', 'you', 'family.']\n",
            "Tokenized: ['[CLS]', 'helen', 'is', 'a', 'wonderful', 'place', 'to', 'take', 'you', 'family', '.', '[SEP]']\n",
            "Original: ['We', 'recommend', 'these', 'cabins!']\n",
            "Tokenized: ['[CLS]', 'we', 'recommend', 'these', 'cabins', '!', '[SEP]']\n",
            "Original: ['Bait', 'and', 'switch,', 'untrained', 'workers']\n",
            "Tokenized: ['[CLS]', 'bait', 'and', 'switch', ',', 'un', '##train', '##ed', 'workers', '[SEP]']\n",
            "Original: ['Called', 'the', 'Bonanza', 'store', '2', 'weeks', 'ago,', 'before', 'I', 'ripped', 'out', '350sqft', 'of', 'ceramic', 'tile...', 'was', 'told', 'I', 'would', 'need', 'a', '\"dual', 'head', 'concrete', 'grinder\"', 'to', 'remove', 'thinset', 'and', 'make', 'a', 'nice', '\"finished\"', 'look', '(ready', 'for', 'concrete', 'stain).']\n",
            "Tokenized: ['[CLS]', 'called', 'the', 'bon', '##anza', 'store', '2', 'weeks', 'ago', ',', 'before', 'i', 'ripped', 'out', '350', '##s', '##q', '##ft', 'of', 'ceramic', 'tile', '.', '.', '.', 'was', 'told', 'i', 'would', 'need', 'a', '\"', 'dual', 'head', 'concrete', 'grind', '##er', '\"', 'to', 'remove', 'thin', '##set', 'and', 'make', 'a', 'nice', '\"', 'finished', '\"', 'look', '(', 'ready', 'for', 'concrete', 'stain', ')', '.', '[SEP]']\n",
            "Original: ['Was', 'quoted', '$55', 'all', 'inclusive', 'of', 'grinder', 'inserts,', 'etc.']\n",
            "Tokenized: ['[CLS]', 'was', 'quoted', '$', '55', 'all', 'inclusive', 'of', 'grind', '##er', 'insert', '##s', ',', 'etc', '.', '[SEP]']\n",
            "Original: ['No', 'problem,', 'sounded', 'like', \"it's\", 'done', 'every', 'day.']\n",
            "Tokenized: ['[CLS]', 'no', 'problem', ',', 'sounded', 'like', 'it', \"'\", 's', 'done', 'every', 'day', '.', '[SEP]']\n",
            "Original: ['Will', 'look', 'beautiful.']\n",
            "Tokenized: ['[CLS]', 'will', 'look', 'beautiful', '.', '[SEP]']\n",
            "Original: ['Got', 'the', 'tile', 'ripped', 'out,', 'call', 'today,', 'now', 'all', 'the', 'sudden', 'this', 'grinder', \"won't\", 'leave', 'a', 'finished', 'look', 'AND', \"it's\", '$125', 'PLUS', 'around', '$75', 'for', 'the', 'inserts.']\n",
            "Tokenized: ['[CLS]', 'got', 'the', 'tile', 'ripped', 'out', ',', 'call', 'today', ',', 'now', 'all', 'the', 'sudden', 'this', 'grind', '##er', 'won', \"'\", 't', 'leave', 'a', 'finished', 'look', 'and', 'it', \"'\", 's', '$', '125', 'plus', 'around', '$', '75', 'for', 'the', 'insert', '##s', '.', '[SEP]']\n",
            "Original: ['I', 'can', 'rent', 'another', 'machine', 'for', 'like', '$60', 'that', 'will', 'give', 'it', 'a', 'finished', 'look.']\n",
            "Tokenized: ['[CLS]', 'i', 'can', 'rent', 'another', 'machine', 'for', 'like', '$', '60', 'that', 'will', 'give', 'it', 'a', 'finished', 'look', '.', '[SEP]']\n",
            "Original: ['So', 'from', '$55', 'to', '$260?']\n",
            "Tokenized: ['[CLS]', 'so', 'from', '$', '55', 'to', '$', '260', '?', '[SEP]']\n",
            "Original: ['Are', 'they', 'serious?']\n",
            "Tokenized: ['[CLS]', 'are', 'they', 'serious', '?', '[SEP]']\n",
            "Original: ['I', 'feel', 'like', 'they', \"didn't\", 'tell', 'me', 'the', 'pitfalls', 'before', 'I', 'pulled', 'out', 'this', 'tile', 'and', 'now', 'that', 'I', 'have', 'no', 'other', 'options', 'they', 'want', '5', 'TIMES', 'the', 'price?']\n",
            "Tokenized: ['[CLS]', 'i', 'feel', 'like', 'they', 'didn', \"'\", 't', 'tell', 'me', 'the', 'pit', '##falls', 'before', 'i', 'pulled', 'out', 'this', 'tile', 'and', 'now', 'that', 'i', 'have', 'no', 'other', 'options', 'they', 'want', '5', 'times', 'the', 'price', '?', '[SEP]']\n",
            "Original: ['Either', 'these', 'people', \"don't\", 'know', 'anything', 'about', 'what', 'they', 'are', 'renting,', 'or', 'worse-', 'they', 'are', 'bait', 'and', 'switching.']\n",
            "Tokenized: ['[CLS]', 'either', 'these', 'people', 'don', \"'\", 't', 'know', 'anything', 'about', 'what', 'they', 'are', 'rent', '##ing', ',', 'or', 'worse', '-', 'they', 'are', 'bait', 'and', 'switching', '.', '[SEP]']\n",
            "Original: ['Poor', 'Taste']\n",
            "Tokenized: ['[CLS]', 'poor', 'taste', '[SEP]']\n",
            "Original: ['There', 'is', 'no', 'lower', 'rating', 'for', \"Noonan's\", 'Liquor,', 'owners', 'and', 'employees.']\n",
            "Tokenized: ['[CLS]', 'there', 'is', 'no', 'lower', 'rating', 'for', 'noon', '##an', \"'\", 's', 'liquor', ',', 'owners', 'and', 'employees', '.', '[SEP]']\n",
            "Original: ['A', 'negative', 'number', 'is', 'not', 'available.']\n",
            "Tokenized: ['[CLS]', 'a', 'negative', 'number', 'is', 'not', 'available', '.', '[SEP]']\n",
            "Original: ['CLH']\n",
            "Tokenized: ['[CLS]', 'cl', '##h', '[SEP]']\n",
            "Original: ['Bulwark', 'regarding', 'service', 'by', 'Eric']\n",
            "Tokenized: ['[CLS]', 'bu', '##l', '##wark', 'regarding', 'service', 'by', 'eric', '[SEP]']\n",
            "Original: ['Just', 'wanted', 'you', 'to', 'know', 'that', 'Eric', 'came', 'by', 'as', 'scheduled', 'today', 'and', 'sprayed', 'our', 'house', 'for', 'scorpions.']\n",
            "Tokenized: ['[CLS]', 'just', 'wanted', 'you', 'to', 'know', 'that', 'eric', 'came', 'by', 'as', 'scheduled', 'today', 'and', 'sprayed', 'our', 'house', 'for', 'scorpion', '##s', '.', '[SEP]']\n",
            "Original: ['He', 'seemed', 'to', 'understand', 'how', 'important', 'it', 'was', 'for', 'us', 'to', 'make', 'sure', 'the', 'whole', 'house', 'was', 'sprayed', 'so', 'he', 'took', 'his', 'time.']\n",
            "Tokenized: ['[CLS]', 'he', 'seemed', 'to', 'understand', 'how', 'important', 'it', 'was', 'for', 'us', 'to', 'make', 'sure', 'the', 'whole', 'house', 'was', 'sprayed', 'so', 'he', 'took', 'his', 'time', '.', '[SEP]']\n",
            "Original: ['Thank', 'you!']\n",
            "Tokenized: ['[CLS]', 'thank', 'you', '!', '[SEP]']\n",
            "Original: ['Best', 'ceviche', 'that', \"I'd\", 'had', 'so', 'far!', ':)']\n",
            "Tokenized: ['[CLS]', 'best', 'ce', '##vich', '##e', 'that', 'i', \"'\", 'd', 'had', 'so', 'far', '!', ':', ')', '[SEP]']\n",
            "Original: ['MUST', 'READ', '-', 'Do', 'not', 'waste', 'your', 'time', 'in', 'this', 'store.']\n",
            "Tokenized: ['[CLS]', 'must', 'read', '-', 'do', 'not', 'waste', 'your', 'time', 'in', 'this', 'store', '.', '[SEP]']\n",
            "Original: ['At', 'my', 'appointment', 'the', 'girl', 'helping', 'me', 'was', 'unable', 'to', 'adequately', 'lace', 'up', 'some', 'of', 'the', 'dresses.']\n",
            "Tokenized: ['[CLS]', 'at', 'my', 'appointment', 'the', 'girl', 'helping', 'me', 'was', 'unable', 'to', 'adequately', 'lace', 'up', 'some', 'of', 'the', 'dresses', '.', '[SEP]']\n",
            "Original: ['They', 'felt', 'like', 'they', 'were', 'going', 'to', 'fall', 'off', 'of', 'me', 'and', 'it', 'was', 'very', 'difficult', 'to', 'see', 'what', 'I', 'would', 'actually', 'look', 'like', 'were', 'I', 'to', 'purchase', 'some', 'of', 'these', 'dresses.']\n",
            "Tokenized: ['[CLS]', 'they', 'felt', 'like', 'they', 'were', 'going', 'to', 'fall', 'off', 'of', 'me', 'and', 'it', 'was', 'very', 'difficult', 'to', 'see', 'what', 'i', 'would', 'actually', 'look', 'like', 'were', 'i', 'to', 'purchase', 'some', 'of', 'these', 'dresses', '.', '[SEP]']\n",
            "Original: ['I', 'thought', 'it', 'would', 'be', 'a', 'good', 'idea', 'to', 'see', 'how', 'a', 'few', 'that', 'I', 'liked', 'would', 'look', 'like', 'on', 'a', 'model', '(by', 'looking', 'the', 'dress', 'up', 'online).']\n",
            "Tokenized: ['[CLS]', 'i', 'thought', 'it', 'would', 'be', 'a', 'good', 'idea', 'to', 'see', 'how', 'a', 'few', 'that', 'i', 'liked', 'would', 'look', 'like', 'on', 'a', 'model', '(', 'by', 'looking', 'the', 'dress', 'up', 'online', ')', '.', '[SEP]']\n",
            "Original: ['So,', 'as', 'I', 'was', 'leaving', 'I', 'asked', 'for', 'the', 'designer/dress', 'name', 'or', 'style', 'number', 'associated', 'with', 'my', 'top', 'picks.']\n",
            "Tokenized: ['[CLS]', 'so', ',', 'as', 'i', 'was', 'leaving', 'i', 'asked', 'for', 'the', 'designer', '/', 'dress', 'name', 'or', 'style', 'number', 'associated', 'with', 'my', 'top', 'picks', '.', '[SEP]']\n",
            "Original: ['They', 'said', 'they', 'were', '\"unable', 'to', 'tell', 'me', 'until', 'they', 'ordered', 'my', 'dress\".']\n",
            "Tokenized: ['[CLS]', 'they', 'said', 'they', 'were', '\"', 'unable', 'to', 'tell', 'me', 'until', 'they', 'ordered', 'my', 'dress', '\"', '.', '[SEP]']\n",
            "Original: ['Hmmm...', 'A', 'person', 'cannot', 'call', 'a', 'company,', 'if', 'you', 'have', 'no', 'idea', 'its', 'name', '(since', 'the', 'designer', 'is', 'unknown...', 'SUPPOSEDLY),', 'and', 'order', 'a', 'gown', 'without', 'a', 'dress', 'name', 'or', 'style', 'number.']\n",
            "Tokenized: ['[CLS]', 'hmm', '##m', '.', '.', '.', 'a', 'person', 'cannot', 'call', 'a', 'company', ',', 'if', 'you', 'have', 'no', 'idea', 'its', 'name', '(', 'since', 'the', 'designer', 'is', 'unknown', '.', '.', '.', 'supposedly', ')', ',', 'and', 'order', 'a', 'gown', 'without', 'a', 'dress', 'name', 'or', 'style', 'number', '.', '[SEP]']\n",
            "Original: ['Do', 'other', 'brides', 'fall', 'for', 'this???']\n",
            "Tokenized: ['[CLS]', 'do', 'other', 'bride', '##s', 'fall', 'for', 'this', '?', '?', '?', '[SEP]']\n",
            "Original: ['They', 'either:', 'a)', \"don't\", 'want', 'to', 'give', 'it', 'to', 'me', 'because', 'they', \"don't\", 'want', 'me', 'purchasing', 'the', 'dress', 'elsewhere', 'or', 'b)', 'are', 'recreating', 'the', 'dresses', 'themselves', '(ie', 'STEALING', 'other', \"designers'\", 'dress', 'designs', 'and', '\"filling', 'the', 'orders\"', 'by', 'their', 'own', 'seamstresses).']\n",
            "Tokenized: ['[CLS]', 'they', 'either', ':', 'a', ')', 'don', \"'\", 't', 'want', 'to', 'give', 'it', 'to', 'me', 'because', 'they', 'don', \"'\", 't', 'want', 'me', 'purchasing', 'the', 'dress', 'elsewhere', 'or', 'b', ')', 'are', 'rec', '##rea', '##ting', 'the', 'dresses', 'themselves', '(', 'ie', 'stealing', 'other', 'designers', \"'\", 'dress', 'designs', 'and', '\"', 'filling', 'the', 'orders', '\"', 'by', 'their', 'own', 'seam', '##st', '##resses', ')', '.', '[SEP]']\n",
            "Original: [\"I'm\", 'no', 'detective', 'but...', 'uhh...', 'seriously?!?']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'm', 'no', 'detective', 'but', '.', '.', '.', 'uh', '##h', '.', '.', '.', 'seriously', '?', '!', '?', '[SEP]']\n",
            "Original: ['Whatever', 'type', 'of', 'operation', 'they', 'are', 'running,', \"I'm\", 'not', 'interested', 'and', 'if', \"you're\", 'smart,', 'you', \"won't\", 'be', 'either.']\n",
            "Tokenized: ['[CLS]', 'whatever', 'type', 'of', 'operation', 'they', 'are', 'running', ',', 'i', \"'\", 'm', 'not', 'interested', 'and', 'if', 'you', \"'\", 're', 'smart', ',', 'you', 'won', \"'\", 't', 'be', 'either', '.', '[SEP]']\n",
            "Original: ['What', 'a', 'waste', 'of', 'TIME.']\n",
            "Tokenized: ['[CLS]', 'what', 'a', 'waste', 'of', 'time', '.', '[SEP]']\n",
            "Original: ['Aside', 'from', 'that', 'little', '*mystery*,', 'one', 'of', 'the', 'sales', 'ladies', 'was', 'quite', 'comfortable', 'telling', 'me', 'how', 'wrong', 'I', 'was', 'about', 'how', 'another', 'dress', 'that', 'I', 'loved', 'compared', 'to', 'one', 'of', 'her', 'dresses', 'that', 'I', 'was', 'trying', 'on.']\n",
            "Tokenized: ['[CLS]', 'aside', 'from', 'that', 'little', '*', 'mystery', '*', ',', 'one', 'of', 'the', 'sales', 'ladies', 'was', 'quite', 'comfortable', 'telling', 'me', 'how', 'wrong', 'i', 'was', 'about', 'how', 'another', 'dress', 'that', 'i', 'loved', 'compared', 'to', 'one', 'of', 'her', 'dresses', 'that', 'i', 'was', 'trying', 'on', '.', '[SEP]']\n",
            "Original: ['Somehow,', 'since', 'she', 'supposedly', \"doesn't\", 'know', 'any', 'names', 'of', 'designers/dresses,', 'after', 'I', 'told', 'her', 'the', 'designer', 'and', 'dress', 'name', 'of', 'the', 'one', 'I', 'was', 'comparing,', 'she', 'knew', '\"exactly', 'which', 'dress\"', 'I', 'was', 'referring', 'to', 'and', 'disagreed', 'with', 'my', 'observation;', 'she', 'said', 'that', 'the', 'bodice', 'did', 'come', 'as', 'low', 'as', 'the', 'one', 'I', 'had', 'on.']\n",
            "Tokenized: ['[CLS]', 'somehow', ',', 'since', 'she', 'supposedly', 'doesn', \"'\", 't', 'know', 'any', 'names', 'of', 'designers', '/', 'dresses', ',', 'after', 'i', 'told', 'her', 'the', 'designer', 'and', 'dress', 'name', 'of', 'the', 'one', 'i', 'was', 'comparing', ',', 'she', 'knew', '\"', 'exactly', 'which', 'dress', '\"', 'i', 'was', 'referring', 'to', 'and', 'disagreed', 'with', 'my', 'observation', ';', 'she', 'said', 'that', 'the', 'bo', '##dice', 'did', 'come', 'as', 'low', 'as', 'the', 'one', 'i', 'had', 'on', '.', '[SEP]']\n",
            "Original: ['My', 'point:', 'Even', 'if', 'I', 'was', 'wrong,', \"don't\", 'sit', 'there', 'and', 'argue', 'with', 'the', 'customer.']\n",
            "Tokenized: ['[CLS]', 'my', 'point', ':', 'even', 'if', 'i', 'was', 'wrong', ',', 'don', \"'\", 't', 'sit', 'there', 'and', 'argue', 'with', 'the', 'customer', '.', '[SEP]']\n",
            "Original: ['Say', 'something', 'like,', '\"Huh.']\n",
            "Tokenized: ['[CLS]', 'say', 'something', 'like', ',', '\"', 'huh', '.', '[SEP]']\n",
            "Original: ['I', \"didn't\", 'think', 'so', 'but', 'you', 'could', 'be', 'right.\"']\n",
            "Tokenized: ['[CLS]', 'i', 'didn', \"'\", 't', 'think', 'so', 'but', 'you', 'could', 'be', 'right', '.', '\"', '[SEP]']\n",
            "Original: ['Unless', 'you', 'want', 'to', 'take', 'the', '\"tell', 'the', 'customer', 'how', 'wrong', 'she', 'is', 'and', 'try', 'and', 'force', 'her', 'into', 'a', 'dress', \"she's\", 'obviously', 'not', 'loving\"', 'approach', 'which', 'will', 'likely', 'get', 'you...', 'uh...', 'nowhere.']\n",
            "Tokenized: ['[CLS]', 'unless', 'you', 'want', 'to', 'take', 'the', '\"', 'tell', 'the', 'customer', 'how', 'wrong', 'she', 'is', 'and', 'try', 'and', 'force', 'her', 'into', 'a', 'dress', 'she', \"'\", 's', 'obviously', 'not', 'loving', '\"', 'approach', 'which', 'will', 'likely', 'get', 'you', '.', '.', '.', 'uh', '.', '.', '.', 'nowhere', '.', '[SEP]']\n",
            "Original: ['Seriously:', 'do', 'not', 'waste', 'your', 'time.']\n",
            "Tokenized: ['[CLS]', 'seriously', ':', 'do', 'not', 'waste', 'your', 'time', '.', '[SEP]']\n",
            "Original: ['Other', 'shops', 'around', 'this', 'city', 'have', 'MUCH', 'NICER', 'and', 'more', 'TRANSPARENT', 'owners.']\n",
            "Tokenized: ['[CLS]', 'other', 'shops', 'around', 'this', 'city', 'have', 'much', 'nice', '##r', 'and', 'more', 'transparent', 'owners', '.', '[SEP]']\n",
            "Original: ['Not', 'owners', 'that', 'seem', 'like', 'they', 'have', 'something', 'to', 'hide', 'and', 'know', 'nothing', 'about', 'common', 'courtesy', 'and', 'customer', 'service.']\n",
            "Tokenized: ['[CLS]', 'not', 'owners', 'that', 'seem', 'like', 'they', 'have', 'something', 'to', 'hide', 'and', 'know', 'nothing', 'about', 'common', 'courtesy', 'and', 'customer', 'service', '.', '[SEP]']\n",
            "Original: ['I', 'felt', 'very', 'much', 'like', 'Wedding', 'Gallery', 'was', 'being', 'dishonest', 'and', 'I', \"wouldn't\", 'trust', 'them', 'to', 'lace', 'me', 'up', 'in', 'another', 'gown', 'let', 'alone', 'trust', 'them', 'with', 'the', 'gown', 'I', 'will', 'wear', 'on', 'the', 'most', 'important', 'day', 'of', 'my', 'life.']\n",
            "Tokenized: ['[CLS]', 'i', 'felt', 'very', 'much', 'like', 'wedding', 'gallery', 'was', 'being', 'dish', '##ones', '##t', 'and', 'i', 'wouldn', \"'\", 't', 'trust', 'them', 'to', 'lace', 'me', 'up', 'in', 'another', 'gown', 'let', 'alone', 'trust', 'them', 'with', 'the', 'gown', 'i', 'will', 'wear', 'on', 'the', 'most', 'important', 'day', 'of', 'my', 'life', '.', '[SEP]']\n",
            "Original: ['Favorite', 'DD', 'spot', 'in', 'the', 'area!']\n",
            "Tokenized: ['[CLS]', 'favorite', 'dd', 'spot', 'in', 'the', 'area', '!', '[SEP]']\n",
            "Original: ['Deep', 'tissue', 'massage', 'helps', 'with', 'pain', 'in', 'neck', 'and', 'shoulders']\n",
            "Tokenized: ['[CLS]', 'deep', 'tissue', 'massage', 'helps', 'with', 'pain', 'in', 'neck', 'and', 'shoulders', '[SEP]']\n",
            "Original: ['Seth', 'provides', 'deep', 'tissue', 'massage', 'which', 'has', 'significantly', 'reduced', 'the', 'pain', 'in', 'my', 'neck', 'and', 'shoulders', 'and', 'added', 'flexibility', 'and', 'movement', 'back', 'to', 'the', 'area.']\n",
            "Tokenized: ['[CLS]', 'seth', 'provides', 'deep', 'tissue', 'massage', 'which', 'has', 'significantly', 'reduced', 'the', 'pain', 'in', 'my', 'neck', 'and', 'shoulders', 'and', 'added', 'flexibility', 'and', 'movement', 'back', 'to', 'the', 'area', '.', '[SEP]']\n",
            "Original: ['He', 'listens', 'and', 'is', 'excellent', 'in', 'diagnosing,', 'addressing', 'and', 'explaining', 'the', 'specific', 'issues', 'and', 'suggesting', 'exercises', 'to', 'use.']\n",
            "Tokenized: ['[CLS]', 'he', 'listen', '##s', 'and', 'is', 'excellent', 'in', 'dia', '##gno', '##sing', ',', 'addressing', 'and', 'explaining', 'the', 'specific', 'issues', 'and', 'suggesting', 'exercises', 'to', 'use', '.', '[SEP]']\n",
            "Original: ['Place', 'is', 'legit.']\n",
            "Tokenized: ['[CLS]', 'place', 'is', 'leg', '##it', '.', '[SEP]']\n",
            "Original: ['We', 'got', 'upgraded', 'to', 'a', 'corner', 'suite!']\n",
            "Tokenized: ['[CLS]', 'we', 'got', 'upgraded', 'to', 'a', 'corner', 'suite', '!', '[SEP]']\n",
            "Original: ['Room', 'was', 'amazing.']\n",
            "Tokenized: ['[CLS]', 'room', 'was', 'amazing', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'used', 'Bright', 'Futures', 'for', 'the', 'last', '7', 'years.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'used', 'bright', 'futures', 'for', 'the', 'last', '7', 'years', '.', '[SEP]']\n",
            "Original: ['I', 'have', '3', 'children', 'there', 'and', 'they', 'are', 'the', 'Best.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', '3', 'children', 'there', 'and', 'they', 'are', 'the', 'best', '.', '[SEP]']\n",
            "Original: ['They', 'are', 'like', 'family.']\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'like', 'family', '.', '[SEP]']\n",
            "Original: ['Your', 'children', 'will', 'be', 'taken', 'care', 'of', 'and', 'loved', 'by', 'a', 'professional', 'staff.']\n",
            "Tokenized: ['[CLS]', 'your', 'children', 'will', 'be', 'taken', 'care', 'of', 'and', 'loved', 'by', 'a', 'professional', 'staff', '.', '[SEP]']\n",
            "Original: ['Beware', 'of', 'Sharayu']\n",
            "Tokenized: ['[CLS]', 'be', '##ware', 'of', 'sha', '##ray', '##u', '[SEP]']\n",
            "Original: ['Hopless', 'service,', 'at', 'the', 'time', 'of', 'booking', 'my', 'car', 'a', 'lot', 'was', 'promised', 'but', 'delivered', 'not', 'even', 'one', 'tenth', 'of', 'what', 'was', 'promised.']\n",
            "Tokenized: ['[CLS]', 'hop', '##less', 'service', ',', 'at', 'the', 'time', 'of', 'booking', 'my', 'car', 'a', 'lot', 'was', 'promised', 'but', 'delivered', 'not', 'even', 'one', 'tenth', 'of', 'what', 'was', 'promised', '.', '[SEP]']\n",
            "Original: ['Apart', 'from', 'that', 'in', 'spite', 'of', 'my', 'repeated', 'attempts', 'I', 'could', 'not', 'get', 'in', 'touch', 'with', 'the', 'manager', 'to', 'even', 'lodge', 'an', 'official', 'complaint.']\n",
            "Tokenized: ['[CLS]', 'apart', 'from', 'that', 'in', 'spite', 'of', 'my', 'repeated', 'attempts', 'i', 'could', 'not', 'get', 'in', 'touch', 'with', 'the', 'manager', 'to', 'even', 'lodge', 'an', 'official', 'complaint', '.', '[SEP]']\n",
            "Original: ['My', 'advise', 'to', 'all', 'is', 'I', 'have', 'fallen', 'into', 'this', 'trap', '...', 'pl', 'ensure', 'you', 'dont!!!']\n",
            "Tokenized: ['[CLS]', 'my', 'advise', 'to', 'all', 'is', 'i', 'have', 'fallen', 'into', 'this', 'trap', '.', '.', '.', 'pl', 'ensure', 'you', 'don', '##t', '!', '!', '!', '[SEP]']\n",
            "Original: ['On', 'time,', 'Clean', 'and', 'very', 'nice']\n",
            "Tokenized: ['[CLS]', 'on', 'time', ',', 'clean', 'and', 'very', 'nice', '[SEP]']\n",
            "Original: ['I', 'called', 'over', 'the', 'weekend', 'due', 'to', 'clogged', 'kitchen', 'sink.']\n",
            "Tokenized: ['[CLS]', 'i', 'called', 'over', 'the', 'weekend', 'due', 'to', 'cl', '##og', '##ged', 'kitchen', 'sink', '.', '[SEP]']\n",
            "Original: ['Scheduled', 'appointment', 'for', '8:30', 'Monday', 'morning.']\n",
            "Tokenized: ['[CLS]', 'scheduled', 'appointment', 'for', '8', ':', '30', 'monday', 'morning', '.', '[SEP]']\n",
            "Original: ['Rich', 'was', 'here', 'before', 'the', 'scheduled', 'time.']\n",
            "Tokenized: ['[CLS]', 'rich', 'was', 'here', 'before', 'the', 'scheduled', 'time', '.', '[SEP]']\n",
            "Original: ['He', 'was', 'very', 'clean,', 'very', 'nice', 'to', 'work', 'with', 'and', 'gave', 'a', 'very', 'reasonable', 'price.']\n",
            "Tokenized: ['[CLS]', 'he', 'was', 'very', 'clean', ',', 'very', 'nice', 'to', 'work', 'with', 'and', 'gave', 'a', 'very', 'reasonable', 'price', '.', '[SEP]']\n",
            "Original: ['HIGHLY', 'recommend.']\n",
            "Tokenized: ['[CLS]', 'highly', 'recommend', '.', '[SEP]']\n",
            "Original: ['thanks', 'Rich']\n",
            "Tokenized: ['[CLS]', 'thanks', 'rich', '[SEP]']\n",
            "Original: ['Seth', 'K.']\n",
            "Tokenized: ['[CLS]', 'seth', 'k', '.', '[SEP]']\n",
            "Original: ['My', 'friend', 'and', 'I', 'were', 'to', 'stay', 'here', 'for', 'a', 'girls', 'night,', 'catch', 'up', 'on', 'our', 'lives', 'evening.']\n",
            "Tokenized: ['[CLS]', 'my', 'friend', 'and', 'i', 'were', 'to', 'stay', 'here', 'for', 'a', 'girls', 'night', ',', 'catch', 'up', 'on', 'our', 'lives', 'evening', '.', '[SEP]']\n",
            "Original: ['We', 'live', 'within', '20', 'miles', 'of', 'the', 'hotel', 'and', 'wanted', 'to', 'get', 'away', 'from', 'our', 'responsibilities', 'for', 'the', 'night.']\n",
            "Tokenized: ['[CLS]', 'we', 'live', 'within', '20', 'miles', 'of', 'the', 'hotel', 'and', 'wanted', 'to', 'get', 'away', 'from', 'our', 'responsibilities', 'for', 'the', 'night', '.', '[SEP]']\n",
            "Original: ['Unfortunalty', 'my', 'husband', 'and', 'I', 'had', 'to', 'put', 'our', '13', 'year', 'old', 'lab', 'down', 'that', 'morning', 'and', 'we', 'were', 'not', 'expecting', 'this.']\n",
            "Tokenized: ['[CLS]', 'un', '##fort', '##una', '##lty', 'my', 'husband', 'and', 'i', 'had', 'to', 'put', 'our', '13', 'year', 'old', 'lab', 'down', 'that', 'morning', 'and', 'we', 'were', 'not', 'expecting', 'this', '.', '[SEP]']\n",
            "Original: ['My', 'friend', 'called', 'the', 'hotel', 'to', 'cancel', 'our', 'room', 'as', 'soon', 'as', 'I', 'called', 'her.']\n",
            "Tokenized: ['[CLS]', 'my', 'friend', 'called', 'the', 'hotel', 'to', 'cancel', 'our', 'room', 'as', 'soon', 'as', 'i', 'called', 'her', '.', '[SEP]']\n",
            "Original: ['They', 'said', 'that', 'we', 'were', 'to', 'be', 'charged', 'for', 'this', 'room', 'regardless', 'because', 'we', 'did', 'not', 'cancel', 'within', 'the', '72', 'hours.']\n",
            "Tokenized: ['[CLS]', 'they', 'said', 'that', 'we', 'were', 'to', 'be', 'charged', 'for', 'this', 'room', 'regardless', 'because', 'we', 'did', 'not', 'cancel', 'within', 'the', '72', 'hours', '.', '[SEP]']\n",
            "Original: ['I', 'called', 'them', 'back', 'a', 'few', 'hours', 'after', 'putting', 'my', 'Bodhi', 'down', 'and', 'they', 'still', \"wouldn't\", 'budge.']\n",
            "Tokenized: ['[CLS]', 'i', 'called', 'them', 'back', 'a', 'few', 'hours', 'after', 'putting', 'my', 'bo', '##dhi', 'down', 'and', 'they', 'still', 'wouldn', \"'\", 't', 'budge', '.', '[SEP]']\n",
            "Original: ['Times', 'are', 'hard,', 'I', 'know,', 'but', 'they', 'had', 'no', 'compassion.']\n",
            "Tokenized: ['[CLS]', 'times', 'are', 'hard', ',', 'i', 'know', ',', 'but', 'they', 'had', 'no', 'compassion', '.', '[SEP]']\n",
            "Original: ['I', 'called', '3', 'times', 'to', 'talked', 'to', 'a', 'manager,', 'never', 'a', 'call', 'back.']\n",
            "Tokenized: ['[CLS]', 'i', 'called', '3', 'times', 'to', 'talked', 'to', 'a', 'manager', ',', 'never', 'a', 'call', 'back', '.', '[SEP]']\n",
            "Original: ['I', 'emailed', '4', 'times,', 'never', 'a', 'response.']\n",
            "Tokenized: ['[CLS]', 'i', 'email', '##ed', '4', 'times', ',', 'never', 'a', 'response', '.', '[SEP]']\n",
            "Original: ['In', 'one', 'of', 'the', 'emails', 'I', 'attached', 'the', 'letter', 'from', 'the', \"Vet's\", 'that', 'expressed', 'their', 'sympathy,', 'this', 'hotel', 'did', 'nothing.']\n",
            "Tokenized: ['[CLS]', 'in', 'one', 'of', 'the', 'emails', 'i', 'attached', 'the', 'letter', 'from', 'the', 'vet', \"'\", 's', 'that', 'expressed', 'their', 'sympathy', ',', 'this', 'hotel', 'did', 'nothing', '.', '[SEP]']\n",
            "Original: ['I', 'even', 'emailed', 'Mackinaw', 'Tourist', 'and', 'nothing.']\n",
            "Tokenized: ['[CLS]', 'i', 'even', 'email', '##ed', 'mack', '##ina', '##w', 'tourist', 'and', 'nothing', '.', '[SEP]']\n",
            "Original: ['I', 'am', 'sure', 'this', 'is', 'a', 'good', 'place', 'to', 'stay', 'from', 'reading', 'the', 'other', 'reviews,', 'but', 'if', 'something', 'unexpected', 'happens', 'in', 'your', 'life,', 'they', 'will', 'not', 'care.']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'sure', 'this', 'is', 'a', 'good', 'place', 'to', 'stay', 'from', 'reading', 'the', 'other', 'reviews', ',', 'but', 'if', 'something', 'unexpected', 'happens', 'in', 'your', 'life', ',', 'they', 'will', 'not', 'care', '.', '[SEP]']\n",
            "Original: ['This', 'insurance', 'co.', 'Is', 'a', 'joke', '!!!']\n",
            "Tokenized: ['[CLS]', 'this', 'insurance', 'co', '.', 'is', 'a', 'joke', '!', '!', '!', '[SEP]']\n",
            "Original: ['They', 'have', 'absolutely', 'no', 'communication', 'skills', 'whatsoever', '.']\n",
            "Tokenized: ['[CLS]', 'they', 'have', 'absolutely', 'no', 'communication', 'skills', 'whatsoever', '.', '[SEP]']\n",
            "Original: ['if', 'you', \"don't\", 'mind', 'being', 'robbed', ',cheated', 'or', 'lied', 'to', 'then', 'this', 'is', 'the', 'company', 'for', 'you', '.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'don', \"'\", 't', 'mind', 'being', 'robbed', ',', 'cheated', 'or', 'lied', 'to', 'then', 'this', 'is', 'the', 'company', 'for', 'you', '.', '[SEP]']\n",
            "Original: ['They', 'will', 'make', 'every', 'attempt', 'to', 'misinform', 'and', 'misrepresent', 'themselves', '.']\n",
            "Tokenized: ['[CLS]', 'they', 'will', 'make', 'every', 'attempt', 'to', 'mis', '##in', '##form', 'and', 'mis', '##re', '##pres', '##ent', 'themselves', '.', '[SEP]']\n",
            "Original: ['They', 'make', 'up', 'excuses', 'in', 'hopes', 'to', 'confuse', 'their', 'policy', 'holders', 'with', 'misinformation', '.']\n",
            "Tokenized: ['[CLS]', 'they', 'make', 'up', 'excuses', 'in', 'hopes', 'to', 'confuse', 'their', 'policy', 'holders', 'with', 'mis', '##in', '##form', '##ation', '.', '[SEP]']\n",
            "Original: ['as', 'an', 'example', 'they', 'took', 'payment', 'for', '5', 'out', 'of', '6', 'monthly', 'plan', 'premiums', 'for', 'a', 'yearly', 'policy', 'and', 'cancelled', 'the', 'contract', 'for', 'the', 'remainder', 'of', 'the', 'policy', 'for', 'reasons', 'they', 'stated', 'was', 'not', 'receiving', 'information', 'on', 'other', 'licensed', 'drivers', 'in', 'the', 'household', '?']\n",
            "Tokenized: ['[CLS]', 'as', 'an', 'example', 'they', 'took', 'payment', 'for', '5', 'out', 'of', '6', 'monthly', 'plan', 'premium', '##s', 'for', 'a', 'yearly', 'policy', 'and', 'cancelled', 'the', 'contract', 'for', 'the', 'remainder', 'of', 'the', 'policy', 'for', 'reasons', 'they', 'stated', 'was', 'not', 'receiving', 'information', 'on', 'other', 'licensed', 'drivers', 'in', 'the', 'household', '?', '[SEP]']\n",
            "Original: ['I', 'personally', 'provided', 'the', 'request', 'on', 'four', 'separate', 'occasions', 'and', 'they', 'claim', 'there', 'is', 'a', 'glitch', 'in', 'their', 'systems', '?']\n",
            "Tokenized: ['[CLS]', 'i', 'personally', 'provided', 'the', 'request', 'on', 'four', 'separate', 'occasions', 'and', 'they', 'claim', 'there', 'is', 'a', 'g', '##lit', '##ch', 'in', 'their', 'systems', '?', '[SEP]']\n",
            "Original: ['Ifa', 'is', 'an', 'acronym', 'for', 'Im', 'a', 'F%#king', 'Assh@%$e', '!!!']\n",
            "Tokenized: ['[CLS]', 'if', '##a', 'is', 'an', 'acronym', 'for', 'im', 'a', 'f', '%', '#', 'king', 'ass', '##h', '@', '%', '$', 'e', '!', '!', '!', '[SEP]']\n",
            "Original: [\"I'll\", 'admit', 'I', \"wasn't\", 'expecting', 'much', 'from', 'this', 'place,', 'but', 'they', 'really', 'did', 'do', 'a', 'good', 'job.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'll', 'admit', 'i', 'wasn', \"'\", 't', 'expecting', 'much', 'from', 'this', 'place', ',', 'but', 'they', 'really', 'did', 'do', 'a', 'good', 'job', '.', '[SEP]']\n",
            "Original: ['The', 'chicken', 'cordon-blu', 'was', 'tasty', 'and', 'came', 'in', 'a', 'huge', 'portion', 'size', 'for', 'the', 'money.']\n",
            "Tokenized: ['[CLS]', 'the', 'chicken', 'cord', '##on', '-', 'blu', 'was', 'ta', '##sty', 'and', 'came', 'in', 'a', 'huge', 'portion', 'size', 'for', 'the', 'money', '.', '[SEP]']\n",
            "Original: ['Service', 'was', 'a', 'touch', 'slow,', 'but', 'friendly.']\n",
            "Tokenized: ['[CLS]', 'service', 'was', 'a', 'touch', 'slow', ',', 'but', 'friendly', '.', '[SEP]']\n",
            "Original: ['OMFG']\n",
            "Tokenized: ['[CLS]', 'om', '##f', '##g', '[SEP]']\n",
            "Original: ['I', 'FUCKING', 'HATE', 'THIS', 'PLACE', 'EVERY', 'TIME', 'i', 'GO', 'THIS', 'HOT', 'CHICK', 'SHOWS', 'UP', 'AND', 'i', 'MEAN', 'REALLY', 'HOT', 'BUT', 'SHE', 'IS', 'LIKE', 'REAAAALLY', 'DUMB', 'AND', 'THEN', 'THEIR', 'IS', 'THIS', 'OTHER', 'CHICK', 'THAT', 'IS', 'REALY', 'UGLY', 'BUT', 'SHE', 'IS', 'LIKE', 'SUPER', 'SMART', 'SHE', 'COULD', 'BE', 'A', 'SCIENTIST,', 'BUT', 'THEN', 'THEIR', 'IS', 'THIS', 'STONER', 'WHO', 'ALWAYS', 'COMES', 'HERE', 'HIGH', 'AND', 'HE', 'ALWAYS', 'BRINGS', 'HIS', 'FUCKING', 'DOG', 'WHO', 'IS', 'SO', 'HIGH', 'FROM', 'THE', 'SECOND', 'HAND', 'SMOKE', 'I', 'THINK', 'HE', 'IS', 'TRYING', 'TO', 'TALK.']\n",
            "Tokenized: ['[CLS]', 'i', 'fucking', 'hate', 'this', 'place', 'every', 'time', 'i', 'go', 'this', 'hot', 'chick', 'shows', 'up', 'and', 'i', 'mean', 'really', 'hot', 'but', 'she', 'is', 'like', 're', '##aa', '##aa', '##lly', 'dumb', 'and', 'then', 'their', 'is', 'this', 'other', 'chick', 'that', 'is', 'real', '##y', 'ugly', 'but', 'she', 'is', 'like', 'super', 'smart', 'she', 'could', 'be', 'a', 'scientist', ',', 'but', 'then', 'their', 'is', 'this', 'stone', '##r', 'who', 'always', 'comes', 'here', 'high', 'and', 'he', 'always', 'brings', 'his', 'fucking', 'dog', 'who', 'is', 'so', 'high', 'from', 'the', 'second', 'hand', 'smoke', 'i', 'think', 'he', 'is', 'trying', 'to', 'talk', '.', '[SEP]']\n",
            "Original: ['ANYWAY', 'WE', 'DRIVE', 'AROUND', 'IN', 'MY', 'VAN', 'AND', 'SOLVE', 'MYSTERYS', 'AND', 'SHIT']\n",
            "Tokenized: ['[CLS]', 'anyway', 'we', 'drive', 'around', 'in', 'my', 'van', 'and', 'solve', 'mystery', '##s', 'and', 'shit', '[SEP]']\n",
            "Original: ['exelent', 'Job']\n",
            "Tokenized: ['[CLS]', 'ex', '##ele', '##nt', 'job', '[SEP]']\n",
            "Original: ['\"Thank', 'you', 'so', 'much', 'for', 'the', 'superior', 'job', 'well', 'done.']\n",
            "Tokenized: ['[CLS]', '\"', 'thank', 'you', 'so', 'much', 'for', 'the', 'superior', 'job', 'well', 'done', '.', '[SEP]']\n",
            "Original: ['We', 'love', 'everything', 'about', 'the', 'fence.']\n",
            "Tokenized: ['[CLS]', 'we', 'love', 'everything', 'about', 'the', 'fence', '.', '[SEP]']\n",
            "Original: ['You', 'company', 'and', 'services', 'will', 'be', 'recommended', 'by', 'us', 'to', 'everyone.\"']\n",
            "Tokenized: ['[CLS]', 'you', 'company', 'and', 'services', 'will', 'be', 'recommended', 'by', 'us', 'to', 'everyone', '.', '\"', '[SEP]']\n",
            "Original: ['Furnace', 'repair']\n",
            "Tokenized: ['[CLS]', 'furnace', 'repair', '[SEP]']\n",
            "Original: ['Tiger', 'Heating', 'is', 'awesome.']\n",
            "Tokenized: ['[CLS]', 'tiger', 'heating', 'is', 'awesome', '.', '[SEP]']\n",
            "Original: ['I', 'had', 'John', 'and', 'Dustin', 'working', 'feverishly', 'to', 'get', 'my', 'exhaust', 'motor', 'replaced.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'john', 'and', 'dustin', 'working', 'fever', '##ishly', 'to', 'get', 'my', 'exhaust', 'motor', 'replaced', '.', '[SEP]']\n",
            "Original: ['These', 'guys', 'were', 'absolutely', 'professional.']\n",
            "Tokenized: ['[CLS]', 'these', 'guys', 'were', 'absolutely', 'professional', '.', '[SEP]']\n",
            "Original: ['John', 'was', 'here', 'in', '45', 'minutes', 'after', 'I', 'called', 'on', 'a', '10', 'below', 'zero', 'early', 'Sunday', 'morning.']\n",
            "Tokenized: ['[CLS]', 'john', 'was', 'here', 'in', '45', 'minutes', 'after', 'i', 'called', 'on', 'a', '10', 'below', 'zero', 'early', 'sunday', 'morning', '.', '[SEP]']\n",
            "Original: ['If', 'you', 'need', 'someone', 'to', 'help', 'you', 'out', 'with', 'your', 'heating', 'problems,', 'I', 'DEFINITELY', 'would', 'call', 'TIGER', 'HEATING', 'and', 'AIR.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'need', 'someone', 'to', 'help', 'you', 'out', 'with', 'your', 'heating', 'problems', ',', 'i', 'definitely', 'would', 'call', 'tiger', 'heating', 'and', 'air', '.', '[SEP]']\n",
            "Original: ['Absolutely', 'a', 'wonderful', 'company.']\n",
            "Tokenized: ['[CLS]', 'absolutely', 'a', 'wonderful', 'company', '.', '[SEP]']\n",
            "Original: ['My', 'family', 'and', 'I', 'thank', 'you!!!!!!!!!!']\n",
            "Tokenized: ['[CLS]', 'my', 'family', 'and', 'i', 'thank', 'you', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['never', 'response', 'the', 'phone', 'call']\n",
            "Tokenized: ['[CLS]', 'never', 'response', 'the', 'phone', 'call', '[SEP]']\n",
            "Original: ['I', 'think', 'that', 'there', 'pretty', 'good.']\n",
            "Tokenized: ['[CLS]', 'i', 'think', 'that', 'there', 'pretty', 'good', '.', '[SEP]']\n",
            "Original: ['First', 'of', 'all,', 'if', 'you', 'call', 'for', 'an', 'appointment', 'they', \"won't\", 'tell', 'you', 'to', 'call', 'back', 'a', 'month', 'later', 'so', 'you', 'can', 'then', 'make', 'one.']\n",
            "Tokenized: ['[CLS]', 'first', 'of', 'all', ',', 'if', 'you', 'call', 'for', 'an', 'appointment', 'they', 'won', \"'\", 't', 'tell', 'you', 'to', 'call', 'back', 'a', 'month', 'later', 'so', 'you', 'can', 'then', 'make', 'one', '.', '[SEP]']\n",
            "Original: ['They', 'also', \"don't\", 'tell', 'you', 'that', \"they're\", 'going', 'to', 'mail', 'you', 'an', '\"application\"', 'to', 'be', 'admitted', 'to', 'be', 'a', 'patient.']\n",
            "Tokenized: ['[CLS]', 'they', 'also', 'don', \"'\", 't', 'tell', 'you', 'that', 'they', \"'\", 're', 'going', 'to', 'mail', 'you', 'an', '\"', 'application', '\"', 'to', 'be', 'admitted', 'to', 'be', 'a', 'patient', '.', '[SEP]']\n",
            "Original: ['I', 'feel', 'that', 'the', 'way', 'some', 'doctors', 'offices', 'work', 'around', 'here', 'is', 'a', 'little', 'bit', 'absurd', 'so', \"I'm\", 'happy', 'that', 'these', 'guys', \"don't\", 'do', 'those', 'two', 'things', 'in', 'particlular.']\n",
            "Tokenized: ['[CLS]', 'i', 'feel', 'that', 'the', 'way', 'some', 'doctors', 'offices', 'work', 'around', 'here', 'is', 'a', 'little', 'bit', 'absurd', 'so', 'i', \"'\", 'm', 'happy', 'that', 'these', 'guys', 'don', \"'\", 't', 'do', 'those', 'two', 'things', 'in', 'part', '##ic', '##lu', '##lar', '.', '[SEP]']\n",
            "Original: ['Usually', 'you', 'can', 'be', 'seen', 'the', 'same', 'week', 'or', 'maybe', 'the', 'following', 'week.']\n",
            "Tokenized: ['[CLS]', 'usually', 'you', 'can', 'be', 'seen', 'the', 'same', 'week', 'or', 'maybe', 'the', 'following', 'week', '.', '[SEP]']\n",
            "Original: ['The', 'doctor', 'did', 'tell', 'me', 'that', 'my', 'male', 'pattern', 'baldness', 'is', 'due', 'to', 'my', 'mothers', 'father', 'because', 'the', 'gene', 'is', 'exclusively', 'passed', 'on', 'through', 'maternal', 'line', 'but', \"that's\", 'a', 'common', 'misconception', '(http://www.consumerreports.org/health/healthy-living/beauty-personal-care/hair-loss-10-08/hair-loss.htm).']\n",
            "Tokenized: ['[CLS]', 'the', 'doctor', 'did', 'tell', 'me', 'that', 'my', 'male', 'pattern', 'bald', '##ness', 'is', 'due', 'to', 'my', 'mothers', 'father', 'because', 'the', 'gene', 'is', 'exclusively', 'passed', 'on', 'through', 'maternal', 'line', 'but', 'that', \"'\", 's', 'a', 'common', 'mis', '##con', '##ception', '(', 'http', ':', '/', '/', 'www', '.', 'consumer', '##re', '##ports', '.', 'org', '/', 'health', '/', 'healthy', '-', 'living', '/', 'beauty', '-', 'personal', '-', 'care', '/', 'hair', '-', 'loss', '-', '10', '-', '08', '/', 'hair', '-', 'loss', '.', 'h', '##tm', ')', '.', '[SEP]']\n",
            "Original: ['In', 'general', 'I', 'would', 'say', 'that', 'the', 'staff', 'is', 'attentive', 'and', 'nice.']\n",
            "Tokenized: ['[CLS]', 'in', 'general', 'i', 'would', 'say', 'that', 'the', 'staff', 'is', 'at', '##ten', '##tive', 'and', 'nice', '.', '[SEP]']\n",
            "Original: ['But', 'then', 'again', 'I', 'was', 'nice', 'to', 'them', 'so', 'that', 'could', 'have', 'been', 'why.']\n",
            "Tokenized: ['[CLS]', 'but', 'then', 'again', 'i', 'was', 'nice', 'to', 'them', 'so', 'that', 'could', 'have', 'been', 'why', '.', '[SEP]']\n",
            "Original: ['Anyhow,', 'after', 'reading', 'some', 'of', 'the', 'other', 'reviews', 'it', 'seems', 'like', 'some', 'of', 'the', 'other', 'reviewers', 'are', 'expecting', 'mircles.']\n",
            "Tokenized: ['[CLS]', 'any', '##how', ',', 'after', 'reading', 'some', 'of', 'the', 'other', 'reviews', 'it', 'seems', 'like', 'some', 'of', 'the', 'other', 'reviewers', 'are', 'expecting', 'mir', '##cles', '.', '[SEP]']\n",
            "Original: ['If', 'you', 'want', 'miracles', \"you'll\", 'have', 'to', 'go', 'to', '21st', 'St.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'want', 'miracles', 'you', \"'\", 'll', 'have', 'to', 'go', 'to', '21st', 'st', '.', '[SEP]']\n",
            "Original: [\"It's\", 'a', 'pretty', 'typical', 'primary', 'care', 'office', 'but', 'for', 'the', 'area', \"it's\", 'very', 'good.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'a', 'pretty', 'typical', 'primary', 'care', 'office', 'but', 'for', 'the', 'area', 'it', \"'\", 's', 'very', 'good', '.', '[SEP]']\n",
            "Original: ['They', \"don't\", 'do', 'certain', 'things', 'that', 'are', 'really', 'annoying', 'about', 'other', 'offices.']\n",
            "Tokenized: ['[CLS]', 'they', 'don', \"'\", 't', 'do', 'certain', 'things', 'that', 'are', 'really', 'annoying', 'about', 'other', 'offices', '.', '[SEP]']\n",
            "Original: ['This', 'place', 'is', 'a', 'Rip-Off']\n",
            "Tokenized: ['[CLS]', 'this', 'place', 'is', 'a', 'rip', '-', 'off', '[SEP]']\n",
            "Original: ['I', 'brought', 'a', 'car', 'in', 'to', 'have', 'the', '\"check', 'engine\"', 'light', 'diagnosed', 'back', 'in', 'March', 'of', '2010.']\n",
            "Tokenized: ['[CLS]', 'i', 'brought', 'a', 'car', 'in', 'to', 'have', 'the', '\"', 'check', 'engine', '\"', 'light', 'diagnosed', 'back', 'in', 'march', 'of', '2010', '.', '[SEP]']\n",
            "Original: ['They', 'said', 'it', 'was', '\"plugs', 'and', 'wires\"', 'and', 'quoted', 'me', '$330', 'to', 'do', 'the', 'work,', 'including', 'parts.']\n",
            "Tokenized: ['[CLS]', 'they', 'said', 'it', 'was', '\"', 'plug', '##s', 'and', 'wires', '\"', 'and', 'quoted', 'me', '$', '330', 'to', 'do', 'the', 'work', ',', 'including', 'parts', '.', '[SEP]']\n",
            "Original: ['I', 'asked', 'why', 'so', 'high', 'and', 'they', 'said', 'it', 'was', 'due', 'to', 'the', 'labor', 'of', 'moving', 'things', 'out', 'of', 'the', 'way.']\n",
            "Tokenized: ['[CLS]', 'i', 'asked', 'why', 'so', 'high', 'and', 'they', 'said', 'it', 'was', 'due', 'to', 'the', 'labor', 'of', 'moving', 'things', 'out', 'of', 'the', 'way', '.', '[SEP]']\n",
            "Original: ['Those', 'things', 'ended', 'up', 'being', 'a', 'windsheild', 'washer', 'fluid', 'tank', '(1', 'screw)', 'and', 'the', 'air', 'filter', 'canister', '(4', 'spring', 'clips).']\n",
            "Tokenized: ['[CLS]', 'those', 'things', 'ended', 'up', 'being', 'a', 'winds', '##hei', '##ld', 'wash', '##er', 'fluid', 'tank', '(', '1', 'screw', ')', 'and', 'the', 'air', 'filter', 'can', '##ister', '(', '4', 'spring', 'clips', ')', '.', '[SEP]']\n",
            "Original: ['I', 'did', 'the', 'work', 'myself', 'for', '$50.']\n",
            "Tokenized: ['[CLS]', 'i', 'did', 'the', 'work', 'myself', 'for', '$', '50', '.', '[SEP]']\n",
            "Original: [\"There's\", 'no', 'excuse', 'for', 'that', 'kind', 'of', 'estimate.']\n",
            "Tokenized: ['[CLS]', 'there', \"'\", 's', 'no', 'excuse', 'for', 'that', 'kind', 'of', 'estimate', '.', '[SEP]']\n",
            "Original: ['I', 'love', 'this', 'place', 'lots', 'of', 'people', 'to', 'talk', 'to', 'and', 'school', 'is', 'across', 'the', 'street!']\n",
            "Tokenized: ['[CLS]', 'i', 'love', 'this', 'place', 'lots', 'of', 'people', 'to', 'talk', 'to', 'and', 'school', 'is', 'across', 'the', 'street', '!', '[SEP]']\n",
            "Original: ['The', 'staff', 'do', 'occasionally', 'get', 'game', 'info', 'correct,', 'but', 'if', 'your', 'looking', 'fot', 'a', 'good', 'game', 'and', \"aren'ta\", 'nerd,', 'dont', 'ask', 'them.']\n",
            "Tokenized: ['[CLS]', 'the', 'staff', 'do', 'occasionally', 'get', 'game', 'info', 'correct', ',', 'but', 'if', 'your', 'looking', 'f', '##ot', 'a', 'good', 'game', 'and', 'aren', \"'\", 'ta', 'ne', '##rd', ',', 'don', '##t', 'ask', 'them', '.', '[SEP]']\n",
            "Original: ['The', 'last', 'time', 'I', 'did', 'that', 'I', 'was', 'suggested', 'to', 'buy', 'oblivion', 'when', 'I', 'told', 'them', 'I', 'was', 'looking', 'for', 'a', 'fps.']\n",
            "Tokenized: ['[CLS]', 'the', 'last', 'time', 'i', 'did', 'that', 'i', 'was', 'suggested', 'to', 'buy', 'oblivion', 'when', 'i', 'told', 'them', 'i', 'was', 'looking', 'for', 'a', 'f', '##ps', '.', '[SEP]']\n",
            "Original: ['I', 'knew', 'what', 'it', 'was', 'and', 'told', 'the', 'guy', 'that', 'it', \"wasn'ta\", 'fps.']\n",
            "Tokenized: ['[CLS]', 'i', 'knew', 'what', 'it', 'was', 'and', 'told', 'the', 'guy', 'that', 'it', 'wasn', \"'\", 'ta', 'f', '##ps', '.', '[SEP]']\n",
            "Original: ['He', 'tried', 'to', 'tell', 'me', 'it', 'was', 'when', 'I', 'told', 'asked', 'him', 'if', 'he', 'knew', 'what', 'fps', 'stood', 'for', 'and', 'he', 'had', 'no', 'clue.']\n",
            "Tokenized: ['[CLS]', 'he', 'tried', 'to', 'tell', 'me', 'it', 'was', 'when', 'i', 'told', 'asked', 'him', 'if', 'he', 'knew', 'what', 'f', '##ps', 'stood', 'for', 'and', 'he', 'had', 'no', 'clue', '.', '[SEP]']\n",
            "Original: ['Overall', 'they', \"aren't\", 'very', 'knowledge', 'about', 'the', 'type', 'of', 'games', 'are', 'on', 'the', 'market.']\n",
            "Tokenized: ['[CLS]', 'overall', 'they', 'aren', \"'\", 't', 'very', 'knowledge', 'about', 'the', 'type', 'of', 'games', 'are', 'on', 'the', 'market', '.', '[SEP]']\n",
            "Original: ['Drove', 'all', 'the', 'way', 'over', 'from', 'the', 'highway...', 'closed', 'at', '7.']\n",
            "Tokenized: ['[CLS]', 'drove', 'all', 'the', 'way', 'over', 'from', 'the', 'highway', '.', '.', '.', 'closed', 'at', '7', '.', '[SEP]']\n",
            "Original: ['Who', 'does', 'that?!']\n",
            "Tokenized: ['[CLS]', 'who', 'does', 'that', '?', '!', '[SEP]']\n",
            "Original: ['ok']\n",
            "Tokenized: ['[CLS]', 'ok', '[SEP]']\n",
            "Original: [\"can't\", 'remember', 'good', 'or', 'bad,', 'so', 'it', 'must', 'have', 'been', 'meh.']\n",
            "Tokenized: ['[CLS]', 'can', \"'\", 't', 'remember', 'good', 'or', 'bad', ',', 'so', 'it', 'must', 'have', 'been', 'me', '##h', '.', '[SEP]']\n",
            "Original: ['First', 'and', 'Last', 'time', \"we'll\", 'eat', 'there']\n",
            "Tokenized: ['[CLS]', 'first', 'and', 'last', 'time', 'we', \"'\", 'll', 'eat', 'there', '[SEP]']\n",
            "Original: ['My', 'friend', 'and', 'I', 'went', 'there', 'for', 'lunch', 'today.']\n",
            "Tokenized: ['[CLS]', 'my', 'friend', 'and', 'i', 'went', 'there', 'for', 'lunch', 'today', '.', '[SEP]']\n",
            "Original: ['Neither', 'one', 'of', 'us', 'had', 'ever', 'been', '-', 'so', 'we', 'thought', \"we'd\", 'try', 'it.']\n",
            "Tokenized: ['[CLS]', 'neither', 'one', 'of', 'us', 'had', 'ever', 'been', '-', 'so', 'we', 'thought', 'we', \"'\", 'd', 'try', 'it', '.', '[SEP]']\n",
            "Original: ['BIG', 'MISTAKE', '-', 'The', 'food', 'was', 'tasteless', 'and', 'cold.']\n",
            "Tokenized: ['[CLS]', 'big', 'mistake', '-', 'the', 'food', 'was', 'taste', '##less', 'and', 'cold', '.', '[SEP]']\n",
            "Original: ['We', 'both', 'kept', 'trying', 'to', 'find', 'something', 'we', 'liked.']\n",
            "Tokenized: ['[CLS]', 'we', 'both', 'kept', 'trying', 'to', 'find', 'something', 'we', 'liked', '.', '[SEP]']\n",
            "Original: ['The', 'only', 'thing', 'I', 'found', 'edible', 'were', 'the', 'potato', 'wedges,', 'I', 'finally', 'gave', 'up,', 'he', 'kept', 'trying', '-', 'he', 'found', 'the', 'fried', 'wantons', 'to', 'be', 'OK', '-', 'His', 'Mongolian', 'bowl', 'was', 'awful.']\n",
            "Tokenized: ['[CLS]', 'the', 'only', 'thing', 'i', 'found', 'edible', 'were', 'the', 'potato', 'wedge', '##s', ',', 'i', 'finally', 'gave', 'up', ',', 'he', 'kept', 'trying', '-', 'he', 'found', 'the', 'fried', 'want', '##ons', 'to', 'be', 'ok', '-', 'his', 'mongolian', 'bowl', 'was', 'awful', '.', '[SEP]']\n",
            "Original: ['Would', 'NOT', 'recommend', 'this', 'place', 'to', 'anyone', '-', 'in', 'fact', '-', 'save', 'your', 'money', 'and', 'go', 'somewhere', 'else.']\n",
            "Tokenized: ['[CLS]', 'would', 'not', 'recommend', 'this', 'place', 'to', 'anyone', '-', 'in', 'fact', '-', 'save', 'your', 'money', 'and', 'go', 'somewhere', 'else', '.', '[SEP]']\n",
            "Original: ['FHS', 'is', 'a', 'good', 'high', 'school', '--', 'c/o', '1998']\n",
            "Tokenized: ['[CLS]', 'f', '##hs', 'is', 'a', 'good', 'high', 'school', '-', '-', 'c', '/', 'o', '1998', '[SEP]']\n",
            "Original: ['I', 'enjoyed', 'my', 'time', 'at', 'Franklin', 'High', 'School.']\n",
            "Tokenized: ['[CLS]', 'i', 'enjoyed', 'my', 'time', 'at', 'franklin', 'high', 'school', '.', '[SEP]']\n",
            "Original: ['We', 'had', 'relatively', 'good', 'facilities', 'at', 'the', 'time', '--', 'decent', 'science', 'labs,', 'with', 'multiple', 'hoods', 'and', 'access', 'to', 'good', 'equipment;', 'nice', 'gymnasiums,', 'and', 'a', 'well-funded', 'music', 'department', '(I', 'was', 'in', 'the', 'guitar', 'ensemble).']\n",
            "Tokenized: ['[CLS]', 'we', 'had', 'relatively', 'good', 'facilities', 'at', 'the', 'time', '-', '-', 'decent', 'science', 'labs', ',', 'with', 'multiple', 'hood', '##s', 'and', 'access', 'to', 'good', 'equipment', ';', 'nice', 'gymnasium', '##s', ',', 'and', 'a', 'well', '-', 'funded', 'music', 'department', '(', 'i', 'was', 'in', 'the', 'guitar', 'ensemble', ')', '.', '[SEP]']\n",
            "Original: ['Now,', 'of', 'course,', \"there's\", 'a', 'new', 'building,', 'with', 'presumably', 'better', 'facilities.']\n",
            "Tokenized: ['[CLS]', 'now', ',', 'of', 'course', ',', 'there', \"'\", 's', 'a', 'new', 'building', ',', 'with', 'presumably', 'better', 'facilities', '.', '[SEP]']\n",
            "Original: ['All', 'to', 'the', 'good.']\n",
            "Tokenized: ['[CLS]', 'all', 'to', 'the', 'good', '.', '[SEP]']\n",
            "Original: ['For', 'those', 'who', 'care,', 'a', 'good', 'proportion', 'of', 'the', 'my', 'class', 'went', 'on', 'to', '4-year', 'colleges,', 'many', 'of', 'them', 'ranked', 'in', 'the', 'top', '50', 'of', 'US', 'News.']\n",
            "Tokenized: ['[CLS]', 'for', 'those', 'who', 'care', ',', 'a', 'good', 'proportion', 'of', 'the', 'my', 'class', 'went', 'on', 'to', '4', '-', 'year', 'colleges', ',', 'many', 'of', 'them', 'ranked', 'in', 'the', 'top', '50', 'of', 'us', 'news', '.', '[SEP]']\n",
            "Original: ['If', 'I', 'remember,', 'students', 'went', 'to', 'Dartmouth,', 'U.Penn,', 'Duke,', 'BU,', 'William', 'and', 'Mary,', 'Vassar,', 'Howard,', 'and', 'Carnegie', 'Mellon,', 'among', 'others.']\n",
            "Tokenized: ['[CLS]', 'if', 'i', 'remember', ',', 'students', 'went', 'to', 'dartmouth', ',', 'u', '.', 'penn', ',', 'duke', ',', 'bu', ',', 'william', 'and', 'mary', ',', 'va', '##ssar', ',', 'howard', ',', 'and', 'carnegie', 'mellon', ',', 'among', 'others', '.', '[SEP]']\n",
            "Original: ['Many', 'students', 'went', 'to', 'Rutgers,', 'including', 'their', 'top-ranked', 'pharmacy', 'program.']\n",
            "Tokenized: ['[CLS]', 'many', 'students', 'went', 'to', 'rutgers', ',', 'including', 'their', 'top', '-', 'ranked', 'pharmacy', 'program', '.', '[SEP]']\n",
            "Original: ['The', 'point', 'is', '--', 'FHS', 'gives', 'you', 'the', 'opportunity', 'to', 'make', 'it', 'to', 'a', 'good', 'college,', 'but', 'you', 'need', 'to', 'work', 'hard.']\n",
            "Tokenized: ['[CLS]', 'the', 'point', 'is', '-', '-', 'f', '##hs', 'gives', 'you', 'the', 'opportunity', 'to', 'make', 'it', 'to', 'a', 'good', 'college', ',', 'but', 'you', 'need', 'to', 'work', 'hard', '.', '[SEP]']\n",
            "Original: ['My', 'favorite', 'place...']\n",
            "Tokenized: ['[CLS]', 'my', 'favorite', 'place', '.', '.', '.', '[SEP]']\n",
            "Original: ['My', 'daughter', 'and', 'I', 'stayed', 'here', 'again', 'from', 'the', '7th', 'to', 'the', '14th', 'of', 'December.']\n",
            "Tokenized: ['[CLS]', 'my', 'daughter', 'and', 'i', 'stayed', 'here', 'again', 'from', 'the', '7th', 'to', 'the', '14th', 'of', 'december', '.', '[SEP]']\n",
            "Original: ['yet', 'again', 'it', 'was', 'a', 'great', 'stay', 'from', 'begiinning', 'to', 'end.']\n",
            "Tokenized: ['[CLS]', 'yet', 'again', 'it', 'was', 'a', 'great', 'stay', 'from', 'beg', '##ii', '##nni', '##ng', 'to', 'end', '.', '[SEP]']\n",
            "Original: ['Okay', 'our', 'room', 'was', 'at', 'times', 'noisy', 'but', 'we', 'were', 'in', 'a', 'mega', 'busy', 'city', 'at', 'an', 'extremely', 'busy', 'time', 'of', 'year.']\n",
            "Tokenized: ['[CLS]', 'okay', 'our', 'room', 'was', 'at', 'times', 'noisy', 'but', 'we', 'were', 'in', 'a', 'mega', 'busy', 'city', 'at', 'an', 'extremely', 'busy', 'time', 'of', 'year', '.', '[SEP]']\n",
            "Original: ['Inford', 'Media']\n",
            "Tokenized: ['[CLS]', 'info', '##rd', 'media', '[SEP]']\n",
            "Original: ['Media,', 'Software,', 'Fun', 'and', 'Games,', 'Website', 'design,', 'Web', 'Promotion,', 'B2B,', 'Business', 'Promotion,', 'Search', 'Engine', 'Optimization.']\n",
            "Tokenized: ['[CLS]', 'media', ',', 'software', ',', 'fun', 'and', 'games', ',', 'website', 'design', ',', 'web', 'promotion', ',', 'b', '##2', '##b', ',', 'business', 'promotion', ',', 'search', 'engine', 'optimization', '.', '[SEP]']\n",
            "Original: ['BEST', 'DENTIST', 'EVER', '-']\n",
            "Tokenized: ['[CLS]', 'best', 'dentist', 'ever', '-', '[SEP]']\n",
            "Original: ['Very', 'fast', 'and', 'efficient', 'service.']\n",
            "Tokenized: ['[CLS]', 'very', 'fast', 'and', 'efficient', 'service', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'had', 'several', 'dentists', 'in', 'my', 'life,', 'but', 'Dr.', 'Deters', 'is', 'by', 'far', 'my', 'favorite.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'had', 'several', 'dentist', '##s', 'in', 'my', 'life', ',', 'but', 'dr', '.', 'deter', '##s', 'is', 'by', 'far', 'my', 'favorite', '.', '[SEP]']\n",
            "Original: ['I', 'never', 'wait', 'in', 'the', 'waiting', 'room', 'more', 'than', 'two', 'minutes', 'and', 'the', 'cleanings', 'are', 'quick', 'and', 'painless.']\n",
            "Tokenized: ['[CLS]', 'i', 'never', 'wait', 'in', 'the', 'waiting', 'room', 'more', 'than', 'two', 'minutes', 'and', 'the', 'cleaning', '##s', 'are', 'quick', 'and', 'pain', '##less', '.', '[SEP]']\n",
            "Original: ['Good', 'local', 'steakhouse,', 'I', 'recommend', 'it!']\n",
            "Tokenized: ['[CLS]', 'good', 'local', 'steak', '##house', ',', 'i', 'recommend', 'it', '!', '[SEP]']\n",
            "Original: ['Good', 'food', 'and', 'coffee', 'with', 'a', 'nice', 'atmosphere']\n",
            "Tokenized: ['[CLS]', 'good', 'food', 'and', 'coffee', 'with', 'a', 'nice', 'atmosphere', '[SEP]']\n",
            "Original: ['prepared', 'the', 'road', 'test', 'with', 'a', 'driving', '...', 'prepared', 'the', 'road', 'test', 'with', 'a', 'driving', 'school', 'in', 'edmonton,', 'but', 'my', 'instructor', 'only', 'trained', 'me', 'in', 'a', 'narrow', 'street,', 'hence', 'I', 'took', 'one', '90', 'minute', 'lesson', 'from', 'the', 'Noble', 'driving', 'school', 'to', 'learn', 'the', 'skill', 'of', 'changing', 'lane,', 'and', 'found', 'them', 'very', 'friendly', 'and', 'professional.']\n",
            "Tokenized: ['[CLS]', 'prepared', 'the', 'road', 'test', 'with', 'a', 'driving', '.', '.', '.', 'prepared', 'the', 'road', 'test', 'with', 'a', 'driving', 'school', 'in', 'edmonton', ',', 'but', 'my', 'instructor', 'only', 'trained', 'me', 'in', 'a', 'narrow', 'street', ',', 'hence', 'i', 'took', 'one', '90', 'minute', 'lesson', 'from', 'the', 'noble', 'driving', 'school', 'to', 'learn', 'the', 'skill', 'of', 'changing', 'lane', ',', 'and', 'found', 'them', 'very', 'friendly', 'and', 'professional', '.', '[SEP]']\n",
            "Original: ['Best', 'in', 'Memphis']\n",
            "Tokenized: ['[CLS]', 'best', 'in', 'memphis', '[SEP]']\n",
            "Original: ['This', 'shop', 'is', 'by', 'far', 'the', 'best', 'I', 'have', 'been', 'to.']\n",
            "Tokenized: ['[CLS]', 'this', 'shop', 'is', 'by', 'far', 'the', 'best', 'i', 'have', 'been', 'to', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'a', 'Saab...which', 'everything', 'is', 'expensive', 'on', 'and', 'they', 'have', 'been', 'extrememly', 'fair', 'and', 'price', 'alot', 'lower', 'than', 'any', 'other', 'shop', 'I', 'called.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'a', 'sa', '##ab', '.', '.', '.', 'which', 'everything', 'is', 'expensive', 'on', 'and', 'they', 'have', 'been', 'extreme', '##ml', '##y', 'fair', 'and', 'price', 'al', '##ot', 'lower', 'than', 'any', 'other', 'shop', 'i', 'called', '.', '[SEP]']\n",
            "Original: ['They', 'are', 'the', 'only', 'place', 'I', 'would', 'take', 'my', 'car', 'peiod.']\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'the', 'only', 'place', 'i', 'would', 'take', 'my', 'car', 'pei', '##od', '.', '[SEP]']\n",
            "Original: ['Great', 'Place!']\n",
            "Tokenized: ['[CLS]', 'great', 'place', '!', '[SEP]']\n",
            "Original: ['Love', 'this', 'place!!']\n",
            "Tokenized: ['[CLS]', 'love', 'this', 'place', '!', '!', '[SEP]']\n",
            "Original: ['Has', 'another', 'store', 'in', 'the', 'st.', 'charles', 'mall.']\n",
            "Tokenized: ['[CLS]', 'has', 'another', 'store', 'in', 'the', 'st', '.', 'charles', 'mall', '.', '[SEP]']\n",
            "Original: ['Place', 'is', 'next', 'to', 'carval', 'and', 'walmart.']\n",
            "Tokenized: ['[CLS]', 'place', 'is', 'next', 'to', 'car', '##val', 'and', 'wal', '##mart', '.', '[SEP]']\n",
            "Original: ['Dont', 'go', 'to', 'the', 'one', 'by', 'pepco,', 'I', 'got', 'confused!!!']\n",
            "Tokenized: ['[CLS]', 'don', '##t', 'go', 'to', 'the', 'one', 'by', 'pep', '##co', ',', 'i', 'got', 'confused', '!', '!', '!', '[SEP]']\n",
            "Original: ['Bad', 'place.']\n",
            "Tokenized: ['[CLS]', 'bad', 'place', '.', '[SEP]']\n",
            "Original: ['Go', 'to', 'ATLANTIC', 'WIRELESS!!']\n",
            "Tokenized: ['[CLS]', 'go', 'to', 'atlantic', 'wireless', '!', '!', '[SEP]']\n",
            "Original: ['I', 'love', 'them!!!']\n",
            "Tokenized: ['[CLS]', 'i', 'love', 'them', '!', '!', '!', '[SEP]']\n",
            "Original: ['I', 'used', 'Birdies', 'for', 'our', 'Annual', 'Walk', 'Against', 'Drugs', 'and', 'Alcohol', 'event.']\n",
            "Tokenized: ['[CLS]', 'i', 'used', 'bird', '##ies', 'for', 'our', 'annual', 'walk', 'against', 'drugs', 'and', 'alcohol', 'event', '.', '[SEP]']\n",
            "Original: ['They', 'were', 'very', 'professional,', 'neat', 'and', 'clean.']\n",
            "Tokenized: ['[CLS]', 'they', 'were', 'very', 'professional', ',', 'neat', 'and', 'clean', '.', '[SEP]']\n",
            "Original: ['They', 'came', 'through', 'on', 'all', 'of', 'their', 'promises', 'and', 'we', 'had', 'a', 'very', 'successful', 'day.']\n",
            "Tokenized: ['[CLS]', 'they', 'came', 'through', 'on', 'all', 'of', 'their', 'promises', 'and', 'we', 'had', 'a', 'very', 'successful', 'day', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'be', 'using', 'Bridies', 'again.']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'be', 'using', 'br', '##idi', '##es', 'again', '.', '[SEP]']\n",
            "Original: ['The', 'Worst', 'Experience', 'Ever!!!']\n",
            "Tokenized: ['[CLS]', 'the', 'worst', 'experience', 'ever', '!', '!', '!', '[SEP]']\n",
            "Original: ['I', 'have', 'worked', 'with', 'Ted', 'Jurek', 'at', 'Decor', 'and', 'You,', 'and', 'it', 'started', 'out', 'as', 'a', 'decent', 'experience.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'worked', 'with', 'ted', 'ju', '##rek', 'at', 'decor', 'and', 'you', ',', 'and', 'it', 'started', 'out', 'as', 'a', 'decent', 'experience', '.', '[SEP]']\n",
            "Original: ['He', 'was', 'referred', 'to', 'me', 'by', 'a', 'friend,', 'who', \"didn't\", 'have', 'the', 'best', 'experience', 'with', 'Ted,', 'but', 'said', 'that', 'Ted', 'was', 'able', 'to', 'make', 'up', 'for', 'his', 'lack', 'of', 'preparedness', 'at', 'the', 'end.']\n",
            "Tokenized: ['[CLS]', 'he', 'was', 'referred', 'to', 'me', 'by', 'a', 'friend', ',', 'who', 'didn', \"'\", 't', 'have', 'the', 'best', 'experience', 'with', 'ted', ',', 'but', 'said', 'that', 'ted', 'was', 'able', 'to', 'make', 'up', 'for', 'his', 'lack', 'of', 'prepared', '##ness', 'at', 'the', 'end', '.', '[SEP]']\n",
            "Original: ['I', 'figure', 'I', 'would', 'give', 'this', 'company', 'a', 'chance.']\n",
            "Tokenized: ['[CLS]', 'i', 'figure', 'i', 'would', 'give', 'this', 'company', 'a', 'chance', '.', '[SEP]']\n",
            "Original: ['However,', 'after', 'giving', 'a', 'required', '$500.00', '(NON', 'REFUNDABLE)', 'deposit', 'before', 'seeing', 'any', 'plans', 'or', 'ideas,', 'I', 'was', 'sorry', 'I', 'did.']\n",
            "Tokenized: ['[CLS]', 'however', ',', 'after', 'giving', 'a', 'required', '$', '500', '.', '00', '(', 'non', 'ref', '##unda', '##ble', ')', 'deposit', 'before', 'seeing', 'any', 'plans', 'or', 'ideas', ',', 'i', 'was', 'sorry', 'i', 'did', '.', '[SEP]']\n",
            "Original: ['We', 'met', 'a', 'couple', 'of', 'weeks', 'later,', 'Ted', 'was', 'late.']\n",
            "Tokenized: ['[CLS]', 'we', 'met', 'a', 'couple', 'of', 'weeks', 'later', ',', 'ted', 'was', 'late', '.', '[SEP]']\n",
            "Original: ['He', 'brought', 'fabric', 'books,', 'and', 'pictures', 'of', 'furniture', 'only,', 'which', 'all', 'came', 'way', 'over', 'budget.']\n",
            "Tokenized: ['[CLS]', 'he', 'brought', 'fabric', 'books', ',', 'and', 'pictures', 'of', 'furniture', 'only', ',', 'which', 'all', 'came', 'way', 'over', 'budget', '.', '[SEP]']\n",
            "Original: ['By', 'the', 'time', 'we', 'got', 'to', 'the', 'budget', 'i', 'told', 'him', 'I', 'could', 'work', 'with,', 'there', 'was', 'basically', 'no', 'design.']\n",
            "Tokenized: ['[CLS]', 'by', 'the', 'time', 'we', 'got', 'to', 'the', 'budget', 'i', 'told', 'him', 'i', 'could', 'work', 'with', ',', 'there', 'was', 'basically', 'no', 'design', '.', '[SEP]']\n",
            "Original: ['Just', 'curtains', 'and', 'a', 'couple', 'of', 'accessories.']\n",
            "Tokenized: ['[CLS]', 'just', 'curtains', 'and', 'a', 'couple', 'of', 'accessories', '.', '[SEP]']\n",
            "Original: ['WHAT', 'IS', 'THAT?????']\n",
            "Tokenized: ['[CLS]', 'what', 'is', 'that', '?', '?', '?', '?', '?', '[SEP]']\n",
            "Original: ['This', 'company', 'is', 'way', 'too', 'expensive', 'with', 'nothing', 'to', 'show', 'for', 'it.']\n",
            "Tokenized: ['[CLS]', 'this', 'company', 'is', 'way', 'too', 'expensive', 'with', 'nothing', 'to', 'show', 'for', 'it', '.', '[SEP]']\n",
            "Original: ['Please', 'everyone....take', 'heed', 'and', \"don't\", 'get', 'caught', 'up', 'in', 'the', 'hype', 'about', 'DECOR', 'and', 'YOU', 'working', 'with', 'every', 'budget...']\n",
            "Tokenized: ['[CLS]', 'please', 'everyone', '.', '.', '.', '.', 'take', 'hee', '##d', 'and', 'don', \"'\", 't', 'get', 'caught', 'up', 'in', 'the', 'h', '##ype', 'about', 'decor', 'and', 'you', 'working', 'with', 'every', 'budget', '.', '.', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'just', 'a', 'way', 'for', 'them', 'to', 'squirm', 'their', 'way', 'into', 'your', 'precious', 'pocket']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'just', 'a', 'way', 'for', 'them', 'to', 'sq', '##ui', '##rm', 'their', 'way', 'into', 'your', 'precious', 'pocket', '[SEP]']\n",
            "Original: ['nice', 'friendly', 'local', 'bagel', 'place']\n",
            "Tokenized: ['[CLS]', 'nice', 'friendly', 'local', 'bag', '##el', 'place', '[SEP]']\n",
            "Original: ['there', 'might', 'be', 'bigger', 'and', 'more', 'well', 'known', 'bagel', 'places', 'in', 'the', 'area', 'but', 'Family', 'Bagels', 'are', 'nice', 'people,', 'small', 'shop', 'and', 'incredibly', 'friendly.']\n",
            "Tokenized: ['[CLS]', 'there', 'might', 'be', 'bigger', 'and', 'more', 'well', 'known', 'bag', '##el', 'places', 'in', 'the', 'area', 'but', 'family', 'bag', '##els', 'are', 'nice', 'people', ',', 'small', 'shop', 'and', 'incredibly', 'friendly', '.', '[SEP]']\n",
            "Original: ['While', 'other', 'may', 'be', 'ok', 'waiting', 'in', 'line', 'at', 'Town', 'Bagel', 'we', 'are', 'happy', 'with', 'the', 'quality', 'and', 'service', 'we', 'get', 'at', 'Family', 'Bagels']\n",
            "Tokenized: ['[CLS]', 'while', 'other', 'may', 'be', 'ok', 'waiting', 'in', 'line', 'at', 'town', 'bag', '##el', 'we', 'are', 'happy', 'with', 'the', 'quality', 'and', 'service', 'we', 'get', 'at', 'family', 'bag', '##els', '[SEP]']\n",
            "Original: [\"you'll\", 'love', 'it']\n",
            "Tokenized: ['[CLS]', 'you', \"'\", 'll', 'love', 'it', '[SEP]']\n",
            "Original: ['I', 'have', 'had', 'my', 'back', 'fused', 'in', '2', 'places.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'had', 'my', 'back', 'fused', 'in', '2', 'places', '.', '[SEP]']\n",
            "Original: ['I', 'wish', 'that', 'I', \"hadn't\", 'done', 'this.']\n",
            "Tokenized: ['[CLS]', 'i', 'wish', 'that', 'i', 'hadn', \"'\", 't', 'done', 'this', '.', '[SEP]']\n",
            "Original: ['With', 'these', 'fusions,', 'chiropractric', \"isn't\", 'as', 'successful,', 'but', 'it', 'still', 'is', 'very', 'helpful.']\n",
            "Tokenized: ['[CLS]', 'with', 'these', 'fusion', '##s', ',', 'chi', '##rop', '##rac', '##tric', 'isn', \"'\", 't', 'as', 'successful', ',', 'but', 'it', 'still', 'is', 'very', 'helpful', '.', '[SEP]']\n",
            "Original: ['Even', 'after', 'my', 'fusions,', 'my', 'back', 'continued', 'to', 'hurt,', 'but', 'now', 'it', \"doesn't\", 'hurt', 'any', 'more.']\n",
            "Tokenized: ['[CLS]', 'even', 'after', 'my', 'fusion', '##s', ',', 'my', 'back', 'continued', 'to', 'hurt', ',', 'but', 'now', 'it', 'doesn', \"'\", 't', 'hurt', 'any', 'more', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'encourage', 'anyone', 'who', 'is', 'thinking', 'of', 'back', 'surgery,', 'to', 'talk', 'about', 'different', 'treatment', 'options', 'first', 'with', 'Dr.', 'de', 'Barros.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'encourage', 'anyone', 'who', 'is', 'thinking', 'of', 'back', 'surgery', ',', 'to', 'talk', 'about', 'different', 'treatment', 'options', 'first', 'with', 'dr', '.', 'de', 'barr', '##os', '.', '[SEP]']\n",
            "Original: ['He', 'really', 'knows', 'what', 'he', 'is', 'talking', 'about', 'and', 'will', 'approach', 'the', 'different', 'options', 'fairly.']\n",
            "Tokenized: ['[CLS]', 'he', 'really', 'knows', 'what', 'he', 'is', 'talking', 'about', 'and', 'will', 'approach', 'the', 'different', 'options', 'fairly', '.', '[SEP]']\n",
            "Original: ['You', 'can', 'see', 'different', 'treatments', 'that', 'a', 'surgeon', \"can't\", 'see,', 'so', 'that', 'you', 'can', 'have', 'several', 'treatment', 'options', 'before', 'you', 'decide', 'what', 'to', 'do.']\n",
            "Tokenized: ['[CLS]', 'you', 'can', 'see', 'different', 'treatments', 'that', 'a', 'surgeon', 'can', \"'\", 't', 'see', ',', 'so', 'that', 'you', 'can', 'have', 'several', 'treatment', 'options', 'before', 'you', 'decide', 'what', 'to', 'do', '.', '[SEP]']\n",
            "Original: ['A', 'friend', 'and', 'I', 'recently', 'took', 'our', '16', 'and', '18', 'month', 'olds', 'here.']\n",
            "Tokenized: ['[CLS]', 'a', 'friend', 'and', 'i', 'recently', 'took', 'our', '16', 'and', '18', 'month', 'olds', 'here', '.', '[SEP]']\n",
            "Original: ['While', 'there', \"wasn't\", 'too', 'much', 'available', 'for', 'their', 'age', '(ball', 'pit,', 'bouncy', 'area', 'and', 'a', 'little', 'padded', 'pyramid', 'to', 'climb', 'on),', 'we', 'went', 'right', 'when', 'they', 'opened', 'at', '10', 'am', 'on', 'a', 'winter', 'weekday', 'and', 'ended', 'up', 'being', 'the', 'only', 'ones', 'there,', 'so', 'we', 'were', 'given', 'a', 'little', 'more', 'liberty', 'than', 'we', 'would', 'have', 'if', 'others', 'had', 'been', 'there.']\n",
            "Tokenized: ['[CLS]', 'while', 'there', 'wasn', \"'\", 't', 'too', 'much', 'available', 'for', 'their', 'age', '(', 'ball', 'pit', ',', 'bo', '##un', '##cy', 'area', 'and', 'a', 'little', 'padded', 'pyramid', 'to', 'climb', 'on', ')', ',', 'we', 'went', 'right', 'when', 'they', 'opened', 'at', '10', 'am', 'on', 'a', 'winter', 'weekday', 'and', 'ended', 'up', 'being', 'the', 'only', 'ones', 'there', ',', 'so', 'we', 'were', 'given', 'a', 'little', 'more', 'liberty', 'than', 'we', 'would', 'have', 'if', 'others', 'had', 'been', 'there', '.', '[SEP]']\n",
            "Original: [\"It's\", 'not', 'the', 'classiest', 'place,', 'but', 'it', 'was', 'cleaner', 'than', 'I', 'expected', 'and', 'the', 'staff', 'was', 'very', 'friendly.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'not', 'the', 'class', '##iest', 'place', ',', 'but', 'it', 'was', 'cleaner', 'than', 'i', 'expected', 'and', 'the', 'staff', 'was', 'very', 'friendly', '.', '[SEP]']\n",
            "Original: ['For', '$4', 'it', 'was', 'a', 'nice', 'break', 'from', 'the', 'monotony', 'of', 'winter', 'indoors', 'with', 'a', 'toddler.']\n",
            "Tokenized: ['[CLS]', 'for', '$', '4', 'it', 'was', 'a', 'nice', 'break', 'from', 'the', 'mono', '##ton', '##y', 'of', 'winter', 'indoors', 'with', 'a', 'todd', '##ler', '.', '[SEP]']\n",
            "Original: ['Excellent', 'Physiotherapists!']\n",
            "Tokenized: ['[CLS]', 'excellent', 'ph', '##ys', '##iot', '##her', '##ap', '##ists', '!', '[SEP]']\n",
            "Original: ['Kusal', 'Goonewardena', 'and', 'his', 'team', 'of', 'Physios', 'are', 'unbelievable!']\n",
            "Tokenized: ['[CLS]', 'ku', '##sal', 'goo', '##ne', '##ward', '##ena', 'and', 'his', 'team', 'of', 'ph', '##ys', '##ios', 'are', 'unbelievable', '!', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'suffering', 'from', 'back', 'pain', 'for', 'over', '12', 'years', 'and', 'have', 'been', 'to', 'numerous', 'specialists,', 'physios,', 'osteos', 'and', 'chiros', 'with', 'no', 'help.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'suffering', 'from', 'back', 'pain', 'for', 'over', '12', 'years', 'and', 'have', 'been', 'to', 'numerous', 'specialists', ',', 'ph', '##ys', '##ios', ',', 'os', '##te', '##os', 'and', 'chi', '##ros', 'with', 'no', 'help', '.', '[SEP]']\n",
            "Original: ['It', 'took', 'the', 'Vigor', 'team', 'only', '4', 'visits', 'to', 'get', 'me', 'feeling', 'normal!']\n",
            "Tokenized: ['[CLS]', 'it', 'took', 'the', 'vigor', 'team', 'only', '4', 'visits', 'to', 'get', 'me', 'feeling', 'normal', '!', '[SEP]']\n",
            "Original: ['And', 'now', '2', 'months', 'after', 'my', 'last', 'appointment', 'I', 'am', 'better', 'than', 'ever.']\n",
            "Tokenized: ['[CLS]', 'and', 'now', '2', 'months', 'after', 'my', 'last', 'appointment', 'i', 'am', 'better', 'than', 'ever', '.', '[SEP]']\n",
            "Original: ['Thanks', 'guys!']\n",
            "Tokenized: ['[CLS]', 'thanks', 'guys', '!', '[SEP]']\n",
            "Original: ['Okay,', \"here's\", 'the', 'scoop.']\n",
            "Tokenized: ['[CLS]', 'okay', ',', 'here', \"'\", 's', 'the', 'scoop', '.', '[SEP]']\n",
            "Original: [\"I'ma\", 'regular', 'at', 'the', 'HH.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'ma', 'regular', 'at', 'the', 'h', '##h', '.', '[SEP]']\n",
            "Original: ['The', 'food', 'is', 'excellent,', 'the', 'serivce', 'is', 'horrible.']\n",
            "Tokenized: ['[CLS]', 'the', 'food', 'is', 'excellent', ',', 'the', 'ser', '##iv', '##ce', 'is', 'horrible', '.', '[SEP]']\n",
            "Original: ['I', 'think', \"they're\", 'still', 'in', 'the', 'mindset', 'from', 'when', 'it', 'was', 'only', 'smokers', 'sitting', 'around', 'and', 'drinking...', 'not', 'in', 'any', 'hurry.']\n",
            "Tokenized: ['[CLS]', 'i', 'think', 'they', \"'\", 're', 'still', 'in', 'the', 'minds', '##et', 'from', 'when', 'it', 'was', 'only', 'smoke', '##rs', 'sitting', 'around', 'and', 'drinking', '.', '.', '.', 'not', 'in', 'any', 'hurry', '.', '[SEP]']\n",
            "Original: ['In', 'my', 'experience', 'the', 'food', 'has', 'been', 'excellent', '(for', 'the', 'most', 'part).']\n",
            "Tokenized: ['[CLS]', 'in', 'my', 'experience', 'the', 'food', 'has', 'been', 'excellent', '(', 'for', 'the', 'most', 'part', ')', '.', '[SEP]']\n",
            "Original: ['However,', 'the', 'bartenders/waitresses', 'definately', 'need', 'to', 'be', 're-trained', '(if', 'they', 'ever', 'had', 'any', 'to', 'begin', 'with)', 'and', 'learn', 'two', 'things:', 'only', 'chat', 'with', 'customers', 'when', 'other', 'customers', 'are', 'not', 'impatiently', 'waiting', 'and', 'to', 'look', 'around', 'more', 'often', 'to', 'see', 'if', 'people', 'are', 'waiting.']\n",
            "Tokenized: ['[CLS]', 'however', ',', 'the', 'bartender', '##s', '/', 'waitress', '##es', 'def', '##inate', '##ly', 'need', 'to', 'be', 're', '-', 'trained', '(', 'if', 'they', 'ever', 'had', 'any', 'to', 'begin', 'with', ')', 'and', 'learn', 'two', 'things', ':', 'only', 'chat', 'with', 'customers', 'when', 'other', 'customers', 'are', 'not', 'impatiently', 'waiting', 'and', 'to', 'look', 'around', 'more', 'often', 'to', 'see', 'if', 'people', 'are', 'waiting', '.', '[SEP]']\n",
            "Original: ['In', 'some', 'cases', 'the', 'result', 'is', 'because', 'of', 'understaffing,', 'in', 'some', 'cases', 'the', 'staff', 'just', \"doesn't\", 'care/know', 'better.']\n",
            "Tokenized: ['[CLS]', 'in', 'some', 'cases', 'the', 'result', 'is', 'because', 'of', 'under', '##sta', '##ffin', '##g', ',', 'in', 'some', 'cases', 'the', 'staff', 'just', 'doesn', \"'\", 't', 'care', '/', 'know', 'better', '.', '[SEP]']\n",
            "Original: ['They', 'must', 'also', 'get', 'something', 'to', 'keep', 'take-out', 'warm,', 'so', \"it's\", 'not', 'room', 'temperature', 'at', 'best', 'when', 'you', 'get', 'it', 'home.']\n",
            "Tokenized: ['[CLS]', 'they', 'must', 'also', 'get', 'something', 'to', 'keep', 'take', '-', 'out', 'warm', ',', 'so', 'it', \"'\", 's', 'not', 'room', 'temperature', 'at', 'best', 'when', 'you', 'get', 'it', 'home', '.', '[SEP]']\n",
            "Original: ['You', 'just', 'have', 'to', 'know', 'what', \"you're\", 'getting', 'into', 'when', 'you', 'go.']\n",
            "Tokenized: ['[CLS]', 'you', 'just', 'have', 'to', 'know', 'what', 'you', \"'\", 're', 'getting', 'into', 'when', 'you', 'go', '.', '[SEP]']\n",
            "Original: ['Nice', 'local', 'pub,', 'excellent', 'food', '(especially', 'wings),', 'and', \"don't\", 'go', 'if', 'you', 'have', 'a', 'limited', 'amount', 'of', 'time.']\n",
            "Tokenized: ['[CLS]', 'nice', 'local', 'pub', ',', 'excellent', 'food', '(', 'especially', 'wings', ')', ',', 'and', 'don', \"'\", 't', 'go', 'if', 'you', 'have', 'a', 'limited', 'amount', 'of', 'time', '.', '[SEP]']\n",
            "Original: ['Rooms', 'were', 'outdated,', 'dirty,', 'and', 'small.']\n",
            "Tokenized: ['[CLS]', 'rooms', 'were', 'outdated', ',', 'dirty', ',', 'and', 'small', '.', '[SEP]']\n",
            "Original: ['Service', 'was', 'horrible.']\n",
            "Tokenized: ['[CLS]', 'service', 'was', 'horrible', '.', '[SEP]']\n",
            "Original: ['Go', 'down', '1', 'block', 'to', 'Super', '8.']\n",
            "Tokenized: ['[CLS]', 'go', 'down', '1', 'block', 'to', 'super', '8', '.', '[SEP]']\n",
            "Original: ['Clean', '&', 'tidy', 'with', 'good', 'atmosphere', '&', 'pleasant', 'staff.']\n",
            "Tokenized: ['[CLS]', 'clean', '&', 'tidy', 'with', 'good', 'atmosphere', '&', 'pleasant', 'staff', '.', '[SEP]']\n",
            "Original: ['Food', 'drastically', \"let's\", 'the', 'place', 'down', 'though']\n",
            "Tokenized: ['[CLS]', 'food', 'drastically', 'let', \"'\", 's', 'the', 'place', 'down', 'though', '[SEP]']\n",
            "Original: ['Wonderful', 'service', 'for', 'large', 'group']\n",
            "Tokenized: ['[CLS]', 'wonderful', 'service', 'for', 'large', 'group', '[SEP]']\n",
            "Original: ['I', 'had', 'my', 'wedding', 'luncheon', 'at', 'this', 'BJs', 'restaurant,', 'and', 'it', 'was', 'one', 'of', 'the', 'best', 'choices', 'that', 'I', 'made.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'my', 'wedding', 'lunch', '##eon', 'at', 'this', 'b', '##js', 'restaurant', ',', 'and', 'it', 'was', 'one', 'of', 'the', 'best', 'choices', 'that', 'i', 'made', '.', '[SEP]']\n",
            "Original: ['It', 'was', 'a', 'great', 'deal--we', 'paid', 'a', 'certain', 'amount', 'per', 'person,', 'and', 'my', 'husband', 'and', 'I', 'chose', '4', 'types', 'of', 'pizza', 'and', 'the', 'servers', 'brought', 'out', 'as', 'much', 'as', 'we', 'wanted.']\n",
            "Tokenized: ['[CLS]', 'it', 'was', 'a', 'great', 'deal', '-', '-', 'we', 'paid', 'a', 'certain', 'amount', 'per', 'person', ',', 'and', 'my', 'husband', 'and', 'i', 'chose', '4', 'types', 'of', 'pizza', 'and', 'the', 'servers', 'brought', 'out', 'as', 'much', 'as', 'we', 'wanted', '.', '[SEP]']\n",
            "Original: ['We', 'were', 'also', 'served', 'salad', 'and', 'soda.']\n",
            "Tokenized: ['[CLS]', 'we', 'were', 'also', 'served', 'salad', 'and', 'soda', '.', '[SEP]']\n",
            "Original: ['We', 'had', 'a', 'large', 'party,', 'about', 'fifty', 'people', 'or', 'so,', 'and', 'yet', 'everything', 'was', 'served', 'quickly', 'and', 'we', 'all', 'had', 'a', 'wonderful', 'time.']\n",
            "Tokenized: ['[CLS]', 'we', 'had', 'a', 'large', 'party', ',', 'about', 'fifty', 'people', 'or', 'so', ',', 'and', 'yet', 'everything', 'was', 'served', 'quickly', 'and', 'we', 'all', 'had', 'a', 'wonderful', 'time', '.', '[SEP]']\n",
            "Original: ['Even', 'though', 'we', 'were', 'only', 'supposed', 'to', 'have', 'those', 'specific', 'types', 'of', 'pizza,', 'when', 'guests', 'asked', 'for', 'a', 'different', 'type,', 'it', 'was', 'brought', 'out', 'with', 'no', 'charge', 'to', 'us!']\n",
            "Tokenized: ['[CLS]', 'even', 'though', 'we', 'were', 'only', 'supposed', 'to', 'have', 'those', 'specific', 'types', 'of', 'pizza', ',', 'when', 'guests', 'asked', 'for', 'a', 'different', 'type', ',', 'it', 'was', 'brought', 'out', 'with', 'no', 'charge', 'to', 'us', '!', '[SEP]']\n",
            "Original: ['I', 'really', 'appreciate', 'BJs', 'for', 'making', 'that', 'special', 'day', 'even', 'better', 'with', 'their', 'wonderful', 'food', 'and', 'service.']\n",
            "Tokenized: ['[CLS]', 'i', 'really', 'appreciate', 'b', '##js', 'for', 'making', 'that', 'special', 'day', 'even', 'better', 'with', 'their', 'wonderful', 'food', 'and', 'service', '.', '[SEP]']\n",
            "Original: ['You', 'are', 'the', 'only', 'one', 'auto', 'glass', 'repair', 'shop', 'in', 'the', 'area', 'I', 'would', 'count', 'on.']\n",
            "Tokenized: ['[CLS]', 'you', 'are', 'the', 'only', 'one', 'auto', 'glass', 'repair', 'shop', 'in', 'the', 'area', 'i', 'would', 'count', 'on', '.', '[SEP]']\n",
            "Original: ['Dr.', 'Mann', 'killed', 'our', 'pet']\n",
            "Tokenized: ['[CLS]', 'dr', '.', 'mann', 'killed', 'our', 'pet', '[SEP]']\n",
            "Original: ['Yea,', 'Dr.', 'Mann', \"ain't\", 'so', 'great.']\n",
            "Tokenized: ['[CLS]', 'ye', '##a', ',', 'dr', '.', 'mann', 'ain', \"'\", 't', 'so', 'great', '.', '[SEP]']\n",
            "Original: ['We', 'took', 'our', 'beloved', 'kitty', 'to', 'him', 'and', 'it', 'came', 'back', 'dead.']\n",
            "Tokenized: ['[CLS]', 'we', 'took', 'our', 'beloved', 'kitty', 'to', 'him', 'and', 'it', 'came', 'back', 'dead', '.', '[SEP]']\n",
            "Original: ['We', 'live', 'in', 'the', 'sub-division', 'around', 'the', 'corner', 'and', 'after', 'it', 'happened', 'and', 'we', 'brought', 'it', 'up', 'at', 'some', 'neighborhood', 'gatherings', 'we', 'discovered', 'that', 'several', 'people', 'from', 'this', 'neighborhood', 'alone', 'had', 'pets', 'goto', 'Dr.', 'Mann', 'for', 'surgical', 'procedures', 'and', 'came', 'back', 'dead.']\n",
            "Tokenized: ['[CLS]', 'we', 'live', 'in', 'the', 'sub', '-', 'division', 'around', 'the', 'corner', 'and', 'after', 'it', 'happened', 'and', 'we', 'brought', 'it', 'up', 'at', 'some', 'neighborhood', 'gatherings', 'we', 'discovered', 'that', 'several', 'people', 'from', 'this', 'neighborhood', 'alone', 'had', 'pets', 'got', '##o', 'dr', '.', 'mann', 'for', 'surgical', 'procedures', 'and', 'came', 'back', 'dead', '.', '[SEP]']\n",
            "Original: ['If', 'you', 'take', 'him', 'here', 'for', 'shots,', 'no', 'big', 'deal', 'but', 'I', 'would', 'never', 'let', 'this', 'man', 'apply', 'anesthetic', 'to', 'my', 'pet', 'ever', 'again.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'take', 'him', 'here', 'for', 'shots', ',', 'no', 'big', 'deal', 'but', 'i', 'would', 'never', 'let', 'this', 'man', 'apply', 'an', '##est', '##hetic', 'to', 'my', 'pet', 'ever', 'again', '.', '[SEP]']\n",
            "Original: ['Oh,', 'they', 'also', 'charged', 'me', 'for', 'the', 'procedure', '($250)', 'AND', 'had', 'the', 'audacity', 'to', 'charge', 'me', 'a', '$25', \"'DISPOSAL'\", 'fee.']\n",
            "Tokenized: ['[CLS]', 'oh', ',', 'they', 'also', 'charged', 'me', 'for', 'the', 'procedure', '(', '$', '250', ')', 'and', 'had', 'the', 'au', '##da', '##city', 'to', 'charge', 'me', 'a', '$', '25', \"'\", 'disposal', \"'\", 'fee', '.', '[SEP]']\n",
            "Original: ['They', 'actually', 'itemized', 'it', 'as', 'a', 'DISPOSAL', 'fee.']\n",
            "Tokenized: ['[CLS]', 'they', 'actually', 'item', '##ized', 'it', 'as', 'a', 'disposal', 'fee', '.', '[SEP]']\n",
            "Original: ['AVOID', 'AT', 'ALL', 'COSTS.']\n",
            "Tokenized: ['[CLS]', 'avoid', 'at', 'all', 'costs', '.', '[SEP]']\n",
            "Original: ['Channel', 'Guide']\n",
            "Tokenized: ['[CLS]', 'channel', 'guide', '[SEP]']\n",
            "Original: ['Believe', 'it', 'or', 'not,', 'but', 'the', 'channel', 'guide', 'has', 'been', 'most', 'helpful', 'to', 'my', 'family', 'members', 'that', 'visit', 'and', \"don't\", 'know', 'where', 'to', 'start', 'when', 'it', 'comes', 'to', 'watching', 'satellite', 'tv.']\n",
            "Tokenized: ['[CLS]', 'believe', 'it', 'or', 'not', ',', 'but', 'the', 'channel', 'guide', 'has', 'been', 'most', 'helpful', 'to', 'my', 'family', 'members', 'that', 'visit', 'and', 'don', \"'\", 't', 'know', 'where', 'to', 'start', 'when', 'it', 'comes', 'to', 'watching', 'satellite', 'tv', '.', '[SEP]']\n",
            "Original: ['I', 'just', 'give', 'them', 'guide', 'and', 'they', 'can', 'find', 'anything', 'they', 'need.']\n",
            "Tokenized: ['[CLS]', 'i', 'just', 'give', 'them', 'guide', 'and', 'they', 'can', 'find', 'anything', 'they', 'need', '.', '[SEP]']\n",
            "Original: ['Thanks', 'again,', 'Directv.']\n",
            "Tokenized: ['[CLS]', 'thanks', 'again', ',', 'direct', '##v', '.', '[SEP]']\n",
            "Original: ['Lovely', 'People,', 'Great', 'Hats']\n",
            "Tokenized: ['[CLS]', 'lovely', 'people', ',', 'great', 'hats', '[SEP]']\n",
            "Original: ['I', 'was', 'saddened', 'to', 'see', 'the', 'reviews', 'that', 'claimed', 'World', 'Hats', 'Mart', 'has', 'poor', 'service.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'sad', '##dened', 'to', 'see', 'the', 'reviews', 'that', 'claimed', 'world', 'hats', 'mart', 'has', 'poor', 'service', '.', '[SEP]']\n",
            "Original: ['It', 'led', 'me', 'to', 'believe', 'that', 'the', 'reviewers', 'simply', 'had', 'difficulty', 'tolerating', 'people', 'with', 'strongly-accented', 'English.']\n",
            "Tokenized: ['[CLS]', 'it', 'led', 'me', 'to', 'believe', 'that', 'the', 'reviewers', 'simply', 'had', 'difficulty', 'to', '##ler', '##ating', 'people', 'with', 'strongly', '-', 'accent', '##ed', 'english', '.', '[SEP]']\n",
            "Original: ['The', 'husband', 'and', 'wife', 'who', 'run', 'this', 'spot', 'are', 'lovely', 'people.']\n",
            "Tokenized: ['[CLS]', 'the', 'husband', 'and', 'wife', 'who', 'run', 'this', 'spot', 'are', 'lovely', 'people', '.', '[SEP]']\n",
            "Original: ['We', 'have', 'had', 'many', 'conversations', 'with', 'them', 'and', 'they', 'have', 'always', 'been', 'extremely', 'patient', 'with', 'me', 'as', 'I', 'tried', 'on', 'multiple', 'hats', 'in', 'search', 'of', 'the', 'right', 'costumes', 'for', 'my', 'magic', 'act.']\n",
            "Tokenized: ['[CLS]', 'we', 'have', 'had', 'many', 'conversations', 'with', 'them', 'and', 'they', 'have', 'always', 'been', 'extremely', 'patient', 'with', 'me', 'as', 'i', 'tried', 'on', 'multiple', 'hats', 'in', 'search', 'of', 'the', 'right', 'costumes', 'for', 'my', 'magic', 'act', '.', '[SEP]']\n",
            "Original: ['I', 'recommend', 'them', 'highly!']\n",
            "Tokenized: ['[CLS]', 'i', 'recommend', 'them', 'highly', '!', '[SEP]']\n",
            "Original: ['The', 'Worst', 'Chinese', \"I've\", 'Ever', 'Had']\n",
            "Tokenized: ['[CLS]', 'the', 'worst', 'chinese', 'i', \"'\", 've', 'ever', 'had', '[SEP]']\n",
            "Original: ['This', 'is', 'by', 'far', 'the', 'worst', 'chinese', 'food', 'I', 'have', 'ever', 'had.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'by', 'far', 'the', 'worst', 'chinese', 'food', 'i', 'have', 'ever', 'had', '.', '[SEP]']\n",
            "Original: ['The', 'service', 'stunk.']\n",
            "Tokenized: ['[CLS]', 'the', 'service', 'stu', '##nk', '.', '[SEP]']\n",
            "Original: ['I', 'called', 'in', 'my', 'order', 'and', 'upon', 'arriving', 'to', 'pick', 'it', 'up,', 'they', 'got', 'my', 'order', 'confused', 'with', 'someone', 'elses.']\n",
            "Tokenized: ['[CLS]', 'i', 'called', 'in', 'my', 'order', 'and', 'upon', 'arriving', 'to', 'pick', 'it', 'up', ',', 'they', 'got', 'my', 'order', 'confused', 'with', 'someone', 'else', '##s', '.', '[SEP]']\n",
            "Original: ['They', 'helped', 'about', 'three', 'other', 'people', 'before', 'they', 'offered', 'to', 'help', 'me', 'again.']\n",
            "Tokenized: ['[CLS]', 'they', 'helped', 'about', 'three', 'other', 'people', 'before', 'they', 'offered', 'to', 'help', 'me', 'again', '.', '[SEP]']\n",
            "Original: ['They', 'also', 'got', 'my', 'friends', 'order', 'mixed', 'up', 'and', 'wanted', 'to', 'charger', 'her', '$10', 'more', 'than', 'what', 'she', 'had', 'wanted.']\n",
            "Tokenized: ['[CLS]', 'they', 'also', 'got', 'my', 'friends', 'order', 'mixed', 'up', 'and', 'wanted', 'to', 'charge', '##r', 'her', '$', '10', 'more', 'than', 'what', 'she', 'had', 'wanted', '.', '[SEP]']\n",
            "Original: ['She', 'asked', 'for', 'the', 'dinner', 'combo', 'and', 'they', 'gave', 'her', 'two', 'dinner', 'plates', 'instead.']\n",
            "Tokenized: ['[CLS]', 'she', 'asked', 'for', 'the', 'dinner', 'combo', 'and', 'they', 'gave', 'her', 'two', 'dinner', 'plates', 'instead', '.', '[SEP]']\n",
            "Original: ['We', 'were', 'standing', 'in', 'the', 'store', 'for', '20minutes', 'to', 'simply', 'pick', 'up', 'an', 'order.']\n",
            "Tokenized: ['[CLS]', 'we', 'were', 'standing', 'in', 'the', 'store', 'for', '20', '##min', '##ute', '##s', 'to', 'simply', 'pick', 'up', 'an', 'order', '.', '[SEP]']\n",
            "Original: ['Not', 'to', 'mention', 'that', 'the', 'wait', 'staff', 'was', 'about', 'as', 'pleasant', 'as', 'dealing', 'with', 'an', 'angry', 'bull.']\n",
            "Tokenized: ['[CLS]', 'not', 'to', 'mention', 'that', 'the', 'wait', 'staff', 'was', 'about', 'as', 'pleasant', 'as', 'dealing', 'with', 'an', 'angry', 'bull', '.', '[SEP]']\n",
            "Original: ['They', 'were', 'abrasive', 'and', 'rude', '-', 'when', 'they', 'were', 'the', 'ones', 'who', 'messed', 'everything', 'up.']\n",
            "Tokenized: ['[CLS]', 'they', 'were', 'ab', '##ras', '##ive', 'and', 'rude', '-', 'when', 'they', 'were', 'the', 'ones', 'who', 'messed', 'everything', 'up', '.', '[SEP]']\n",
            "Original: ['We', 'had', 'to', 'throw', 'out', 'about', '80', 'percent', 'of', 'our', 'meals', 'because', 'the', 'food', 'tasted', 'so', 'horrible.']\n",
            "Tokenized: ['[CLS]', 'we', 'had', 'to', 'throw', 'out', 'about', '80', 'percent', 'of', 'our', 'meals', 'because', 'the', 'food', 'tasted', 'so', 'horrible', '.', '[SEP]']\n",
            "Original: ['I', 'dont', 'know', 'how', 'it', 'is', 'possible', 'to', 'make', 'orange', 'chicken,', 'sesame', 'chicken', 'and', 'kung', 'pao', 'chicken', 'as', 'well', 'as', 'cheese', 'puffs', 'taste', 'THAT', 'bad', 'but', 'China', 'Delight', 'accomplished', 'that.']\n",
            "Tokenized: ['[CLS]', 'i', 'don', '##t', 'know', 'how', 'it', 'is', 'possible', 'to', 'make', 'orange', 'chicken', ',', 'sesame', 'chicken', 'and', 'kung', 'pa', '##o', 'chicken', 'as', 'well', 'as', 'cheese', 'puff', '##s', 'taste', 'that', 'bad', 'but', 'china', 'delight', 'accomplished', 'that', '.', '[SEP]']\n",
            "Original: ['The', 'only', 'thing', 'that', 'was', 'edible', 'was', 'the', 'steamed', 'rice', 'and', 'the', 'vegetable', 'lo', 'mein', 'was', 'barely', 'tolerable.']\n",
            "Tokenized: ['[CLS]', 'the', 'only', 'thing', 'that', 'was', 'edible', 'was', 'the', 'steamed', 'rice', 'and', 'the', 'vegetable', 'lo', 'mein', 'was', 'barely', 'to', '##ler', '##able', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'NEVER', 'go', 'here', 'again.']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'never', 'go', 'here', 'again', '.', '[SEP]']\n",
            "Original: ['Lucky', 'Panda', 'in', 'Willis', 'is', 'a', 'billion', 'times', 'better', 'in', 'service', 'and', 'quality', 'of', 'the', 'meal.']\n",
            "Tokenized: ['[CLS]', 'lucky', 'panda', 'in', 'willis', 'is', 'a', 'billion', 'times', 'better', 'in', 'service', 'and', 'quality', 'of', 'the', 'meal', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'no', 'idea', 'how', 'China', 'Delight', 'won', 'number', '1', 'Chinese', 'restaurant', 'in', 'Montgomery', '-', 'There', 'needs', 'to', 'be', 'a', 'recount', 'on', 'that', 'vote.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'no', 'idea', 'how', 'china', 'delight', 'won', 'number', '1', 'chinese', 'restaurant', 'in', 'montgomery', '-', 'there', 'needs', 'to', 'be', 'a', 'rec', '##ount', 'on', 'that', 'vote', '.', '[SEP]']\n",
            "Original: ['Hobbs', 'on', 'Mass.']\n",
            "Tokenized: ['[CLS]', 'hobbs', 'on', 'mass', '.', '[SEP]']\n",
            "Original: ['Absolutely', 'my', 'favorite', 'store', 'in', 'Lawrence,', 'KS']\n",
            "Tokenized: ['[CLS]', 'absolutely', 'my', 'favorite', 'store', 'in', 'lawrence', ',', 'ks', '[SEP]']\n",
            "Original: ['The', 'Best', 'Breakfast', 'in', 'Solana', 'Beach!']\n",
            "Tokenized: ['[CLS]', 'the', 'best', 'breakfast', 'in', 'sol', '##ana', 'beach', '!', '[SEP]']\n",
            "Original: ['We', 'love', \"T's\", 'Cafe!']\n",
            "Tokenized: ['[CLS]', 'we', 'love', 't', \"'\", 's', 'cafe', '!', '[SEP]']\n",
            "Original: ['Without', 'a', 'doubt', 'the', 'best', 'place', 'to', 'grab', 'a', 'tall', 'bloody', 'mary', 'and', 'some', 'eggs', 'benedict.']\n",
            "Tokenized: ['[CLS]', 'without', 'a', 'doubt', 'the', 'best', 'place', 'to', 'grab', 'a', 'tall', 'bloody', 'mary', 'and', 'some', 'eggs', 'benedict', '.', '[SEP]']\n",
            "Original: [\"T's\", 'has', 'been', 'a', 'North', 'County', 'landmark', 'for', 'thirty', 'years', 'and', 'with', 'good', 'reason.']\n",
            "Tokenized: ['[CLS]', 't', \"'\", 's', 'has', 'been', 'a', 'north', 'county', 'landmark', 'for', 'thirty', 'years', 'and', 'with', 'good', 'reason', '.', '[SEP]']\n",
            "Original: ['Family', 'owned', 'and', 'operated', 'makes', 'sure', 'the', 'atmosphere', 'is', 'relaxed', 'and', 'the', 'food', 'home-cooked', 'with', 'style.']\n",
            "Tokenized: ['[CLS]', 'family', 'owned', 'and', 'operated', 'makes', 'sure', 'the', 'atmosphere', 'is', 'relaxed', 'and', 'the', 'food', 'home', '-', 'cooked', 'with', 'style', '.', '[SEP]']\n",
            "Original: ['I', 'highly', 'recommend', 'picking', 'up', 'a', 'jug', 'of', 'their', 'homemade', 'bloody', 'mary', 'mix', '-', 'definitely', 'the', 'best.']\n",
            "Tokenized: ['[CLS]', 'i', 'highly', 'recommend', 'picking', 'up', 'a', 'jug', 'of', 'their', 'homemade', 'bloody', 'mary', 'mix', '-', 'definitely', 'the', 'best', '.', '[SEP]']\n",
            "Original: ['If', \"you've\", 'been', 'to', 'North', 'County,', 'chances', 'are', \"it's\", 'in', 'your', 'favorites', 'list', 'already.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', \"'\", 've', 'been', 'to', 'north', 'county', ',', 'chances', 'are', 'it', \"'\", 's', 'in', 'your', 'favorites', 'list', 'already', '.', '[SEP]']\n",
            "Original: ['Quick', 'and', 'Cheap']\n",
            "Tokenized: ['[CLS]', 'quick', 'and', 'cheap', '[SEP]']\n",
            "Original: ['Walked', 'in', 'and', 'was', 'outta', 'there', 'in', '10', 'mins', 'with', 'a', 'really', 'good', 'deal', 'i', 'thought', 'i', 'was', 'going', 'to', 'be', 'paying', 'alot', 'because', 'i', 'had', 'a', 'DUI', 'but', 'with', 'my', 'DUI', 'and', 'Sr-22', 'they', 'were', 'able', 'to', 'get', 'me', 'the', 'best', 'deal', 'out', 'there.']\n",
            "Tokenized: ['[CLS]', 'walked', 'in', 'and', 'was', 'outta', 'there', 'in', '10', 'min', '##s', 'with', 'a', 'really', 'good', 'deal', 'i', 'thought', 'i', 'was', 'going', 'to', 'be', 'paying', 'al', '##ot', 'because', 'i', 'had', 'a', 'du', '##i', 'but', 'with', 'my', 'du', '##i', 'and', 'sr', '-', '22', 'they', 'were', 'able', 'to', 'get', 'me', 'the', 'best', 'deal', 'out', 'there', '.', '[SEP]']\n",
            "Original: ['This', 'place', 'is', 'bad.']\n",
            "Tokenized: ['[CLS]', 'this', 'place', 'is', 'bad', '.', '[SEP]']\n",
            "Original: [\"It's\", 'dark,', 'dingy', '&', 'dirty.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'dark', ',', 'ding', '##y', '&', 'dirty', '.', '[SEP]']\n",
            "Original: ['The', 'salads', 'are', 'limp', 'and', 'the', 'rest', 'of', 'the', 'food', \"isn't\", 'any', 'better', '(ok,', 'the', 'nachos', 'are', 'not', 'too', 'bad!)']\n",
            "Tokenized: ['[CLS]', 'the', 'salad', '##s', 'are', 'limp', 'and', 'the', 'rest', 'of', 'the', 'food', 'isn', \"'\", 't', 'any', 'better', '(', 'ok', ',', 'the', 'na', '##cho', '##s', 'are', 'not', 'too', 'bad', '!', ')', '[SEP]']\n",
            "Original: ['This', 'place', 'may', 'have', 'been', 'something', 'sometime;', 'but', 'it', 'way', 'past', 'it', '\"sell', 'by', 'date\".']\n",
            "Tokenized: ['[CLS]', 'this', 'place', 'may', 'have', 'been', 'something', 'sometime', ';', 'but', 'it', 'way', 'past', 'it', '\"', 'sell', 'by', 'date', '\"', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'eaten', 'here', 'twice', 'in', 'the', 'past', 'year', 'and', 'will', 'not', 'go', 'back', 'and', 'cannot', 'recommend', 'it.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'eaten', 'here', 'twice', 'in', 'the', 'past', 'year', 'and', 'will', 'not', 'go', 'back', 'and', 'cannot', 'recommend', 'it', '.', '[SEP]']\n",
            "Original: ['Le', 'petit', 'is', 'the', 'best', 'place', 'to', 'get', 'your', 'nails', 'done!']\n",
            "Tokenized: ['[CLS]', 'le', 'petit', 'is', 'the', 'best', 'place', 'to', 'get', 'your', 'nails', 'done', '!', '[SEP]']\n",
            "Original: ['It', 'is', 'very', 'clean,', 'staff', 'is', 'friendly,', 'and', 'I', 'have', 'never', 'waited!']\n",
            "Tokenized: ['[CLS]', 'it', 'is', 'very', 'clean', ',', 'staff', 'is', 'friendly', ',', 'and', 'i', 'have', 'never', 'waited', '!', '[SEP]']\n",
            "Original: ['I', 'go', 'every', 'other', 'week', 'for', 'the', 'shallac/gel', 'manicure', 'which', 'is', 'only', '$25', 'and', 'it', 'truly', 'lasts', '2', 'weeks!']\n",
            "Tokenized: ['[CLS]', 'i', 'go', 'every', 'other', 'week', 'for', 'the', 'shall', '##ac', '/', 'gel', 'mani', '##cure', 'which', 'is', 'only', '$', '25', 'and', 'it', 'truly', 'lasts', '2', 'weeks', '!', '[SEP]']\n",
            "Original: ['I', 'love', 'it.']\n",
            "Tokenized: ['[CLS]', 'i', 'love', 'it', '.', '[SEP]']\n",
            "Original: ['Pedicures', 'are', 'also', 'great.']\n",
            "Tokenized: ['[CLS]', 'pe', '##dic', '##ures', 'are', 'also', 'great', '.', '[SEP]']\n",
            "Original: ['Try', 'this', 'place', 'out!']\n",
            "Tokenized: ['[CLS]', 'try', 'this', 'place', 'out', '!', '[SEP]']\n",
            "Original: ['I', 'promise', 'you', 'will', 'not', 'be', 'disappointed!']\n",
            "Tokenized: ['[CLS]', 'i', 'promise', 'you', 'will', 'not', 'be', 'disappointed', '!', '[SEP]']\n",
            "Original: ['Excellent', 'Service', 'and', 'Reasonable', 'Prices']\n",
            "Tokenized: ['[CLS]', 'excellent', 'service', 'and', 'reasonable', 'prices', '[SEP]']\n",
            "Original: ['Boutique', 'stores', 'dealing', 'in', \"children's\", 'clothing/gifts', 'are', 'often', 'outrageously', 'priced', '(who', 'wants', 'to', 'pay', '40', 'dollars', 'for', 'a', 'newborn', 'onesie?)']\n",
            "Tokenized: ['[CLS]', 'boutique', 'stores', 'dealing', 'in', 'children', \"'\", 's', 'clothing', '/', 'gifts', 'are', 'often', 'outrageous', '##ly', 'priced', '(', 'who', 'wants', 'to', 'pay', '40', 'dollars', 'for', 'a', 'newborn', 'ones', '##ie', '?', ')', '[SEP]']\n",
            "Original: ['but', 'I', 'was', 'pleasantly', 'surprised', 'to', 'find', 'that', 'the', 'Purple', \"Goose's\", 'prices', 'are', 'reasonable', '(for', 'the', 'SAME', 'products', 'found', 'at', 'other', 'area', 'boutiques,', 'the', 'prices', 'were', '20-25%', 'cheaper).']\n",
            "Tokenized: ['[CLS]', 'but', 'i', 'was', 'pleasantly', 'surprised', 'to', 'find', 'that', 'the', 'purple', 'goose', \"'\", 's', 'prices', 'are', 'reasonable', '(', 'for', 'the', 'same', 'products', 'found', 'at', 'other', 'area', 'boutique', '##s', ',', 'the', 'prices', 'were', '20', '-', '25', '%', 'cheaper', ')', '.', '[SEP]']\n",
            "Original: ['The', 'service', 'was', 'also', 'excellent', '-', 'friendly,', 'helpful', 'and', 'informative', 'without', 'being', 'overbearing.']\n",
            "Tokenized: ['[CLS]', 'the', 'service', 'was', 'also', 'excellent', '-', 'friendly', ',', 'helpful', 'and', 'inform', '##ative', 'without', 'being', 'over', '##be', '##aring', '.', '[SEP]']\n",
            "Original: ['Will', 'definitely', 'return.']\n",
            "Tokenized: ['[CLS]', 'will', 'definitely', 'return', '.', '[SEP]']\n",
            "Original: ['STAY', 'AWAY']\n",
            "Tokenized: ['[CLS]', 'stay', 'away', '[SEP]']\n",
            "Original: ['Horrible', 'service.']\n",
            "Tokenized: ['[CLS]', 'horrible', 'service', '.', '[SEP]']\n",
            "Original: ['Absolutely', 'rude.']\n",
            "Tokenized: ['[CLS]', 'absolutely', 'rude', '.', '[SEP]']\n",
            "Original: ['Did', 'services', 'I', 'asked', 'them', 'NOTto', 'do', 'and', 'was', 'still', 'charged.']\n",
            "Tokenized: ['[CLS]', 'did', 'services', 'i', 'asked', 'them', 'not', '##to', 'do', 'and', 'was', 'still', 'charged', '.', '[SEP]']\n",
            "Original: ['Great', 'Barber']\n",
            "Tokenized: ['[CLS]', 'great', 'barber', '[SEP]']\n",
            "Original: ['Firstly,', 'the', 'other', 'reviewer', 'clearly', 'has', 'never', 'been', 'to', \"Nick's,\", 'or', 'he', 'would', 'know', 'that', 'Nick', 'only', 'charges', '$13', 'for', 'a', 'haircut', 'which', 'is', 'pretty', 'much', 'industry', 'standard.']\n",
            "Tokenized: ['[CLS]', 'firstly', ',', 'the', 'other', 'reviewer', 'clearly', 'has', 'never', 'been', 'to', 'nick', \"'\", 's', ',', 'or', 'he', 'would', 'know', 'that', 'nick', 'only', 'charges', '$', '13', 'for', 'a', 'hair', '##cut', 'which', 'is', 'pretty', 'much', 'industry', 'standard', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'going', 'to', 'Nick', 'for', '5', 'months', 'now', 'precisely', 'because', 'he', 'does', 'pay', 'attention', 'to', 'detail.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'going', 'to', 'nick', 'for', '5', 'months', 'now', 'precisely', 'because', 'he', 'does', 'pay', 'attention', 'to', 'detail', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'terrible', 'hair', 'and', 'he', 'really', 'takes', 'his', 'time', 'to', 'make', 'it', 'look', 'right.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'terrible', 'hair', 'and', 'he', 'really', 'takes', 'his', 'time', 'to', 'make', 'it', 'look', 'right', '.', '[SEP]']\n",
            "Original: ['Some', 'of', 'the', 'younger', 'kids', 'that', 'work', 'there', 'are', 'a', 'bit', 'sub', 'par,', 'but', 'if', 'you', 'wait', 'for', 'Nick...', \"you'll\", 'be', 'good.']\n",
            "Tokenized: ['[CLS]', 'some', 'of', 'the', 'younger', 'kids', 'that', 'work', 'there', 'are', 'a', 'bit', 'sub', 'par', ',', 'but', 'if', 'you', 'wait', 'for', 'nick', '.', '.', '.', 'you', \"'\", 'll', 'be', 'good', '.', '[SEP]']\n",
            "Original: ['Home', 'made', 'product']\n",
            "Tokenized: ['[CLS]', 'home', 'made', 'product', '[SEP]']\n",
            "Original: ['I', 'sometimes', 'go', 'into', 'this', 'store', 'just', 'for', 'something', 'to', 'do', 'on', 'a', 'sunday', 'afternoon.']\n",
            "Tokenized: ['[CLS]', 'i', 'sometimes', 'go', 'into', 'this', 'store', 'just', 'for', 'something', 'to', 'do', 'on', 'a', 'sunday', 'afternoon', '.', '[SEP]']\n",
            "Original: ['I', 'love', 'the', 'people,', 'the', 'product', 'and', 'the', 'service!']\n",
            "Tokenized: ['[CLS]', 'i', 'love', 'the', 'people', ',', 'the', 'product', 'and', 'the', 'service', '!', '[SEP]']\n",
            "Original: ['Nothing', 'compares', 'to', 'a', 'home', 'made', 'product', 'that', 'really', 'stands', 'the', 'test', 'of', 'time.', '-']\n",
            "Tokenized: ['[CLS]', 'nothing', 'compares', 'to', 'a', 'home', 'made', 'product', 'that', 'really', 'stands', 'the', 'test', 'of', 'time', '.', '-', '[SEP]']\n",
            "Original: ['The', 'Brick,', 'Ikea,', 'and', \"Leon's\", 'have', 'their', 'place.']\n",
            "Tokenized: ['[CLS]', 'the', 'brick', ',', 'ike', '##a', ',', 'and', 'leon', \"'\", 's', 'have', 'their', 'place', '.', '[SEP]']\n",
            "Original: ['But', 'furniture', 'like', 'this', 'will', 'truly', 'be', 'around', 'forever.']\n",
            "Tokenized: ['[CLS]', 'but', 'furniture', 'like', 'this', 'will', 'truly', 'be', 'around', 'forever', '.', '[SEP]']\n",
            "Original: ['Awesome', 'bacon', 'egg', 'and', 'cheese', 'sandwich', 'for', 'breakfast.']\n",
            "Tokenized: ['[CLS]', 'awesome', 'bacon', 'egg', 'and', 'cheese', 'sandwich', 'for', 'breakfast', '.', '[SEP]']\n",
            "Original: ['You', 'are', 'the', 'best!']\n",
            "Tokenized: ['[CLS]', 'you', 'are', 'the', 'best', '!', '[SEP]']\n",
            "Original: ['Just', 'received', 'from', 'your', 'flower', 'store.']\n",
            "Tokenized: ['[CLS]', 'just', 'received', 'from', 'your', 'flower', 'store', '.', '[SEP]']\n",
            "Original: ['They', 'are', 'sooooo', 'beautiful.']\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'soo', '##oo', '##o', 'beautiful', '.', '[SEP]']\n",
            "Original: ['Thank', 'you', 'guys.']\n",
            "Tokenized: ['[CLS]', 'thank', 'you', 'guys', '.', '[SEP]']\n",
            "Original: ['Rude', 'Rude', 'Rude']\n",
            "Tokenized: ['[CLS]', 'rude', 'rude', 'rude', '[SEP]']\n",
            "Original: ['went', 'in', 'there', 'and', 'got', 'my', 'dog', 'groomed', 'came', 'home', 'to', 'an', 'uneven', 'dog', 'then', 'took', 'him', 'back', 'to', 'get', 'evened', 'up', 'what', 'a', 'mistake!']\n",
            "Tokenized: ['[CLS]', 'went', 'in', 'there', 'and', 'got', 'my', 'dog', 'groom', '##ed', 'came', 'home', 'to', 'an', 'uneven', 'dog', 'then', 'took', 'him', 'back', 'to', 'get', 'even', '##ed', 'up', 'what', 'a', 'mistake', '!', '[SEP]']\n",
            "Original: ['she', 'didnt', 'even', 'let', 'me', 'finish', 'a', 'sentence', 'without', 'insulting', 'me', 'and', 'telling', 'me', 'how', 'i', 'should', 'have', 'said', 'it!']\n",
            "Tokenized: ['[CLS]', 'she', 'didn', '##t', 'even', 'let', 'me', 'finish', 'a', 'sentence', 'without', 'insulting', 'me', 'and', 'telling', 'me', 'how', 'i', 'should', 'have', 'said', 'it', '!', '[SEP]']\n",
            "Original: ['i', 'wont', 'go', 'back!']\n",
            "Tokenized: ['[CLS]', 'i', 'won', '##t', 'go', 'back', '!', '[SEP]']\n",
            "Original: ['she', 'needs', 'to', 'develop', 'a', 'personality', '!']\n",
            "Tokenized: ['[CLS]', 'she', 'needs', 'to', 'develop', 'a', 'personality', '!', '[SEP]']\n",
            "Original: ['brought', 'dog', 'home', 'and', 'its', 'all', 'choppy', 'now!']\n",
            "Tokenized: ['[CLS]', 'brought', 'dog', 'home', 'and', 'its', 'all', 'chop', '##py', 'now', '!', '[SEP]']\n",
            "Original: ['very', 'professional', '/', 'very', 'helpful']\n",
            "Tokenized: ['[CLS]', 'very', 'professional', '/', 'very', 'helpful', '[SEP]']\n",
            "Original: ['the', 'people', 'at', 'Fidelity', 'Leasing', 'were', 'very', 'friendly', 'and', 'helpful.']\n",
            "Tokenized: ['[CLS]', 'the', 'people', 'at', 'fidelity', 'leasing', 'were', 'very', 'friendly', 'and', 'helpful', '.', '[SEP]']\n",
            "Original: ['i', 'was', 'looking', 'for', 'a', 'car', 'but', 'did', 'not', 'really', 'know', 'what', 'i', 'wanted', 'and', 'they', 'were', 'very', 'helpful', 'and', 'took', 'the', 'time', 'to', 'first', 'figure', 'out', 'what', 'my', 'needs', 'were', 'and', 'showing', 'me', 'various', 'options', 'to', 'meet', 'those', 'needs.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'looking', 'for', 'a', 'car', 'but', 'did', 'not', 'really', 'know', 'what', 'i', 'wanted', 'and', 'they', 'were', 'very', 'helpful', 'and', 'took', 'the', 'time', 'to', 'first', 'figure', 'out', 'what', 'my', 'needs', 'were', 'and', 'showing', 'me', 'various', 'options', 'to', 'meet', 'those', 'needs', '.', '[SEP]']\n",
            "Original: ['they', 'seemed', 'more', 'interested', 'in', 'helping', 'me', 'find', 'the', 'right', 'car', 'rather', 'then', 'just', 'make', 'a', 'sale.']\n",
            "Tokenized: ['[CLS]', 'they', 'seemed', 'more', 'interested', 'in', 'helping', 'me', 'find', 'the', 'right', 'car', 'rather', 'then', 'just', 'make', 'a', 'sale', '.', '[SEP]']\n",
            "Original: ['my', 'experience', 'with', 'them', 'was', 'great', '-', 'low', 'stress,', 'very', 'helpful', 'and', 'very', 'personal.']\n",
            "Tokenized: ['[CLS]', 'my', 'experience', 'with', 'them', 'was', 'great', '-', 'low', 'stress', ',', 'very', 'helpful', 'and', 'very', 'personal', '.', '[SEP]']\n",
            "Original: ['after', 'finding', 'the', 'car', 'i', 'wanted', 'they', 'took', 'the', 'time', 'to', 'go', 'over', 'each', 'step', 'and', 'making', 'it', 'as', 'painless', 'as', 'possible.']\n",
            "Tokenized: ['[CLS]', 'after', 'finding', 'the', 'car', 'i', 'wanted', 'they', 'took', 'the', 'time', 'to', 'go', 'over', 'each', 'step', 'and', 'making', 'it', 'as', 'pain', '##less', 'as', 'possible', '.', '[SEP]']\n",
            "Original: ['from', 'start', 'to', 'finish', 'they', 'were', 'top', 'notch.']\n",
            "Tokenized: ['[CLS]', 'from', 'start', 'to', 'finish', 'they', 'were', 'top', 'notch', '.', '[SEP]']\n",
            "Original: ['i', 'would', 'highly', 'recommend', 'calling', 'these', 'people', 'up', 'for', 'your', 'next', 'car.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'highly', 'recommend', 'calling', 'these', 'people', 'up', 'for', 'your', 'next', 'car', '.', '[SEP]']\n",
            "Original: ['Rude', 'and', 'Untrustworthy']\n",
            "Tokenized: ['[CLS]', 'rude', 'and', 'un', '##trust', '##worthy', '[SEP]']\n",
            "Original: ['These', 'guys', 'took', 'Customer', 'Service', '101', 'from', 'a', 'Neanderthal.']\n",
            "Tokenized: ['[CLS]', 'these', 'guys', 'took', 'customer', 'service', '101', 'from', 'a', 'ne', '##ander', '##thal', '.', '[SEP]']\n",
            "Original: ['They', 'are', 'especially', 'rude', 'to', 'women.']\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'especially', 'rude', 'to', 'women', '.', '[SEP]']\n",
            "Original: ['Do', 'not', 'trust', 'them!']\n",
            "Tokenized: ['[CLS]', 'do', 'not', 'trust', 'them', '!', '[SEP]']\n",
            "Original: ['Food', 'Craving', 'Gone', 'and', 'Weight', 'Loss', 'at', 'Acupuncture', 'Doctor']\n",
            "Tokenized: ['[CLS]', 'food', 'craving', 'gone', 'and', 'weight', 'loss', 'at', 'ac', '##up', '##un', '##cture', 'doctor', '[SEP]']\n",
            "Original: ['I', 'am', 'a', 'college', 'student.']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'a', 'college', 'student', '.', '[SEP]']\n",
            "Original: ['Before', 'treatment,', 'my', 'food', 'cravings', 'were', '\"out', 'of', 'control\"', 'which', 'caused', 'me', 'to', 'be', 'stressed', 'out.']\n",
            "Tokenized: ['[CLS]', 'before', 'treatment', ',', 'my', 'food', 'craving', '##s', 'were', '\"', 'out', 'of', 'control', '\"', 'which', 'caused', 'me', 'to', 'be', 'stressed', 'out', '.', '[SEP]']\n",
            "Original: ['I', 'experienced', 'a', 'Definite', 'Decrease', 'in', 'food', 'craving', '(about', '50%)', 'and', 'decrease', 'in', 'stress', 'after', 'the', '1st', 'treatment.']\n",
            "Tokenized: ['[CLS]', 'i', 'experienced', 'a', 'definite', 'decrease', 'in', 'food', 'craving', '(', 'about', '50', '%', ')', 'and', 'decrease', 'in', 'stress', 'after', 'the', '1st', 'treatment', '.', '[SEP]']\n",
            "Original: ['I', 'actually', 'loss', '4', 'pounds', 'after', 'my', '1st', 'treatment', 'and', '2', 'pounds', 'after', 'my', '2nd', 'treatment.']\n",
            "Tokenized: ['[CLS]', 'i', 'actually', 'loss', '4', 'pounds', 'after', 'my', '1st', 'treatment', 'and', '2', 'pounds', 'after', 'my', '2nd', 'treatment', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'amazed.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'amazed', '.', '[SEP]']\n",
            "Original: ['I', 'am', 'now', 'more', 'at', 'peace', 'and', 'my', 'food', 'craving', 'is', 'about', '99%', 'gone', 'after', 'only', '3', 'treatments.']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'now', 'more', 'at', 'peace', 'and', 'my', 'food', 'craving', 'is', 'about', '99', '%', 'gone', 'after', 'only', '3', 'treatments', '.', '[SEP]']\n",
            "Original: ['Before', 'coming', 'to', 'Acupuncture', 'DOCTOR', 'I', 'was', 'a', 'big', 'baby', 'about', 'needles', 'and', 'only', 'came', 'because', 'my', \"boyfriend's\", 'aunt', 'recommended', 'it.']\n",
            "Tokenized: ['[CLS]', 'before', 'coming', 'to', 'ac', '##up', '##un', '##cture', 'doctor', 'i', 'was', 'a', 'big', 'baby', 'about', 'needles', 'and', 'only', 'came', 'because', 'my', 'boyfriend', \"'\", 's', 'aunt', 'recommended', 'it', '.', '[SEP]']\n",
            "Original: ['It', 'hurt', 'very', 'little,', 'felt', 'more', 'like', 'pressure', 'than', 'pain.']\n",
            "Tokenized: ['[CLS]', 'it', 'hurt', 'very', 'little', ',', 'felt', 'more', 'like', 'pressure', 'than', 'pain', '.', '[SEP]']\n",
            "Original: ['What', 'I', 'like', 'most', 'about', 'Dr.', 'Liau', 'is', 'that', 'she', 'is', 'very', 'caring.']\n",
            "Tokenized: ['[CLS]', 'what', 'i', 'like', 'most', 'about', 'dr', '.', 'lia', '##u', 'is', 'that', 'she', 'is', 'very', 'caring', '.', '[SEP]']\n",
            "Original: ['She', 'talks', 'to', 'you', 'at', 'each', 'appointment.']\n",
            "Tokenized: ['[CLS]', 'she', 'talks', 'to', 'you', 'at', 'each', 'appointment', '.', '[SEP]']\n",
            "Original: ['I', 'can', 'tell', 'she', 'really', 'cares', 'and', 'wants', 'to', 'help.']\n",
            "Tokenized: ['[CLS]', 'i', 'can', 'tell', 'she', 'really', 'cares', 'and', 'wants', 'to', 'help', '.', '[SEP]']\n",
            "Original: ['I', 'am', 'SO', 'GLAD', 'to', 'have', 'found', 'Dr.', 'Liau.']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'so', 'glad', 'to', 'have', 'found', 'dr', '.', 'lia', '##u', '.', '[SEP]']\n",
            "Original: ['Now', 'I', 'feel', 'more', 'confident', 'wearing', 'my', 'bathing', 'suit', 'in', 'the', 'summer.']\n",
            "Tokenized: ['[CLS]', 'now', 'i', 'feel', 'more', 'confident', 'wearing', 'my', 'bathing', 'suit', 'in', 'the', 'summer', '.', '[SEP]']\n",
            "Original: ['College', 'is', 'a', 'Joke', 'and', 'the', 'Salon', 'is', 'a', 'JOKE!']\n",
            "Tokenized: ['[CLS]', 'college', 'is', 'a', 'joke', 'and', 'the', 'salon', 'is', 'a', 'joke', '!', '[SEP]']\n",
            "Original: ['Went', 'to', 'the', 'school', 'here', 'OVER', 'PRICED!!!!!!']\n",
            "Tokenized: ['[CLS]', 'went', 'to', 'the', 'school', 'here', 'over', 'priced', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['You', 'do', 'NOT', 'learn', 'the', 'things', 'you', 'were', 'promised.']\n",
            "Tokenized: ['[CLS]', 'you', 'do', 'not', 'learn', 'the', 'things', 'you', 'were', 'promised', '.', '[SEP]']\n",
            "Original: ['You', 'have', 'to', 'bring', 'in', 'your', 'own', 'models', 'and', 'they', 'have', 'to', 'pay', 'for', 'you', 'to', 'use', 'them', 'if', 'you', 'dont', 'then', 'you', 'can', 'graduate!']\n",
            "Tokenized: ['[CLS]', 'you', 'have', 'to', 'bring', 'in', 'your', 'own', 'models', 'and', 'they', 'have', 'to', 'pay', 'for', 'you', 'to', 'use', 'them', 'if', 'you', 'don', '##t', 'then', 'you', 'can', 'graduate', '!', '[SEP]']\n",
            "Original: ['Going', 'back', 'after', 'graduating', 'your', 'told', 'you', 'get', 'a', 'discount', 'on', 'services', 'nope', 'you', 'dont.']\n",
            "Tokenized: ['[CLS]', 'going', 'back', 'after', 'graduating', 'your', 'told', 'you', 'get', 'a', 'discount', 'on', 'services', 'nope', 'you', 'don', '##t', '.', '[SEP]']\n",
            "Original: ['Staff', 'is', 'under', 'educated.']\n",
            "Tokenized: ['[CLS]', 'staff', 'is', 'under', 'educated', '.', '[SEP]']\n",
            "Original: ['Going', 'there', 'you', 'learn', 'the', 'school', 'does', 'not', 'care', 'about', 'the', 'services', 'given', 'just', 'about', 'the', 'money.']\n",
            "Tokenized: ['[CLS]', 'going', 'there', 'you', 'learn', 'the', 'school', 'does', 'not', 'care', 'about', 'the', 'services', 'given', 'just', 'about', 'the', 'money', '.', '[SEP]']\n",
            "Original: ['Over', 'priced', 'for', 'students', 'to', 'learn!']\n",
            "Tokenized: ['[CLS]', 'over', 'priced', 'for', 'students', 'to', 'learn', '!', '[SEP]']\n",
            "Original: ['Beware', 'of', 'the', 'nail', 'program', 'you', 'are', 'not', 'taught', 'to', 'use', 'a', 'nail', 'drill', 'AT', 'ALL', 'you', 'learn', 'the', 'old', 'fashioned', 'way', 'of', 'doing', 'nails', 'you', 'will', 'not', 'be', 'able', 'to', 'do', 'well', 'in', 'a', 'salon!!!']\n",
            "Tokenized: ['[CLS]', 'be', '##ware', 'of', 'the', 'nail', 'program', 'you', 'are', 'not', 'taught', 'to', 'use', 'a', 'nail', 'drill', 'at', 'all', 'you', 'learn', 'the', 'old', 'fashioned', 'way', 'of', 'doing', 'nails', 'you', 'will', 'not', 'be', 'able', 'to', 'do', 'well', 'in', 'a', 'salon', '!', '!', '!', '[SEP]']\n",
            "Original: ['BEWARE!']\n",
            "Tokenized: ['[CLS]', 'be', '##ware', '!', '[SEP]']\n",
            "Original: ['Tried', 'Crust', 'on', 'Broad', 'on', '3', 'occasions.']\n",
            "Tokenized: ['[CLS]', 'tried', 'crust', 'on', 'broad', 'on', '3', 'occasions', '.', '[SEP]']\n",
            "Original: ['Twice', 'for', 'dinner', 'and', 'once', 'for', 'lunch', 'Absolutely', 'rude', 'service', 'every', 'time!']\n",
            "Tokenized: ['[CLS]', 'twice', 'for', 'dinner', 'and', 'once', 'for', 'lunch', 'absolutely', 'rude', 'service', 'every', 'time', '!', '[SEP]']\n",
            "Original: ['The', 'staff', 'will', 'not', 'even', 'answer', 'the', 'phone', 'for', 'take', 'out.']\n",
            "Tokenized: ['[CLS]', 'the', 'staff', 'will', 'not', 'even', 'answer', 'the', 'phone', 'for', 'take', 'out', '.', '[SEP]']\n",
            "Original: ['Tonight,', 'I', 'called', 'several', 'times', 'with', 'no', 'answer', '(Btwn', '5:30', 'and', '6', 'pm)', 'and', 'finally', 'drove', 'there', 'to', 'place', 'my', 'order', 'in', 'person.']\n",
            "Tokenized: ['[CLS]', 'tonight', ',', 'i', 'called', 'several', 'times', 'with', 'no', 'answer', '(', 'bt', '##wn', '5', ':', '30', 'and', '6', 'pm', ')', 'and', 'finally', 'drove', 'there', 'to', 'place', 'my', 'order', 'in', 'person', '.', '[SEP]']\n",
            "Original: ['There', 'was', 'not', 'a', 'customer', 'to', 'be', 'found.']\n",
            "Tokenized: ['[CLS]', 'there', 'was', 'not', 'a', 'customer', 'to', 'be', 'found', '.', '[SEP]']\n",
            "Original: ['Pure', 'Pilates!!']\n",
            "Tokenized: ['[CLS]', 'pure', 'pi', '##lates', '!', '!', '[SEP]']\n",
            "Original: ['It', 'is', 'the', 'real', 'thing', '-', 'I', 'have', 'been', 'practicing', 'Pilates', 'for', 'over', '7', 'years', 'and', 'would', 'not', 'go', 'anywhere', 'else.']\n",
            "Tokenized: ['[CLS]', 'it', 'is', 'the', 'real', 'thing', '-', 'i', 'have', 'been', 'practicing', 'pi', '##lates', 'for', 'over', '7', 'years', 'and', 'would', 'not', 'go', 'anywhere', 'else', '.', '[SEP]']\n",
            "Original: ['It', 'is', 'the', 'attention', 'to', 'detail', 'and', 'the', 'quality', 'of', 'the', 'work', 'taught', 'at', 'TomiPilates', 'that', 'sets', 'this', 'studio', 'apart', 'from', 'the', 'others.']\n",
            "Tokenized: ['[CLS]', 'it', 'is', 'the', 'attention', 'to', 'detail', 'and', 'the', 'quality', 'of', 'the', 'work', 'taught', 'at', 'tom', '##ip', '##ila', '##tes', 'that', 'sets', 'this', 'studio', 'apart', 'from', 'the', 'others', '.', '[SEP]']\n",
            "Original: ['The', 'teachers', 'are', 'highly', 'trained', 'and', 'are', 'expert', 'at', 'handling', 'all', 'types', 'of', 'clients.']\n",
            "Tokenized: ['[CLS]', 'the', 'teachers', 'are', 'highly', 'trained', 'and', 'are', 'expert', 'at', 'handling', 'all', 'types', 'of', 'clients', '.', '[SEP]']\n",
            "Original: ['Took', 'a', 'laptop', 'in', 'for', 'a', 'video', 'cable', 'to', 'be', 'replaced.']\n",
            "Tokenized: ['[CLS]', 'took', 'a', 'laptop', 'in', 'for', 'a', 'video', 'cable', 'to', 'be', 'replaced', '.', '[SEP]']\n",
            "Original: ['Everything', 'except', 'the', 'display', 'worked', 'fine', 'before', 'I', 'took', 'it', 'in.']\n",
            "Tokenized: ['[CLS]', 'everything', 'except', 'the', 'display', 'worked', 'fine', 'before', 'i', 'took', 'it', 'in', '.', '[SEP]']\n",
            "Original: ['The', 'video', 'cable', 'was', 'replaced', 'and', 'suddenly', 'the', 'motherboard', 'was', 'dead.']\n",
            "Tokenized: ['[CLS]', 'the', 'video', 'cable', 'was', 'replaced', 'and', 'suddenly', 'the', 'mother', '##board', 'was', 'dead', '.', '[SEP]']\n",
            "Original: ['Phone', 'calls', \"weren't\", 'returned', 'when', 'promised', 'and', 'the', 'botched', 'repair', 'took', 'a', 'week', 'longer', 'than', 'promised.']\n",
            "Tokenized: ['[CLS]', 'phone', 'calls', 'weren', \"'\", 't', 'returned', 'when', 'promised', 'and', 'the', 'bot', '##ched', 'repair', 'took', 'a', 'week', 'longer', 'than', 'promised', '.', '[SEP]']\n",
            "Original: ['No', 'Customer', 'Service']\n",
            "Tokenized: ['[CLS]', 'no', 'customer', 'service', '[SEP]']\n",
            "Original: ['Employees', 'seemed', 'to', 'be', 'having', 'a', 'good', 'time', 'chatting', 'and', 'laughing', 'with', 'each', 'other,', 'while', 'myself', 'and', 'other', 'customers', 'were', 'completely', 'ignored.']\n",
            "Tokenized: ['[CLS]', 'employees', 'seemed', 'to', 'be', 'having', 'a', 'good', 'time', 'chatting', 'and', 'laughing', 'with', 'each', 'other', ',', 'while', 'myself', 'and', 'other', 'customers', 'were', 'completely', 'ignored', '.', '[SEP]']\n",
            "Original: ['Another', 'person', 'in', 'the', 'store', 'stood', 'there', 'with', 'an', 'item', 'and', 'repeatedly', 'tried', 'to', 'get', 'a', 'sales', 'persons', 'attention.']\n",
            "Tokenized: ['[CLS]', 'another', 'person', 'in', 'the', 'store', 'stood', 'there', 'with', 'an', 'item', 'and', 'repeatedly', 'tried', 'to', 'get', 'a', 'sales', 'persons', 'attention', '.', '[SEP]']\n",
            "Original: ['It', \"wasn't\", 'until', 'he', 'gave', 'up', 'and', 'walked', 'out', 'the', 'door', 'that', 'someone', 'asked', 'Can', 'I', 'help', 'you.']\n",
            "Tokenized: ['[CLS]', 'it', 'wasn', \"'\", 't', 'until', 'he', 'gave', 'up', 'and', 'walked', 'out', 'the', 'door', 'that', 'someone', 'asked', 'can', 'i', 'help', 'you', '.', '[SEP]']\n",
            "Original: ['great', 'garage', 'and', 'customer', 'service.']\n",
            "Tokenized: ['[CLS]', 'great', 'garage', 'and', 'customer', 'service', '.', '[SEP]']\n",
            "Original: ['great', 'knowledge', 'and', 'prices', 'compared', 'to', 'anyone', 'in', 'the', 'industry.']\n",
            "Tokenized: ['[CLS]', 'great', 'knowledge', 'and', 'prices', 'compared', 'to', 'anyone', 'in', 'the', 'industry', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'my', 'favorite', 'coffee', 'store.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'my', 'favorite', 'coffee', 'store', '.', '[SEP]']\n",
            "Original: ['Just', 'ask', 'American', 'Express']\n",
            "Tokenized: ['[CLS]', 'just', 'ask', 'american', 'express', '[SEP]']\n",
            "Original: ['Great', 'People', 'and', 'even', 'better', 'service!']\n",
            "Tokenized: ['[CLS]', 'great', 'people', 'and', 'even', 'better', 'service', '!', '[SEP]']\n",
            "Original: ['Best', 'to', 'deal', 'with!']\n",
            "Tokenized: ['[CLS]', 'best', 'to', 'deal', 'with', '!', '[SEP]']\n",
            "Original: ['In', 'a', 'few', 'words', '...', \"I'm\", 'pleasantly', 'surprised', 'that', 'you', 'can', 'still', 'find', '\"old', 'school\"', 'service', 'out', 'there', 'where', 'company', 'care', 'more', 'about', 'good', 'name', 'and', 'customers', 'than', 'their', 'pockets...']\n",
            "Tokenized: ['[CLS]', 'in', 'a', 'few', 'words', '.', '.', '.', 'i', \"'\", 'm', 'pleasantly', 'surprised', 'that', 'you', 'can', 'still', 'find', '\"', 'old', 'school', '\"', 'service', 'out', 'there', 'where', 'company', 'care', 'more', 'about', 'good', 'name', 'and', 'customers', 'than', 'their', 'pockets', '.', '.', '.', '[SEP]']\n",
            "Original: ['Highly', 'recommended', 'people', '/', 'business.']\n",
            "Tokenized: ['[CLS]', 'highly', 'recommended', 'people', '/', 'business', '.', '[SEP]']\n",
            "Original: ['Thanks', 'Josh!']\n",
            "Tokenized: ['[CLS]', 'thanks', 'josh', '!', '[SEP]']\n",
            "Original: ['Absolutely', 'amazing', 'job!']\n",
            "Tokenized: ['[CLS]', 'absolutely', 'amazing', 'job', '!', '[SEP]']\n",
            "Original: ['Susanna', 'is', 'the', 'best', 'dress', 'maker/tailor', \"I've\", 'ever', 'come', 'across', 'in', 'my', 'whole', 'life!']\n",
            "Tokenized: ['[CLS]', 'susanna', 'is', 'the', 'best', 'dress', 'maker', '/', 'tailor', 'i', \"'\", 've', 'ever', 'come', 'across', 'in', 'my', 'whole', 'life', '!', '[SEP]']\n",
            "Original: ['She', 'makes', 'every', 'item', 'fit', 'you', 'perfectly.']\n",
            "Tokenized: ['[CLS]', 'she', 'makes', 'every', 'item', 'fit', 'you', 'perfectly', '.', '[SEP]']\n",
            "Original: ['She', 'is', 'always', 'so', 'busy,', 'too,', 'which', 'is', 'a', 'good', 'indication', 'of', 'her', 'talent.']\n",
            "Tokenized: ['[CLS]', 'she', 'is', 'always', 'so', 'busy', ',', 'too', ',', 'which', 'is', 'a', 'good', 'indication', 'of', 'her', 'talent', '.', '[SEP]']\n",
            "Original: [\"She's\", 'the', 'best!']\n",
            "Tokenized: ['[CLS]', 'she', \"'\", 's', 'the', 'best', '!', '[SEP]']\n",
            "Original: ['Thank', 'you', 'for', 'helping', 'me', 'get', 'more', 'healthy!']\n",
            "Tokenized: ['[CLS]', 'thank', 'you', 'for', 'helping', 'me', 'get', 'more', 'healthy', '!', '[SEP]']\n",
            "Original: ['I', 'came', 'in', 'and', 'saw', 'Dr.', 'Ruona', 'about', 'a', 'month', 'ago', 'for', 'quitting', 'smoking.']\n",
            "Tokenized: ['[CLS]', 'i', 'came', 'in', 'and', 'saw', 'dr', '.', 'ru', '##ona', 'about', 'a', 'month', 'ago', 'for', 'quit', '##ting', 'smoking', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'to', 'tell', 'you', 'that', 'I', \"haven't\", 'had', 'one', 'and', \"don't\", 'want', 'one.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'to', 'tell', 'you', 'that', 'i', 'haven', \"'\", 't', 'had', 'one', 'and', 'don', \"'\", 't', 'want', 'one', '.', '[SEP]']\n",
            "Original: ['It', 'is', 'the', 'easiest', 'thing', 'that', 'I', 'have', 'ever', 'done', 'and', 'I', 'tell', 'all', 'my', 'friends', 'that', 'they', 'should', 'do', 'it', 'too.']\n",
            "Tokenized: ['[CLS]', 'it', 'is', 'the', 'easiest', 'thing', 'that', 'i', 'have', 'ever', 'done', 'and', 'i', 'tell', 'all', 'my', 'friends', 'that', 'they', 'should', 'do', 'it', 'too', '.', '[SEP]']\n",
            "Original: ['I', 'sent', 'a', 'customer', 'of', 'mine', 'to', 'you.']\n",
            "Tokenized: ['[CLS]', 'i', 'sent', 'a', 'customer', 'of', 'mine', 'to', 'you', '.', '[SEP]']\n",
            "Original: ['Dr.', 'Ruona,', 'if', 'you', 'read', 'this,', 'thank', 'you', 'for', 'helping', 'me', 'get', 'more', 'healthy.']\n",
            "Tokenized: ['[CLS]', 'dr', '.', 'ru', '##ona', ',', 'if', 'you', 'read', 'this', ',', 'thank', 'you', 'for', 'helping', 'me', 'get', 'more', 'healthy', '.', '[SEP]']\n",
            "Original: ['I', 'feel', 'lighter', 'and', 'feel', 'that', 'I', 'have', 'more', 'possibilities', 'open', 'to', 'me', 'now', 'than', 'I', 'did', 'before.']\n",
            "Tokenized: ['[CLS]', 'i', 'feel', 'lighter', 'and', 'feel', 'that', 'i', 'have', 'more', 'possibilities', 'open', 'to', 'me', 'now', 'than', 'i', 'did', 'before', '.', '[SEP]']\n",
            "Original: ['I', \"can't\", 'thank', 'you', 'enough.']\n",
            "Tokenized: ['[CLS]', 'i', 'can', \"'\", 't', 'thank', 'you', 'enough', '.', '[SEP]']\n",
            "Original: ['no', 'feathers', 'in', 'stock!!!!']\n",
            "Tokenized: ['[CLS]', 'no', 'feathers', 'in', 'stock', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['I', 'was', 'very', 'upset', 'when', 'I', 'went', 'to', 'Mother', 'Plucker,', 'they', 'had', 'NO', 'FEATHERS', 'and', 'the', 'quality', 'is', 'TERRIBLE.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'very', 'upset', 'when', 'i', 'went', 'to', 'mother', 'pl', '##uck', '##er', ',', 'they', 'had', 'no', 'feathers', 'and', 'the', 'quality', 'is', 'terrible', '.', '[SEP]']\n",
            "Original: ['I', 'had', 'to', 'dig', 'in', 'a', 'bag', 'to', 'find', 'one', 'nice', 'feather,', 'what', 'a', 'joke!']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'to', 'dig', 'in', 'a', 'bag', 'to', 'find', 'one', 'nice', 'feather', ',', 'what', 'a', 'joke', '!', '[SEP]']\n",
            "Original: ['Cheapest', 'drinks', 'in', 'Keene!']\n",
            "Tokenized: ['[CLS]', 'cheap', '##est', 'drinks', 'in', 'keen', '##e', '!', '[SEP]']\n",
            "Original: ['I', 'hired', 'this', 'company', 'to', 'unlock', 'my', 'car.']\n",
            "Tokenized: ['[CLS]', 'i', 'hired', 'this', 'company', 'to', 'unlock', 'my', 'car', '.', '[SEP]']\n",
            "Original: ['The', 'price', 'they', 'gave', 'was', 'good', 'so', 'I', 'said', 'hey', 'this', 'seems', 'great.']\n",
            "Tokenized: ['[CLS]', 'the', 'price', 'they', 'gave', 'was', 'good', 'so', 'i', 'said', 'hey', 'this', 'seems', 'great', '.', '[SEP]']\n",
            "Original: ['After', 'they', 'showed', 'up', 'there', 'was', 'a', 'little', 'trouble', 'to', 'get', 'my', 'car', 'unlocked,', 'it', 'took', 'quite', 'a', 'bit', 'of', 'time', 'but', 'the', 'job', 'was', 'well', 'done.']\n",
            "Tokenized: ['[CLS]', 'after', 'they', 'showed', 'up', 'there', 'was', 'a', 'little', 'trouble', 'to', 'get', 'my', 'car', 'unlocked', ',', 'it', 'took', 'quite', 'a', 'bit', 'of', 'time', 'but', 'the', 'job', 'was', 'well', 'done', '.', '[SEP]']\n",
            "Original: ['Rate', 'a', 'church?']\n",
            "Tokenized: ['[CLS]', 'rate', 'a', 'church', '?', '[SEP]']\n",
            "Original: ['Might', 'as', 'well', 'just', 'hotpot', 'the', 'curb', 'and', 'rate', 'the', 'traffic', 'light.']\n",
            "Tokenized: ['[CLS]', 'might', 'as', 'well', 'just', 'hot', '##pot', 'the', 'curb', 'and', 'rate', 'the', 'traffic', 'light', '.', '[SEP]']\n",
            "Original: [\"It's\", 'a', 'bloody', 'church,', 'for', 'chrisssake!']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'a', 'bloody', 'church', ',', 'for', 'chris', '##ssa', '##ke', '!', '[SEP]']\n",
            "Original: ['Big,', 'grey', 'and', 'imposing.']\n",
            "Tokenized: ['[CLS]', 'big', ',', 'grey', 'and', 'imposing', '.', '[SEP]']\n",
            "Original: ['Go', 'there', 'on', 'christian', 'holidays', 'for', 'a', 'bit', 'of', 'churchy', 'grandure.']\n",
            "Tokenized: ['[CLS]', 'go', 'there', 'on', 'christian', 'holidays', 'for', 'a', 'bit', 'of', 'church', '##y', 'grand', '##ure', '.', '[SEP]']\n",
            "Original: ['The', 'pastor', 'at', 'this', 'church', 'is', 'cool,', 'I', 'met', 'him', 'after', 'some', 'holiday', 'service.']\n",
            "Tokenized: ['[CLS]', 'the', 'pastor', 'at', 'this', 'church', 'is', 'cool', ',', 'i', 'met', 'him', 'after', 'some', 'holiday', 'service', '.', '[SEP]']\n",
            "Original: ['He', 'had', 'a', 'robe', 'that', 'was', 'made', 'back', 'in', 'the', \"'60s.\"]\n",
            "Tokenized: ['[CLS]', 'he', 'had', 'a', 'robe', 'that', 'was', 'made', 'back', 'in', 'the', \"'\", '60s', '.', '[SEP]']\n",
            "Original: ['God', 'was', 'pleased', 'with', 'that', 'one!']\n",
            "Tokenized: ['[CLS]', 'god', 'was', 'pleased', 'with', 'that', 'one', '!', '[SEP]']\n",
            "Original: [\"It's\", 'historical', 'for', 'Sf,', 'so', 'when', 'your', 'aunte', 'comes', 'for', 'a', 'visit,', 'take', 'her', 'there.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'historical', 'for', 'sf', ',', 'so', 'when', 'your', 'aunt', '##e', 'comes', 'for', 'a', 'visit', ',', 'take', 'her', 'there', '.', '[SEP]']\n",
            "Original: ['Great', 'for', 'the', 'kiddies', '-', 'they', 'love', 'the', 'labyrinth', \"(don't\", 'forget', 'to', 'tell', \"'em\", 'its', 'really', 'a', \"'pagen'\", 'thing!).']\n",
            "Tokenized: ['[CLS]', 'great', 'for', 'the', 'kidd', '##ies', '-', 'they', 'love', 'the', 'labyrinth', '(', 'don', \"'\", 't', 'forget', 'to', 'tell', \"'\", 'em', 'its', 'really', 'a', \"'\", 'page', '##n', \"'\", 'thing', '!', ')', '.', '[SEP]']\n",
            "Original: ['Stop', 'by', 'at', 'least', 'once', 'or', \"you'll\", 'go', 'to', 'heck!']\n",
            "Tokenized: ['[CLS]', 'stop', 'by', 'at', 'least', 'once', 'or', 'you', \"'\", 'll', 'go', 'to', 'heck', '!', '[SEP]']\n",
            "Original: ['Remember', 'seeing', '\"Stop', 'Making', 'Sense\"', 'at', 'Cinema', '21', 'multiple', 'times!']\n",
            "Tokenized: ['[CLS]', 'remember', 'seeing', '\"', 'stop', 'making', 'sense', '\"', 'at', 'cinema', '21', 'multiple', 'times', '!', '[SEP]']\n",
            "Original: ['YAY,', 'great', 'theater!!!']\n",
            "Tokenized: ['[CLS]', 'ya', '##y', ',', 'great', 'theater', '!', '!', '!', '[SEP]']\n",
            "Original: ['they', 'recovered', 'the', 'pics', 'geeksquad', 'deleted.']\n",
            "Tokenized: ['[CLS]', 'they', 'recovered', 'the', 'pic', '##s', 'geek', '##s', '##qua', '##d', 'deleted', '.', '[SEP]']\n",
            "Original: ['many', 'thanks']\n",
            "Tokenized: ['[CLS]', 'many', 'thanks', '[SEP]']\n",
            "Original: ['Love', 'Hop', 'City']\n",
            "Tokenized: ['[CLS]', 'love', 'hop', 'city', '[SEP]']\n",
            "Original: ['This', 'place', 'is', 'great!']\n",
            "Tokenized: ['[CLS]', 'this', 'place', 'is', 'great', '!', '[SEP]']\n",
            "Original: ['Craig', 'and', 'Nate', 'are', 'wonderful.']\n",
            "Tokenized: ['[CLS]', 'craig', 'and', 'nate', 'are', 'wonderful', '.', '[SEP]']\n",
            "Original: ['I', 'know', 'now', 'where', 'to', 'get', 'all', 'of', 'my', 'wine', 'and', 'beer.']\n",
            "Tokenized: ['[CLS]', 'i', 'know', 'now', 'where', 'to', 'get', 'all', 'of', 'my', 'wine', 'and', 'beer', '.', '[SEP]']\n",
            "Original: ['No', 'need', 'to', 'go', 'to', 'a', 'grocer', 'again.']\n",
            "Tokenized: ['[CLS]', 'no', 'need', 'to', 'go', 'to', 'a', 'gr', '##oc', '##er', 'again', '.', '[SEP]']\n",
            "Original: ['Course', 'has', 'come', 'a', 'long', 'way!!']\n",
            "Tokenized: ['[CLS]', 'course', 'has', 'come', 'a', 'long', 'way', '!', '!', '[SEP]']\n",
            "Original: [\"HCC's\", 'new', 'nine', 'was', 'a', 'little', 'shaky', 'at', 'first,', 'but', 'the', 'NEW', 'grounds', 'superintendant', 'has', 'done', 'wonders', 'for', 'the', 'course!!']\n",
            "Tokenized: ['[CLS]', 'hc', '##c', \"'\", 's', 'new', 'nine', 'was', 'a', 'little', 'shaky', 'at', 'first', ',', 'but', 'the', 'new', 'grounds', 'super', '##int', '##end', '##ant', 'has', 'done', 'wonders', 'for', 'the', 'course', '!', '!', '[SEP]']\n",
            "Original: ['The', 'comment', 'below', 'definitely', 'needs', 'to', 'be', 'retracted!']\n",
            "Tokenized: ['[CLS]', 'the', 'comment', 'below', 'definitely', 'needs', 'to', 'be', 'retracted', '!', '[SEP]']\n",
            "Original: ['Come', 'back', 'and', 'give', 'HCC', 'a', 'second', 'chance', 'at', 'least!']\n",
            "Tokenized: ['[CLS]', 'come', 'back', 'and', 'give', 'hc', '##c', 'a', 'second', 'chance', 'at', 'least', '!', '[SEP]']\n",
            "Original: ['It', 'is', 'a', 'great', 'course', 'for', 'local', 'golfers', 'to', 'be', 'proud', 'of', 'and', 'all', 'the', 'comments', 'in', '2008', 'have', 'been', 'very', 'positive!!']\n",
            "Tokenized: ['[CLS]', 'it', 'is', 'a', 'great', 'course', 'for', 'local', 'golfer', '##s', 'to', 'be', 'proud', 'of', 'and', 'all', 'the', 'comments', 'in', '2008', 'have', 'been', 'very', 'positive', '!', '!', '[SEP]']\n",
            "Original: [\"Don't\", 'waste', 'your', 'money', 'on', 'the', 'jukebox']\n",
            "Tokenized: ['[CLS]', 'don', \"'\", 't', 'waste', 'your', 'money', 'on', 'the', 'ju', '##ke', '##box', '[SEP]']\n",
            "Original: ['The', 'bartender', 'is', 'a', 'douchebag', 'and', 'he', 'has', 'a', 'little', 'console', 'behind', 'the', 'bar', 'where', 'he', 'can', 'delete', 'songs', 'he', \"doesn't\", 'like,', 'and', 'you', 'end', 'up', 'paying', 'for', 'it.']\n",
            "Tokenized: ['[CLS]', 'the', 'bartender', 'is', 'a', 'do', '##uche', '##bag', 'and', 'he', 'has', 'a', 'little', 'console', 'behind', 'the', 'bar', 'where', 'he', 'can', 'del', '##ete', 'songs', 'he', 'doesn', \"'\", 't', 'like', ',', 'and', 'you', 'end', 'up', 'paying', 'for', 'it', '.', '[SEP]']\n",
            "Original: ['Not', 'enough', 'seating.']\n",
            "Tokenized: ['[CLS]', 'not', 'enough', 'seating', '.', '[SEP]']\n",
            "Original: ['Lovley', 'food', 'and', 'fab', 'chips']\n",
            "Tokenized: ['[CLS]', 'lo', '##v', '##ley', 'food', 'and', 'fa', '##b', 'chips', '[SEP]']\n",
            "Original: ['I', \"wouldn't\", 'send', 'my', 'dogs', 'there.']\n",
            "Tokenized: ['[CLS]', 'i', 'wouldn', \"'\", 't', 'send', 'my', 'dogs', 'there', '.', '[SEP]']\n",
            "Original: ['I', 'had', 'a', 'conversation', 'with', 'the', 'woman', 'running', 'this', 'place', 'in', 'April', '2010.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'a', 'conversation', 'with', 'the', 'woman', 'running', 'this', 'place', 'in', 'april', '2010', '.', '[SEP]']\n",
            "Original: ['She', 'basically', 'said', 'if', 'the', 'children', 'getting', 'off', 'the', 'bus', \"aren't\", 'paying', 'to', 'enter', 'her', 'building', 'she', 'was', 'going', 'to', 'let', 'them', 'wander', 'around', 'the', 'streets.']\n",
            "Tokenized: ['[CLS]', 'she', 'basically', 'said', 'if', 'the', 'children', 'getting', 'off', 'the', 'bus', 'aren', \"'\", 't', 'paying', 'to', 'enter', 'her', 'building', 'she', 'was', 'going', 'to', 'let', 'them', 'wander', 'around', 'the', 'streets', '.', '[SEP]']\n",
            "Original: ['Wow,', 'really?']\n",
            "Tokenized: ['[CLS]', 'wow', ',', 'really', '?', '[SEP]']\n",
            "Original: ['With', 'all', 'the', 'child', 'predators', 'out', 'there,', 'a', 'busy', 'road,', 'cars', 'speeding', 'by......', 'and', 'you', 'are', 'going', 'to', 'let', 'some', '4/5', 'year', 'olds', 'wonder', 'around', 'cause', \"you're\", 'money', 'hungry?']\n",
            "Tokenized: ['[CLS]', 'with', 'all', 'the', 'child', 'predators', 'out', 'there', ',', 'a', 'busy', 'road', ',', 'cars', 'speeding', 'by', '.', '.', '.', '.', '.', '.', 'and', 'you', 'are', 'going', 'to', 'let', 'some', '4', '/', '5', 'year', 'olds', 'wonder', 'around', 'cause', 'you', \"'\", 're', 'money', 'hungry', '?', '[SEP]']\n",
            "Original: ['REAL', 'CHRISTIAN', 'OF', 'YOU!!!!!!!!!!!!!!!!!!']\n",
            "Tokenized: ['[CLS]', 'real', 'christian', 'of', 'you', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['Whether', 'they', 'pay', 'me', 'or', 'not,', 'if', 'their', 'parents', 'get', 'into', 'an', 'accident,', 'stuck', 'in', 'traffic,', 'etc.', 'THE', 'LAST', 'THING', 'I', 'WOULD', 'DO', 'IS', 'LET', 'A', 'CHILD', 'GET', 'RAPED', 'BECAUSE', 'I', \"WASN'T\", 'PAID.']\n",
            "Tokenized: ['[CLS]', 'whether', 'they', 'pay', 'me', 'or', 'not', ',', 'if', 'their', 'parents', 'get', 'into', 'an', 'accident', ',', 'stuck', 'in', 'traffic', ',', 'etc', '.', 'the', 'last', 'thing', 'i', 'would', 'do', 'is', 'let', 'a', 'child', 'get', 'raped', 'because', 'i', 'wasn', \"'\", 't', 'paid', '.', '[SEP]']\n",
            "Original: ['Good', 'local', 'bikeshop']\n",
            "Tokenized: ['[CLS]', 'good', 'local', 'bikes', '##hop', '[SEP]']\n",
            "Original: ['Good', 'local', 'bike', 'shop', '.']\n",
            "Tokenized: ['[CLS]', 'good', 'local', 'bike', 'shop', '.', '[SEP]']\n",
            "Original: ['Jason', 'and', 'the', 'boys', 'can', 'do', 'about', 'anything', 'you', 'need.']\n",
            "Tokenized: ['[CLS]', 'jason', 'and', 'the', 'boys', 'can', 'do', 'about', 'anything', 'you', 'need', '.', '[SEP]']\n",
            "Original: ['The', 'shop', 'is', 'located', 'just', 'off', 'the', 'river', 'road', '.']\n",
            "Tokenized: ['[CLS]', 'the', 'shop', 'is', 'located', 'just', 'off', 'the', 'river', 'road', '.', '[SEP]']\n",
            "Original: ['I', 'phoned', 'this', 'company', 'for', 'advice', 'on', 'our', 'office', 'refurb', 'and', 'although', 'we', 'did', 'not', 'use', 'them', 'in', 'the', 'end(as', 'our', 'building', 'contractor', 'carried', 'out', 'the', 'electrical', 'work),', 'they', 'provided', 'me', 'with', 'plenty', 'of', 'useful', 'information', 'over', 'an', 'hour', 'phone', 'call', 'and', 'subsequently', 'we', 'are', 'now', 'using', 'PJC', 'as', 'our', 'electrical', 'maintenance', 'contractor.']\n",
            "Tokenized: ['[CLS]', 'i', 'phone', '##d', 'this', 'company', 'for', 'advice', 'on', 'our', 'office', 'ref', '##ur', '##b', 'and', 'although', 'we', 'did', 'not', 'use', 'them', 'in', 'the', 'end', '(', 'as', 'our', 'building', 'contractor', 'carried', 'out', 'the', 'electrical', 'work', ')', ',', 'they', 'provided', 'me', 'with', 'plenty', 'of', 'useful', 'information', 'over', 'an', 'hour', 'phone', 'call', 'and', 'subsequently', 'we', 'are', 'now', 'using', 'p', '##j', '##c', 'as', 'our', 'electrical', 'maintenance', 'contractor', '.', '[SEP]']\n",
            "Original: ['Thoroughly', 'recommended']\n",
            "Tokenized: ['[CLS]', 'thoroughly', 'recommended', '[SEP]']\n",
            "Original: ['recommended']\n",
            "Tokenized: ['[CLS]', 'recommended', '[SEP]']\n",
            "Original: ['Excellent', 'location.']\n",
            "Tokenized: ['[CLS]', 'excellent', 'location', '.', '[SEP]']\n",
            "Original: ['Good', 'sports', 'bar.']\n",
            "Tokenized: ['[CLS]', 'good', 'sports', 'bar', '.', '[SEP]']\n",
            "Original: ['Hyatt', 'web', 'site', 'improved.']\n",
            "Tokenized: ['[CLS]', 'h', '##yat', '##t', 'web', 'site', 'improved', '.', '[SEP]']\n",
            "Original: ['Accurate', 'check-out.']\n",
            "Tokenized: ['[CLS]', 'accurate', 'check', '-', 'out', '.', '[SEP]']\n",
            "Original: ['Rooms', 'clean.']\n",
            "Tokenized: ['[CLS]', 'rooms', 'clean', '.', '[SEP]']\n",
            "Original: ['Lifts', 'quick,', 'clean,', 'accurate,', 'and', 'correctly', 'sized.']\n",
            "Tokenized: ['[CLS]', 'lifts', 'quick', ',', 'clean', ',', 'accurate', ',', 'and', 'correctly', 'sized', '.', '[SEP]']\n",
            "Original: ['Choose', 'this', 'hotel', 'over', 'the', 'Hilton', '(which', 'is', 'on', 'the', 'next', 'block).']\n",
            "Tokenized: ['[CLS]', 'choose', 'this', 'hotel', 'over', 'the', 'hilton', '(', 'which', 'is', 'on', 'the', 'next', 'block', ')', '.', '[SEP]']\n",
            "Original: [\"Orr's\", 'a', 'nightmare!']\n",
            "Tokenized: ['[CLS]', 'orr', \"'\", 's', 'a', 'nightmare', '!', '[SEP]']\n",
            "Original: [\"DON'T\", 'GO!']\n",
            "Tokenized: ['[CLS]', 'don', \"'\", 't', 'go', '!', '[SEP]']\n",
            "Original: ['My', 'boyfriend', 'and', 'I', 'were', 'woken', 'up', 'in', 'the', 'middle', 'of', 'the', 'night', 'at', 'our', 'campsite', 'by', 'drunken', 'kids', \"who'd\", 'arrived', 'around', '1:00', 'am', 'and', 'were', 'drinking', 'at', 'the', 'springs.']\n",
            "Tokenized: ['[CLS]', 'my', 'boyfriend', 'and', 'i', 'were', 'woken', 'up', 'in', 'the', 'middle', 'of', 'the', 'night', 'at', 'our', 'camps', '##ite', 'by', 'drunken', 'kids', 'who', \"'\", 'd', 'arrived', 'around', '1', ':', '00', 'am', 'and', 'were', 'drinking', 'at', 'the', 'springs', '.', '[SEP]']\n",
            "Original: ['When', 'we', 'complained', 'to', 'the', 'management', '(as', 'we', 'were', 'instructed', 'to', 'do', 'if', 'there', 'were', 'any', 'problems),', 'nothing', 'happened!']\n",
            "Tokenized: ['[CLS]', 'when', 'we', 'complained', 'to', 'the', 'management', '(', 'as', 'we', 'were', 'instructed', 'to', 'do', 'if', 'there', 'were', 'any', 'problems', ')', ',', 'nothing', 'happened', '!', '[SEP]']\n",
            "Original: ['Then,', 'the', 'next', 'morning,', 'when', 'we,', 'among', 'many', 'other', 'irate', 'guests,', 'asked', 'to', 'speak', 'to', 'the', 'owner,', 'he', 'refused', 'to', 'do', 'so.']\n",
            "Tokenized: ['[CLS]', 'then', ',', 'the', 'next', 'morning', ',', 'when', 'we', ',', 'among', 'many', 'other', 'ira', '##te', 'guests', ',', 'asked', 'to', 'speak', 'to', 'the', 'owner', ',', 'he', 'refused', 'to', 'do', 'so', '.', '[SEP]']\n",
            "Original: ['Not', 'only', 'that,', 'but', 'he', 'told', 'us,', 'via', 'his', 'manager,', 'that', 'it', 'was', 'our', 'responsibility', 'to', 'get', 'up', 'a', 'second', 'time', 'in', 'the', 'middle', 'of', 'the', 'night', 'and', 'call', 'the', 'night', 'staff', 'if', 'the', 'problem', 'continued,', 'even', 'though', 'his', 'manager', 'did', 'nothing', 'the', 'first', 'time', 'to', 'stop', 'the', 'kids.']\n",
            "Tokenized: ['[CLS]', 'not', 'only', 'that', ',', 'but', 'he', 'told', 'us', ',', 'via', 'his', 'manager', ',', 'that', 'it', 'was', 'our', 'responsibility', 'to', 'get', 'up', 'a', 'second', 'time', 'in', 'the', 'middle', 'of', 'the', 'night', 'and', 'call', 'the', 'night', 'staff', 'if', 'the', 'problem', 'continued', ',', 'even', 'though', 'his', 'manager', 'did', 'nothing', 'the', 'first', 'time', 'to', 'stop', 'the', 'kids', '.', '[SEP]']\n",
            "Original: ['Orr', 'is', 'not', 'relaxing,', \"it's\", 'not', 'a', 'safe', 'space,', 'and', 'the', 'owner', 'is', 'awful.']\n",
            "Tokenized: ['[CLS]', 'orr', 'is', 'not', 'relaxing', ',', 'it', \"'\", 's', 'not', 'a', 'safe', 'space', ',', 'and', 'the', 'owner', 'is', 'awful', '.', '[SEP]']\n",
            "Original: ['To', 'add', 'insult', 'to', 'injury,', 'he', 'refused', 'to', 'refund', 'our', 'money.']\n",
            "Tokenized: ['[CLS]', 'to', 'add', 'insult', 'to', 'injury', ',', 'he', 'refused', 'to', 'ref', '##und', 'our', 'money', '.', '[SEP]']\n",
            "Original: [\"Don't\", 'go!']\n",
            "Tokenized: ['[CLS]', 'don', \"'\", 't', 'go', '!', '[SEP]']\n",
            "Original: ['Nice', 'teachers', 'good', 'school']\n",
            "Tokenized: ['[CLS]', 'nice', 'teachers', 'good', 'school', '[SEP]']\n",
            "Original: ['They', 'are', 'very', 'good', 'teachers', 'and', 'nice', 'people', 'to', 'meet', 'here.']\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'very', 'good', 'teachers', 'and', 'nice', 'people', 'to', 'meet', 'here', '.', '[SEP]']\n",
            "Original: ['I', 'enjoyed', 'very', 'much', 'to', 'study', 'here.']\n",
            "Tokenized: ['[CLS]', 'i', 'enjoyed', 'very', 'much', 'to', 'study', 'here', '.', '[SEP]']\n",
            "Original: ['A', 'good', 'place', 'to', 'improve', 'your', 'English']\n",
            "Tokenized: ['[CLS]', 'a', 'good', 'place', 'to', 'improve', 'your', 'english', '[SEP]']\n",
            "Original: ['Believe', 'me.']\n",
            "Tokenized: ['[CLS]', 'believe', 'me', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'THE', 'premier', 'university', 'in', 'Virginia.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'the', 'premier', 'university', 'in', 'virginia', '.', '[SEP]']\n",
            "Original: ['It', 'is', 'also', 'the', 'largest.']\n",
            "Tokenized: ['[CLS]', 'it', 'is', 'also', 'the', 'largest', '.', '[SEP]']\n",
            "Original: ['With', 'a', 'Pizza', 'Hut,', 'IHOP,', '3', 'Starbucks,', 'Chilis,', 'Panera,', 'and', 'Chipotle', 'on', 'campus', 'ALONE,', 'VCU', 'has', 'some', 'of', 'the', 'best', 'eating', 'options', 'for', 'students.']\n",
            "Tokenized: ['[CLS]', 'with', 'a', 'pizza', 'hut', ',', 'i', '##hop', ',', '3', 'starbucks', ',', 'chili', '##s', ',', 'pan', '##era', ',', 'and', 'chip', '##ot', '##le', 'on', 'campus', 'alone', ',', 'vc', '##u', 'has', 'some', 'of', 'the', 'best', 'eating', 'options', 'for', 'students', '.', '[SEP]']\n",
            "Original: ['VCU', 'has', 'the', '#1', 'art', 'school', 'in', 'America,', 'and', 'EXCELS', 'in', 'healthcare', 'and', 'medical', 'schooling.']\n",
            "Tokenized: ['[CLS]', 'vc', '##u', 'has', 'the', '#', '1', 'art', 'school', 'in', 'america', ',', 'and', 'excel', '##s', 'in', 'healthcare', 'and', 'medical', 'schooling', '.', '[SEP]']\n",
            "Original: ['The', 'Rams,', 'the', 'VCU', 'sports', 'team,', 'also', 'made', 'the', 'NCAA', 'Final', '4', 'this', 'year,', 'too!']\n",
            "Tokenized: ['[CLS]', 'the', 'rams', ',', 'the', 'vc', '##u', 'sports', 'team', ',', 'also', 'made', 'the', 'ncaa', 'final', '4', 'this', 'year', ',', 'too', '!', '[SEP]']\n",
            "Original: ['VCU', 'also', 'offers', 'high-rise', 'living', 'for', 'students', 'and', 'professors,', 'and', 'the', 'historic', 'yet', 'fixed-up', 'houses', 'in', 'the', 'Fan', 'are', 'also', 'available', 'to', 'students,', 'professors,', 'or', 'even', 'people', 'wanting', 'to', 'live', 'in', 'a', 'safe,', 'viable', 'community.']\n",
            "Tokenized: ['[CLS]', 'vc', '##u', 'also', 'offers', 'high', '-', 'rise', 'living', 'for', 'students', 'and', 'professors', ',', 'and', 'the', 'historic', 'yet', 'fixed', '-', 'up', 'houses', 'in', 'the', 'fan', 'are', 'also', 'available', 'to', 'students', ',', 'professors', ',', 'or', 'even', 'people', 'wanting', 'to', 'live', 'in', 'a', 'safe', ',', 'viable', 'community', '.', '[SEP]']\n",
            "Original: ['VCU', 'is', 'also', 'minutes', 'from', 'Downtown', 'Richmond,', 'Canal', 'Walk,', 'Carytown,', 'Stony', 'Point,', 'Short', 'Pump,', 'and', 'the', 'VCU', 'Medical', 'Center.']\n",
            "Tokenized: ['[CLS]', 'vc', '##u', 'is', 'also', 'minutes', 'from', 'downtown', 'richmond', ',', 'canal', 'walk', ',', 'cary', '##town', ',', 'stony', 'point', ',', 'short', 'pump', ',', 'and', 'the', 'vc', '##u', 'medical', 'center', '.', '[SEP]']\n",
            "Original: ['It', 'is', 'the', 'best', 'university', 'in', 'Virginia', 'and', 'continuously', 'receives', 'rave', 'reviews', 'every', 'year.']\n",
            "Tokenized: ['[CLS]', 'it', 'is', 'the', 'best', 'university', 'in', 'virginia', 'and', 'continuously', 'receives', 'rave', 'reviews', 'every', 'year', '.', '[SEP]']\n",
            "Original: ['The', 'U', 'of', 'R', 'is', 'also', 'recommended,', 'too!']\n",
            "Tokenized: ['[CLS]', 'the', 'u', 'of', 'r', 'is', 'also', 'recommended', ',', 'too', '!', '[SEP]']\n",
            "Original: ['Great', 'Service', 'Great', 'People']\n",
            "Tokenized: ['[CLS]', 'great', 'service', 'great', 'people', '[SEP]']\n",
            "Original: ['Bisconti', 'wanted', 'over', '$300', 'to', 'fix', 'my', 'laptop', 'and', 'these', 'guys', 'fixed', 'it', 'for', '$90!']\n",
            "Tokenized: ['[CLS]', 'bis', '##con', '##ti', 'wanted', 'over', '$', '300', 'to', 'fix', 'my', 'laptop', 'and', 'these', 'guys', 'fixed', 'it', 'for', '$', '90', '!', '[SEP]']\n",
            "Original: ['Call', 'them', 'today!']\n",
            "Tokenized: ['[CLS]', 'call', 'them', 'today', '!', '[SEP]']\n",
            "Original: ['Had', 'it', 'fixed', 'within', 'a', 'few', 'days', '(had', 'to', 'order', 'a', 'part).']\n",
            "Tokenized: ['[CLS]', 'had', 'it', 'fixed', 'within', 'a', 'few', 'days', '(', 'had', 'to', 'order', 'a', 'part', ')', '.', '[SEP]']\n",
            "Original: ['An', 'Hour', 'Of', 'Prego', 'Bliss!']\n",
            "Tokenized: ['[CLS]', 'an', 'hour', 'of', 'pre', '##go', 'bliss', '!', '[SEP]']\n",
            "Original: ['I', 'schedule', 'my', 'weekly', 'appointment', 'here', 'just', 'to', 'get', 'a', 'chance', 'to', 'lie', 'comfortably', 'on', 'my', 'tummy.']\n",
            "Tokenized: ['[CLS]', 'i', 'schedule', 'my', 'weekly', 'appointment', 'here', 'just', 'to', 'get', 'a', 'chance', 'to', 'lie', 'comfortably', 'on', 'my', 'tu', '##mmy', '.', '[SEP]']\n",
            "Original: ['And', 'the', 'massages', 'are', 'heavenly!']\n",
            "Tokenized: ['[CLS]', 'and', 'the', 'massage', '##s', 'are', 'heavenly', '!', '[SEP]']\n",
            "Original: ['Very', 'friendly', 'place.']\n",
            "Tokenized: ['[CLS]', 'very', 'friendly', 'place', '.', '[SEP]']\n",
            "Original: ['Best', 'Chineese', 'food', 'in', 'the', 'area']\n",
            "Tokenized: ['[CLS]', 'best', 'chin', '##ees', '##e', 'food', 'in', 'the', 'area', '[SEP]']\n",
            "Original: ['The', 'food', 'here', 'is', 'fresh', 'and', 'hot', 'out', 'of', 'the', 'Wok.']\n",
            "Tokenized: ['[CLS]', 'the', 'food', 'here', 'is', 'fresh', 'and', 'hot', 'out', 'of', 'the', 'wo', '##k', '.', '[SEP]']\n",
            "Original: ['The', 'food', 'is', 'cooked', 'fast', 'by', 'the', 'two', 'chefs', 'on', 'duty.']\n",
            "Tokenized: ['[CLS]', 'the', 'food', 'is', 'cooked', 'fast', 'by', 'the', 'two', 'chefs', 'on', 'duty', '.', '[SEP]']\n",
            "Original: ['The', 'lunch', 'specials', 'are', 'more', 'food', 'than', 'most', 'people', 'can', 'eat', 'for', 'about', '$6.']\n",
            "Tokenized: ['[CLS]', 'the', 'lunch', 'specials', 'are', 'more', 'food', 'than', 'most', 'people', 'can', 'eat', 'for', 'about', '$', '6', '.', '[SEP]']\n",
            "Original: ['It', 'is', 'busy', 'every', 'day', 'at', 'lunch', 'for', 'a', 'reason,', 'the', 'service', 'is', 'fast', 'and', 'the', 'food', 'is', 'great.']\n",
            "Tokenized: ['[CLS]', 'it', 'is', 'busy', 'every', 'day', 'at', 'lunch', 'for', 'a', 'reason', ',', 'the', 'service', 'is', 'fast', 'and', 'the', 'food', 'is', 'great', '.', '[SEP]']\n",
            "Original: ['Thanks', 'For', 'A', 'Great', 'Job']\n",
            "Tokenized: ['[CLS]', 'thanks', 'for', 'a', 'great', 'job', '[SEP]']\n",
            "Original: ['Thanks', 'For', 'The', 'Prompt', 'Service', 'And', 'Great', 'Job', 'You', 'And', 'Your', 'Boys', 'Have', 'Done', 'On', 'Our', 'New', 'Solar', 'System,', 'The', 'Panels', 'On', 'The', 'Roof', 'Look', 'Great', 'And', 'The', 'Power', 'We', 'Are', 'Putting', 'Back', 'In', 'To', 'The', 'Grid', 'Is', 'Great,', 'Great', 'Job', 'Thanks']\n",
            "Tokenized: ['[CLS]', 'thanks', 'for', 'the', 'prompt', 'service', 'and', 'great', 'job', 'you', 'and', 'your', 'boys', 'have', 'done', 'on', 'our', 'new', 'solar', 'system', ',', 'the', 'panels', 'on', 'the', 'roof', 'look', 'great', 'and', 'the', 'power', 'we', 'are', 'putting', 'back', 'in', 'to', 'the', 'grid', 'is', 'great', ',', 'great', 'job', 'thanks', '[SEP]']\n",
            "Original: ['The', 'finest', 'German', 'bedding', 'and', 'linens', 'store.']\n",
            "Tokenized: ['[CLS]', 'the', 'finest', 'german', 'bed', '##ding', 'and', 'linen', '##s', 'store', '.', '[SEP]']\n",
            "Original: ['Quality', 'and', 'service', 'come', 'first', 'here.']\n",
            "Tokenized: ['[CLS]', 'quality', 'and', 'service', 'come', 'first', 'here', '.', '[SEP]']\n",
            "Original: ['So', 'very', 'delicious!']\n",
            "Tokenized: ['[CLS]', 'so', 'very', 'delicious', '!', '[SEP]']\n",
            "Original: ['Excellent!']\n",
            "Tokenized: ['[CLS]', 'excellent', '!', '[SEP]']\n",
            "Original: ['November', '7,', '2010', 'First', 'time', 'eating', 'at', 'Caffe', 'Bella', 'Italia', 'and', 'it', 'was', 'a', 'wonderful', 'experience.']\n",
            "Tokenized: ['[CLS]', 'november', '7', ',', '2010', 'first', 'time', 'eating', 'at', 'caf', '##fe', 'bella', 'italia', 'and', 'it', 'was', 'a', 'wonderful', 'experience', '.', '[SEP]']\n",
            "Original: ['From', 'the', 'delectable', 'Antipasto', 'Misto', 'to', 'the', 'Spaghetti', 'alla', 'Barese', 'and', 'the', 'Parmigiana', 'and', 'ending', 'with', 'gelato,', 'all', 'was', 'mouth', 'watering.']\n",
            "Tokenized: ['[CLS]', 'from', 'the', 'del', '##ect', '##able', 'anti', '##pas', '##to', 'mist', '##o', 'to', 'the', 'spaghetti', 'alla', 'bare', '##se', 'and', 'the', 'par', '##mi', '##gia', '##na', 'and', 'ending', 'with', 'gel', '##ato', ',', 'all', 'was', 'mouth', 'watering', '.', '[SEP]']\n",
            "Original: ['Too', 'bad', 'they', 'were', 'out', 'of', 'the', 'Chocolate', 'Lava', 'Cake.']\n",
            "Tokenized: ['[CLS]', 'too', 'bad', 'they', 'were', 'out', 'of', 'the', 'chocolate', 'lava', 'cake', '.', '[SEP]']\n",
            "Original: ['Maybe', 'next', 'time', 'they', 'will', 'have', 'it.']\n",
            "Tokenized: ['[CLS]', 'maybe', 'next', 'time', 'they', 'will', 'have', 'it', '.', '[SEP]']\n",
            "Original: ['Service', 'was', 'excellent!']\n",
            "Tokenized: ['[CLS]', 'service', 'was', 'excellent', '!', '[SEP]']\n",
            "Original: ['Sandy']\n",
            "Tokenized: ['[CLS]', 'sandy', '[SEP]']\n",
            "Original: ['Great', 'Doc']\n",
            "Tokenized: ['[CLS]', 'great', 'doc', '[SEP]']\n",
            "Original: ['Dr', 'Greenwalt', 'fixed', 'my', 'neck', 'from', 'a', 'snowboard', 'injury', 'and', 'was', 'way', 'more', 'effective', 'that', 'a', 'regular', 'doctor.']\n",
            "Tokenized: ['[CLS]', 'dr', 'green', '##wal', '##t', 'fixed', 'my', 'neck', 'from', 'a', 'snow', '##board', 'injury', 'and', 'was', 'way', 'more', 'effective', 'that', 'a', 'regular', 'doctor', '.', '[SEP]']\n",
            "Original: ['He', 'didnt', 'prescribe', 'pain', 'meds', 'or', 'other', 'drugs,', 'he', 'used', 'his', 'bodytalk', 'method', 'which', 'is', 'unusual', 'but', 'the', 'results', 'are', 'undeniable.']\n",
            "Tokenized: ['[CLS]', 'he', 'didn', '##t', 'pre', '##scribe', 'pain', 'med', '##s', 'or', 'other', 'drugs', ',', 'he', 'used', 'his', 'body', '##talk', 'method', 'which', 'is', 'unusual', 'but', 'the', 'results', 'are', 'und', '##enia', '##ble', '.', '[SEP]']\n",
            "Original: ['My', 'neck', 'is', 'fixed!.']\n",
            "Tokenized: ['[CLS]', 'my', 'neck', 'is', 'fixed', '!', '.', '[SEP]']\n",
            "Original: ['He', 'knows', 'what', 'hes', 'doing.']\n",
            "Tokenized: ['[CLS]', 'he', 'knows', 'what', 'he', '##s', 'doing', '.', '[SEP]']\n",
            "Original: ['I', 'won', 'a', 'golf', 'lesson', 'certificate', 'with', 'Adz', 'through', 'a', 'charity', 'auction.']\n",
            "Tokenized: ['[CLS]', 'i', 'won', 'a', 'golf', 'lesson', 'certificate', 'with', 'ad', '##z', 'through', 'a', 'charity', 'auction', '.', '[SEP]']\n",
            "Original: ['The', 'lesson', 'was', 'donated', 'by', 'the', 'teacher', 'Adz.']\n",
            "Tokenized: ['[CLS]', 'the', 'lesson', 'was', 'donated', 'by', 'the', 'teacher', 'ad', '##z', '.', '[SEP]']\n",
            "Original: ['So', 'i', 'booked', 'the', 'lesson', 'and', 'loved', 'it.']\n",
            "Tokenized: ['[CLS]', 'so', 'i', 'booked', 'the', 'lesson', 'and', 'loved', 'it', '.', '[SEP]']\n",
            "Original: ['Great', 'teacher.']\n",
            "Tokenized: ['[CLS]', 'great', 'teacher', '.', '[SEP]']\n",
            "Original: ['tricky', 'short', 'guy']\n",
            "Tokenized: ['[CLS]', 'tricky', 'short', 'guy', '[SEP]']\n",
            "Original: ['The', 'new', 'management', 'is', 'tricky', 'and', 'talk', 'you', 'into', 'getting', 'video', 'rental', 'agreement.']\n",
            "Tokenized: ['[CLS]', 'the', 'new', 'management', 'is', 'tricky', 'and', 'talk', 'you', 'into', 'getting', 'video', 'rental', 'agreement', '.', '[SEP]']\n",
            "Original: ['To', 'my', 'surprise', '$20', 'deposit....', 'New', 'movies', 'not', 'on', 'shelf..under', 'the', 'counter', 'for', 'Telugu', 'Speaking', 'people', 'only...', 'or', 'people', 'who', 'spend', '$30', 'or', 'more', 'groceries..']\n",
            "Tokenized: ['[CLS]', 'to', 'my', 'surprise', '$', '20', 'deposit', '.', '.', '.', '.', 'new', 'movies', 'not', 'on', 'shelf', '.', '.', 'under', 'the', 'counter', 'for', 'telugu', 'speaking', 'people', 'only', '.', '.', '.', 'or', 'people', 'who', 'spend', '$', '30', 'or', 'more', 'groceries', '.', '.', '[SEP]']\n",
            "Original: ['That', 'did', 'it', 'for', 'me..no', 'more', \"Raina's.\"]\n",
            "Tokenized: ['[CLS]', 'that', 'did', 'it', 'for', 'me', '.', '.', 'no', 'more', 'rain', '##a', \"'\", 's', '.', '[SEP]']\n",
            "Original: ['Besides', 'parking', 'is', 'a', 'pain..cramped', 'and', 'un-ruly', 'with', 'Kumon', 'Parents', 'next', 'door....gives', 'me', 'heebee', 'gee', \"bees'\"]\n",
            "Tokenized: ['[CLS]', 'besides', 'parking', 'is', 'a', 'pain', '.', '.', 'cramped', 'and', 'un', '-', 'ru', '##ly', 'with', 'ku', '##mon', 'parents', 'next', 'door', '.', '.', '.', '.', 'gives', 'me', 'hee', '##bee', 'gee', 'bees', \"'\", '[SEP]']\n",
            "Original: ['Wonderful', 'Atmosphere']\n",
            "Tokenized: ['[CLS]', 'wonderful', 'atmosphere', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'going', 'there', 'since', 'I', 'was', 'a', 'little', 'girl', 'and', 'love', 'the', 'friendly', 'and', 'relaxing', 'atmosphere.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'going', 'there', 'since', 'i', 'was', 'a', 'little', 'girl', 'and', 'love', 'the', 'friendly', 'and', 'relaxing', 'atmosphere', '.', '[SEP]']\n",
            "Original: ['Dr.', 'Stiefvater', 'has', 'always', 'been', 'very', 'professional', 'and', 'helpful.']\n",
            "Tokenized: ['[CLS]', 'dr', '.', 'st', '##ie', '##f', '##vate', '##r', 'has', 'always', 'been', 'very', 'professional', 'and', 'helpful', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'recommend', 'Bayside', 'Chiropractic', 'to', 'anyone', 'who', 'is', 'in', 'need', 'of', 'a', 'regular', 'adjustment', 'or', 'is', 'suffering', 'from', 'a', 'chronic', 'condition.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'recommend', 'bays', '##ide', 'chi', '##rop', '##rac', '##tic', 'to', 'anyone', 'who', 'is', 'in', 'need', 'of', 'a', 'regular', 'adjustment', 'or', 'is', 'suffering', 'from', 'a', 'chronic', 'condition', '.', '[SEP]']\n",
            "Original: ['Was', 'there', 'this', 'past', 'weekend', 'and', 'the', 'guy', 'behind', 'the', 'counter', 'yelled', 'at', 'me', 'and', 'my', 'son', 'because', 'we', 'believed', 'he', 'left', 'the', 'grandma', 'slice', 'in', 'the', 'oven', 'a', 'little', 'too', 'long', 'and', 'the', 'cheese', 'got', 'all', 'dried', 'out', 'and', 'the', 'slice', 'tasted', 'more', 'like', 'a', 'cracker.']\n",
            "Tokenized: ['[CLS]', 'was', 'there', 'this', 'past', 'weekend', 'and', 'the', 'guy', 'behind', 'the', 'counter', 'yelled', 'at', 'me', 'and', 'my', 'son', 'because', 'we', 'believed', 'he', 'left', 'the', 'grandma', 'slice', 'in', 'the', 'oven', 'a', 'little', 'too', 'long', 'and', 'the', 'cheese', 'got', 'all', 'dried', 'out', 'and', 'the', 'slice', 'tasted', 'more', 'like', 'a', 'crack', '##er', '.', '[SEP]']\n",
            "Original: ['He', 'preceded', 'to', 'grab', 'the', 'slice', 'off', 'the', 'countertop', 'and', 'throw', 'it', 'into', 'the', 'trash', 'while', 'yelling', 'at', 'me', 'saying,', '\"you', 'do', 'not', 'order', 'what', 'you', 'do', 'not', 'know', 'about\"', 'and', '\"you', \"don't\", 'know', 'how', 'pizza', 'is', 'made\".']\n",
            "Tokenized: ['[CLS]', 'he', 'preceded', 'to', 'grab', 'the', 'slice', 'off', 'the', 'counter', '##top', 'and', 'throw', 'it', 'into', 'the', 'trash', 'while', 'yelling', 'at', 'me', 'saying', ',', '\"', 'you', 'do', 'not', 'order', 'what', 'you', 'do', 'not', 'know', 'about', '\"', 'and', '\"', 'you', 'don', \"'\", 't', 'know', 'how', 'pizza', 'is', 'made', '\"', '.', '[SEP]']\n",
            "Original: ['It', 'was', 'very', 'upsetting', 'to', 'see', 'this', 'kind', 'of', 'behavior', 'especially', 'in', 'front', 'of', 'my', 'four', 'year', 'old.']\n",
            "Tokenized: ['[CLS]', 'it', 'was', 'very', 'upset', '##ting', 'to', 'see', 'this', 'kind', 'of', 'behavior', 'especially', 'in', 'front', 'of', 'my', 'four', 'year', 'old', '.', '[SEP]']\n",
            "Original: ['He', 'was', 'in', 'the', 'process', 'of', 'making', 'me', 'a', 'pie', 'and', 'when', 'I', 'got', 'home,', 'the', 'pie', 'was', 'the', 'worst', 'I', 'have', 'ever', 'seen.']\n",
            "Tokenized: ['[CLS]', 'he', 'was', 'in', 'the', 'process', 'of', 'making', 'me', 'a', 'pie', 'and', 'when', 'i', 'got', 'home', ',', 'the', 'pie', 'was', 'the', 'worst', 'i', 'have', 'ever', 'seen', '.', '[SEP]']\n",
            "Original: ['Cheese', 'was', 'falling', 'off,', 'so', 'oily', 'and', 'greasy.']\n",
            "Tokenized: ['[CLS]', 'cheese', 'was', 'falling', 'off', ',', 'so', 'oil', '##y', 'and', 'greasy', '.', '[SEP]']\n",
            "Original: ['I', 'think', 'he', 'did', 'it', 'on', 'purpose', 'because', 'of', 'my', 'simple', 'request', 'for', 'another', 'slice', 'of', 'grandma', 'that', \"wasn't\", 'so', 'well-done', '(actually,', 'burnt).']\n",
            "Tokenized: ['[CLS]', 'i', 'think', 'he', 'did', 'it', 'on', 'purpose', 'because', 'of', 'my', 'simple', 'request', 'for', 'another', 'slice', 'of', 'grandma', 'that', 'wasn', \"'\", 't', 'so', 'well', '-', 'done', '(', 'actually', ',', 'burnt', ')', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'never', 'go', 'back', 'to', 'this', 'place', 'and', 'I', 'am', 'reporting', 'them', 'to', 'the', 'better', 'business', 'bureau', 'for', 'such', 'horrible', 'customer', 'relations', 'and', 'basically', 'sabotaging', 'my', 'pizza', 'and', 'taking', 'my', '$25.']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'never', 'go', 'back', 'to', 'this', 'place', 'and', 'i', 'am', 'reporting', 'them', 'to', 'the', 'better', 'business', 'bureau', 'for', 'such', 'horrible', 'customer', 'relations', 'and', 'basically', 'sa', '##bot', '##aging', 'my', 'pizza', 'and', 'taking', 'my', '$', '25', '.', '[SEP]']\n",
            "Original: ['THIS', 'STORY', 'IS', '100%', 'TRUE.']\n",
            "Tokenized: ['[CLS]', 'this', 'story', 'is', '100', '%', 'true', '.', '[SEP]']\n",
            "Original: ['That', 'little', 'man', 'who', 'thinks', 'he', 'invented', 'pizza', 'can', 'kiss', 'my', '*ss.']\n",
            "Tokenized: ['[CLS]', 'that', 'little', 'man', 'who', 'thinks', 'he', 'invented', 'pizza', 'can', 'kiss', 'my', '*', 'ss', '.', '[SEP]']\n",
            "Original: ['Great', 'staff.']\n",
            "Tokenized: ['[CLS]', 'great', 'staff', '.', '[SEP]']\n",
            "Original: ['Very', 'helpful!!!!']\n",
            "Tokenized: ['[CLS]', 'very', 'helpful', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['They', 'interviewed', 'me,', 'gave', 'me', 'tests', 'in', 'the', 'software', 'I', 'included', 'on', 'my', 'resume,', 'and', 'placed', 'me', 'in', 'a', 'position', 'that', 'I', 'kept', 'for', 'several', 'years.']\n",
            "Tokenized: ['[CLS]', 'they', 'interviewed', 'me', ',', 'gave', 'me', 'tests', 'in', 'the', 'software', 'i', 'included', 'on', 'my', 'resume', ',', 'and', 'placed', 'me', 'in', 'a', 'position', 'that', 'i', 'kept', 'for', 'several', 'years', '.', '[SEP]']\n",
            "Original: ['Re-interviewed', 'and', 'am', 'going', 'on', 'interviews', 'for', 'a', 'new', 'job.']\n",
            "Tokenized: ['[CLS]', 're', '-', 'interviewed', 'and', 'am', 'going', 'on', 'interviews', 'for', 'a', 'new', 'job', '.', '[SEP]']\n",
            "Original: [\"I'm\", 'really', 'thankful', 'for', 'the', 'folks', 'at', 'HR', 'Office.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'm', 'really', 'thankful', 'for', 'the', 'folks', 'at', 'hr', 'office', '.', '[SEP]']\n",
            "Original: ['They', 'are', 'dependable,', 'have', 'great', 'connections', 'in', 'the', 'community,', 'and', 'are', 'a', 'great', 'resource', 'for', 'finding', 'a', 'job.']\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'depend', '##able', ',', 'have', 'great', 'connections', 'in', 'the', 'community', ',', 'and', 'are', 'a', 'great', 'resource', 'for', 'finding', 'a', 'job', '.', '[SEP]']\n",
            "Original: ['I', 'highly', 'recommend', 'them', 'to', 'all', 'of', 'my', 'friends!!!!!']\n",
            "Tokenized: ['[CLS]', 'i', 'highly', 'recommend', 'them', 'to', 'all', 'of', 'my', 'friends', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['Staten', 'Island', 'Computers']\n",
            "Tokenized: ['[CLS]', 'staten', 'island', 'computers', '[SEP]']\n",
            "Original: ['Superior', 'work', '-', 'always', 'comes', 'through', 'when', 'we', 'need', 'him.']\n",
            "Tokenized: ['[CLS]', 'superior', 'work', '-', 'always', 'comes', 'through', 'when', 'we', 'need', 'him', '.', '[SEP]']\n",
            "Original: ['We', 'have', 'tried', 'many', 'different', 'computer', 'people', 'until', 'now', '-', 'we', 'will', 'stick', 'with', 'Qualitech', 'Computers!!!']\n",
            "Tokenized: ['[CLS]', 'we', 'have', 'tried', 'many', 'different', 'computer', 'people', 'until', 'now', '-', 'we', 'will', 'stick', 'with', 'qu', '##ali', '##tech', 'computers', '!', '!', '!', '[SEP]']\n",
            "Original: ['So', 'Handy', 'for', 'Local', 'La', 'Jolla', 'Stuff', '-', 'Especially', 'for', 'Finding', 'Residential', 'Numbers']\n",
            "Tokenized: ['[CLS]', 'so', 'handy', 'for', 'local', 'la', 'jo', '##lla', 'stuff', '-', 'especially', 'for', 'finding', 'residential', 'numbers', '[SEP]']\n",
            "Original: ['Since', 'moving', 'back', 'from', 'college', 'and', 'trying', 'to', 'settle', 'roots', 'in', 'the', 'Village,', 'I', 'have', 'referred', 'to', 'the', 'La', 'Jolla', 'Blue', 'Book', 'for', 'a', 'ton', 'of', 'local', 'numbers.']\n",
            "Tokenized: ['[CLS]', 'since', 'moving', 'back', 'from', 'college', 'and', 'trying', 'to', 'settle', 'roots', 'in', 'the', 'village', ',', 'i', 'have', 'referred', 'to', 'the', 'la', 'jo', '##lla', 'blue', 'book', 'for', 'a', 'ton', 'of', 'local', 'numbers', '.', '[SEP]']\n",
            "Original: ['The', 'white', 'pages', 'allowed', 'me', 'to', 'get', 'in', 'touch', 'with', 'parents', 'of', 'my', 'high', 'school', 'friends', 'so', 'that', 'I', 'could', 'track', 'people', 'down', 'one', 'by', 'one', 'and', 'the', 'restaurant', 'section', 'is', 'basically', 'my', 'cookbook.']\n",
            "Tokenized: ['[CLS]', 'the', 'white', 'pages', 'allowed', 'me', 'to', 'get', 'in', 'touch', 'with', 'parents', 'of', 'my', 'high', 'school', 'friends', 'so', 'that', 'i', 'could', 'track', 'people', 'down', 'one', 'by', 'one', 'and', 'the', 'restaurant', 'section', 'is', 'basically', 'my', 'cook', '##book', '.', '[SEP]']\n",
            "Original: [\"It's\", 'really', 'cool', 'that', 'so', 'many', 'local', 'businesses', 'are', 'found', 'so', 'quickly', 'in', 'one', 'place', 'and', 'I', 'want', 'to', 'spread', 'the', 'word.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'really', 'cool', 'that', 'so', 'many', 'local', 'businesses', 'are', 'found', 'so', 'quickly', 'in', 'one', 'place', 'and', 'i', 'want', 'to', 'spread', 'the', 'word', '.', '[SEP]']\n",
            "Original: ['THX:-)']\n",
            "Tokenized: ['[CLS]', 'th', '##x', ':', '-', ')', '[SEP]']\n",
            "Original: ['AMAZINGLY', 'YUMMY!']\n",
            "Tokenized: ['[CLS]', 'amazingly', 'yu', '##mmy', '!', '[SEP]']\n",
            "Original: ['I', 'just', 'got', 'back', 'from', 'france', 'yesterday', 'and', 'just', 'missed', 'the', 'food', 'already!']\n",
            "Tokenized: ['[CLS]', 'i', 'just', 'got', 'back', 'from', 'france', 'yesterday', 'and', 'just', 'missed', 'the', 'food', 'already', '!', '[SEP]']\n",
            "Original: ['My', 'sister', 'in', 'law', 'told', 'me', 'about', 'this', 'amazing', 'new', 'crepe', 'place', 'in', 'town,', 'I', 'was', 'so', 'excited', 'I', 'just', 'wanted', 'to', 'go', 'and', 'test', 'it', 'out', 'for', 'my', 'self!']\n",
            "Tokenized: ['[CLS]', 'my', 'sister', 'in', 'law', 'told', 'me', 'about', 'this', 'amazing', 'new', 'cr', '##ep', '##e', 'place', 'in', 'town', ',', 'i', 'was', 'so', 'excited', 'i', 'just', 'wanted', 'to', 'go', 'and', 'test', 'it', 'out', 'for', 'my', 'self', '!', '[SEP]']\n",
            "Original: ['Their', 'customer', 'service', 'was', 'perfect!']\n",
            "Tokenized: ['[CLS]', 'their', 'customer', 'service', 'was', 'perfect', '!', '[SEP]']\n",
            "Original: ['Their', 'Food', 'was', 'better', 'then', 'anything', 'I', 'had', 'ever', 'tasted.']\n",
            "Tokenized: ['[CLS]', 'their', 'food', 'was', 'better', 'then', 'anything', 'i', 'had', 'ever', 'tasted', '.', '[SEP]']\n",
            "Original: ['EVEN', 'IN', 'FRANCE!']\n",
            "Tokenized: ['[CLS]', 'even', 'in', 'france', '!', '[SEP]']\n",
            "Original: ['I', 'would', 'highly', 'recommend', 'this', 'place', 'to', 'anyone', 'looking', 'for', 'a', 'great', 'atmosphere,', 'amazing', 'food,', 'and', 'great', 'customer', 'service!']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'highly', 'recommend', 'this', 'place', 'to', 'anyone', 'looking', 'for', 'a', 'great', 'atmosphere', ',', 'amazing', 'food', ',', 'and', 'great', 'customer', 'service', '!', '[SEP]']\n",
            "Original: ['Thank', 'you', 'Roll', 'UP', 'Crepes!']\n",
            "Tokenized: ['[CLS]', 'thank', 'you', 'roll', 'up', 'cr', '##ep', '##es', '!', '[SEP]']\n",
            "Original: ['Learn', 'from', 'a', 'Cesar', 'Gracie', 'black', 'belt', 'and', 'former', 'ufc', 'fighter!']\n",
            "Tokenized: ['[CLS]', 'learn', 'from', 'a', 'cesar', 'gracie', 'black', 'belt', 'and', 'former', 'ufc', 'fighter', '!', '[SEP]']\n",
            "Original: ['When', 'i', 'say', 'jiu-jitsu', 'or', 'mma', 'i', 'mean', 'it!']\n",
            "Tokenized: ['[CLS]', 'when', 'i', 'say', 'ji', '##u', '-', 'ji', '##tsu', 'or', 'mma', 'i', 'mean', 'it', '!', '[SEP]']\n",
            "Original: ['Best', 'jiu-jitsu', 'mma', 'in', 'Santa', 'Rosa', 'and', 'i', 'have', 'the', 'experience', 'and', 'belt', 'to', 'back', 'it', 'up!']\n",
            "Tokenized: ['[CLS]', 'best', 'ji', '##u', '-', 'ji', '##tsu', 'mma', 'in', 'santa', 'rosa', 'and', 'i', 'have', 'the', 'experience', 'and', 'belt', 'to', 'back', 'it', 'up', '!', '[SEP]']\n",
            "Original: ['When', 'you', 'come', 'to', 'ncfa', 'you', 'will', 'see', 'a', 'real', 'instructor', 'that', 'teaches', 'and', 'trains', 'everyday!']\n",
            "Tokenized: ['[CLS]', 'when', 'you', 'come', 'to', 'nc', '##fa', 'you', 'will', 'see', 'a', 'real', 'instructor', 'that', 'teaches', 'and', 'trains', 'everyday', '!', '[SEP]']\n",
            "Original: ['If', 'your', 'coach', 'has', 'no', 'fights', 'and', 'you', 'never', 'see', 'him', 'train', 'and', 'sweat', 'something', 'is', 'wrong!']\n",
            "Tokenized: ['[CLS]', 'if', 'your', 'coach', 'has', 'no', 'fights', 'and', 'you', 'never', 'see', 'him', 'train', 'and', 'sweat', 'something', 'is', 'wrong', '!', '[SEP]']\n",
            "Original: ['Dave', 'Terrell', 'www.norcalfightingalliance.com']\n",
            "Tokenized: ['[CLS]', 'dave', 'terre', '##ll', 'www', '.', 'nor', '##cal', '##fighting', '##all', '##iance', '.', 'com', '[SEP]']\n",
            "Original: [\"I'm\", 'really', 'surprised', 'by', 'the', 'negative', 'reviews.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'm', 'really', 'surprised', 'by', 'the', 'negative', 'reviews', '.', '[SEP]']\n",
            "Original: [\"We've\", 'had', 'about', '5', 'repairs', 'done', 'on', '3', 'different', 'laptops.']\n",
            "Tokenized: ['[CLS]', 'we', \"'\", 've', 'had', 'about', '5', 'repairs', 'done', 'on', '3', 'different', 'laptop', '##s', '.', '[SEP]']\n",
            "Original: [\"They've\", 'always', 'been', 'timely', 'and', 'inexpensive.']\n",
            "Tokenized: ['[CLS]', 'they', \"'\", 've', 'always', 'been', 'timely', 'and', 'inexpensive', '.', '[SEP]']\n",
            "Original: ['teeth']\n",
            "Tokenized: ['[CLS]', 'teeth', '[SEP]']\n",
            "Original: ['this', 'dentist', 'want', 'to', 'pull', 'the', 'tooth', 'out', 'always..', 'always', 'wants', 'to', 'do', 'the', 'cheapest', 'for', 'his', 'benefit..', 'not', 'unless', 'he', 'knows', 'you.']\n",
            "Tokenized: ['[CLS]', 'this', 'dentist', 'want', 'to', 'pull', 'the', 'tooth', 'out', 'always', '.', '.', 'always', 'wants', 'to', 'do', 'the', 'cheap', '##est', 'for', 'his', 'benefit', '.', '.', 'not', 'unless', 'he', 'knows', 'you', '.', '[SEP]']\n",
            "Original: ['and', 'hopefully', 'you', 'do', 'not', 'know', 'the', 'same', 'people', 'because', 'he', 'tells', 'others', 'about', 'you', 'payment', 'status.']\n",
            "Tokenized: ['[CLS]', 'and', 'hopefully', 'you', 'do', 'not', 'know', 'the', 'same', 'people', 'because', 'he', 'tells', 'others', 'about', 'you', 'payment', 'status', '.', '[SEP]']\n",
            "Original: ['Which', 'should', 'be', 'a', 'private', 'issue']\n",
            "Tokenized: ['[CLS]', 'which', 'should', 'be', 'a', 'private', 'issue', '[SEP]']\n",
            "Original: ['Grocery', 'and', 'Daily', 'Needs', 'Store']\n",
            "Tokenized: ['[CLS]', 'grocery', 'and', 'daily', 'needs', 'store', '[SEP]']\n",
            "Original: ['Before', 'using', 'FusionRetail', 'Before', 'installing', 'FusionRetail', 'store', 'was', 'running', 'on', 'a', 'dos', 'based', 'software.']\n",
            "Tokenized: ['[CLS]', 'before', 'using', 'fusion', '##ret', '##ail', 'before', 'installing', 'fusion', '##ret', '##ail', 'store', 'was', 'running', 'on', 'a', 'dos', 'based', 'software', '.', '[SEP]']\n",
            "Original: ['We', 'were', 'having', 'a', 'major', 'problem', 'in', 'maintaining', 'cash.']\n",
            "Tokenized: ['[CLS]', 'we', 'were', 'having', 'a', 'major', 'problem', 'in', 'maintaining', 'cash', '.', '[SEP]']\n",
            "Original: ['Being', 'a', 'grocery', 'shop,', 'maintaining', '5000', 'different', 'products', 'was', 'a', 'challenging', 'job.']\n",
            "Tokenized: ['[CLS]', 'being', 'a', 'grocery', 'shop', ',', 'maintaining', '5000', 'different', 'products', 'was', 'a', 'challenging', 'job', '.', '[SEP]']\n",
            "Original: ['Managing', 'POS', 'counter', 'without', 'barcoding', 'was', 'really', 'a', 'tough', 'time.']\n",
            "Tokenized: ['[CLS]', 'managing', 'po', '##s', 'counter', 'without', 'bar', '##co', '##ding', 'was', 'really', 'a', 'tough', 'time', '.', '[SEP]']\n",
            "Original: ['How', 'FusionRetail', 'has', 'overcome', 'these', 'issues', '?']\n",
            "Tokenized: ['[CLS]', 'how', 'fusion', '##ret', '##ail', 'has', 'overcome', 'these', 'issues', '?', '[SEP]']\n",
            "Original: ['FusionRetail', 'helps', 'us', 'to', 'maintain', 'the', 'store', 'in', 'an', 'organised', 'way.']\n",
            "Tokenized: ['[CLS]', 'fusion', '##ret', '##ail', 'helps', 'us', 'to', 'maintain', 'the', 'store', 'in', 'an', 'organised', 'way', '.', '[SEP]']\n",
            "Original: ['Usage', 'of', 'product', 'barcodes', 'and', 'smooth', 'maintenance', 'of', 'inventory', 'with', 'proper', 'recording', 'of', 'transactions', 'like', 'sale,', 'purchase', 'and', 'returns', 'was', 'never', 'easy', 'before.']\n",
            "Tokenized: ['[CLS]', 'usage', 'of', 'product', 'bar', '##codes', 'and', 'smooth', 'maintenance', 'of', 'inventory', 'with', 'proper', 'recording', 'of', 'transactions', 'like', 'sale', ',', 'purchase', 'and', 'returns', 'was', 'never', 'easy', 'before', '.', '[SEP]']\n",
            "Original: ['How', 'long', 'does', 'it', 'take', 'to', 'train', 'new', 'people', 'at', 'work', '?']\n",
            "Tokenized: ['[CLS]', 'how', 'long', 'does', 'it', 'take', 'to', 'train', 'new', 'people', 'at', 'work', '?', '[SEP]']\n",
            "Original: ['Billing', 'takes', '15', 'minutes', 'and', 'backoffice', 'jobs', 'takes', '1', \"day's\", 'training', 'How', 'fast', 'your', 'support', 'queries', 'get', 'answered', '?']\n",
            "Tokenized: ['[CLS]', 'billing', 'takes', '15', 'minutes', 'and', 'back', '##off', '##ice', 'jobs', 'takes', '1', 'day', \"'\", 's', 'training', 'how', 'fast', 'your', 'support', 'que', '##ries', 'get', 'answered', '?', '[SEP]']\n",
            "Original: ['Over', 'telephone,', 'immediate.']\n",
            "Tokenized: ['[CLS]', 'over', 'telephone', ',', 'immediate', '.', '[SEP]']\n",
            "Original: ['On', 'call,', 'it', 'takes', 'a', 'day', 'to', 'get', 'our', 'issues', 'resolved.']\n",
            "Tokenized: ['[CLS]', 'on', 'call', ',', 'it', 'takes', 'a', 'day', 'to', 'get', 'our', 'issues', 'resolved', '.', '[SEP]']\n",
            "Original: ['The', 'landlord', 'is', 'not', 'nice', 'nor', 'helpful']\n",
            "Tokenized: ['[CLS]', 'the', 'landlord', 'is', 'not', 'nice', 'nor', 'helpful', '[SEP]']\n",
            "Original: ['I', 'have', 'lived', 'in', 'Buckingham', 'Condominiums', 'townhouse', 'for', '2', 'years.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'lived', 'in', 'buckingham', 'condom', '##inium', '##s', 'town', '##house', 'for', '2', 'years', '.', '[SEP]']\n",
            "Original: ['I', 'love', 'the', 'location', 'and', 'the', 'apartment!!']\n",
            "Tokenized: ['[CLS]', 'i', 'love', 'the', 'location', 'and', 'the', 'apartment', '!', '!', '[SEP]']\n",
            "Original: [\"I'ma\", 'single', 'female', 'and', 'I', 'feel', 'safe', 'coming', 'home', 'at', 'night.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'ma', 'single', 'female', 'and', 'i', 'feel', 'safe', 'coming', 'home', 'at', 'night', '.', '[SEP]']\n",
            "Original: ['The', 'maintenance', 'people', 'are', 'AWESOME!!!!']\n",
            "Tokenized: ['[CLS]', 'the', 'maintenance', 'people', 'are', 'awesome', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['And', 'the', 'exterminator', 'is', 'very', 'nice,', 'also.']\n",
            "Tokenized: ['[CLS]', 'and', 'the', 'ex', '##ter', '##mina', '##tor', 'is', 'very', 'nice', ',', 'also', '.', '[SEP]']\n",
            "Original: ['Yes,', 'you', 'still', 'have', 'a', 'few', 'bugs,', 'but', \"that's\", 'going', 'to', 'be', 'anywhere', 'you', 'go!']\n",
            "Tokenized: ['[CLS]', 'yes', ',', 'you', 'still', 'have', 'a', 'few', 'bugs', ',', 'but', 'that', \"'\", 's', 'going', 'to', 'be', 'anywhere', 'you', 'go', '!', '[SEP]']\n",
            "Original: ['The', 'only', 'problem', 'that', 'I', 'have', 'experienced', 'is', 'the', 'landlord.']\n",
            "Tokenized: ['[CLS]', 'the', 'only', 'problem', 'that', 'i', 'have', 'experienced', 'is', 'the', 'landlord', '.', '[SEP]']\n",
            "Original: ['She', 'is', 'a', 'pure', 'b****!!!']\n",
            "Tokenized: ['[CLS]', 'she', 'is', 'a', 'pure', 'b', '*', '*', '*', '*', '!', '!', '!', '[SEP]']\n",
            "Original: ['I', 'know', \"that's\", 'ugly...but', 'she', \"won't\", 'help', 'you', 'out', 'for', 'anything...sad', 'story.']\n",
            "Tokenized: ['[CLS]', 'i', 'know', 'that', \"'\", 's', 'ugly', '.', '.', '.', 'but', 'she', 'won', \"'\", 't', 'help', 'you', 'out', 'for', 'anything', '.', '.', '.', 'sad', 'story', '.', '[SEP]']\n",
            "Original: ['Other', 'than', 'that,', 'I', 'would', 'recommend', 'living', 'here.', ':)']\n",
            "Tokenized: ['[CLS]', 'other', 'than', 'that', ',', 'i', 'would', 'recommend', 'living', 'here', '.', ':', ')', '[SEP]']\n",
            "Original: ['The', 'best', 'Supermarket', 'in', 'Bay', 'Ridge', 'have', 'everything', 'what', 'a', 'customer', 'needs.']\n",
            "Tokenized: ['[CLS]', 'the', 'best', 'supermarket', 'in', 'bay', 'ridge', 'have', 'everything', 'what', 'a', 'customer', 'needs', '.', '[SEP]']\n",
            "Original: ['Our', 'company', 'is', 'a', 'high', 'end', 'designer', 'handbag', 'and', 'fashion', 'accessories', 'company,', 'thus', 'we', 'are', 'certainly', 'a', 'niche', 'market.']\n",
            "Tokenized: ['[CLS]', 'our', 'company', 'is', 'a', 'high', 'end', 'designer', 'hand', '##bag', 'and', 'fashion', 'accessories', 'company', ',', 'thus', 'we', 'are', 'certainly', 'a', 'niche', 'market', '.', '[SEP]']\n",
            "Original: ['We', 'had', 'been', 'evaluating', 'SEO', 'providers', 'for', 'quite', 'some', 'time', 'and', 'finally', 'decided', 'to', 'take', 'the', 'plunge', 'with', 'Stuart,', 'and', 'Ulistic.']\n",
            "Tokenized: ['[CLS]', 'we', 'had', 'been', 'evaluating', 'seo', 'providers', 'for', 'quite', 'some', 'time', 'and', 'finally', 'decided', 'to', 'take', 'the', 'plunge', 'with', 'stuart', ',', 'and', 'ul', '##istic', '.', '[SEP]']\n",
            "Original: ['We', 'made', 'the', 'decision', 'for', 'a', 'couple', 'of', 'reasons.']\n",
            "Tokenized: ['[CLS]', 'we', 'made', 'the', 'decision', 'for', 'a', 'couple', 'of', 'reasons', '.', '[SEP]']\n",
            "Original: ['1.', 'Social', 'Media.']\n",
            "Tokenized: ['[CLS]', '1', '.', 'social', 'media', '.', '[SEP]']\n",
            "Original: ['We', 'were', 'familiar', 'with', 'Search', 'Engine', 'Optimization', 'strategies,', 'but', 'new', 'nothing', 'about', 'Social', 'Media', '-', 'we', 'just', 'heard', 'that', 'it', 'was', 'the', 'next', 'big', 'thing.']\n",
            "Tokenized: ['[CLS]', 'we', 'were', 'familiar', 'with', 'search', 'engine', 'optimization', 'strategies', ',', 'but', 'new', 'nothing', 'about', 'social', 'media', '-', 'we', 'just', 'heard', 'that', 'it', 'was', 'the', 'next', 'big', 'thing', '.', '[SEP]']\n",
            "Original: ['2.', 'We', 'really', 'liked', 'the', 'fact', 'that', 'Stuart', 'sets', 'defined', 'objectives', 'and', 'we', 'meet', 'once', 'a', 'month', 'to', 'go', 'over', 'our', 'Key', 'Performance', 'Indicators.']\n",
            "Tokenized: ['[CLS]', '2', '.', 'we', 'really', 'liked', 'the', 'fact', 'that', 'stuart', 'sets', 'defined', 'objectives', 'and', 'we', 'meet', 'once', 'a', 'month', 'to', 'go', 'over', 'our', 'key', 'performance', 'indicators', '.', '[SEP]']\n",
            "Original: ['How', 'has', 'it', 'gone', 'so', 'far?']\n",
            "Tokenized: ['[CLS]', 'how', 'has', 'it', 'gone', 'so', 'far', '?', '[SEP]']\n",
            "Original: ['Well', 'we', 'have', 'been', 'working', 'with', 'Ulistic', 'for', '1.5', 'months,', 'and', 'have', '100', 'people', 'following', 'our', 'site', 'on', 'Facebook,', 'and', 'our', 'web', 'site,', 'www.designofashion.com', 'has', 'seen', 'a', '3', 'fold', 'increase', 'in', 'traffic,', 'which', 'is', 'significantly', 'beating', 'our', 'expectations', '-', 'SEO', 'is', 'a', 'process', 'that', 'takes', 'time', 'and', 'to', 'get', 'results', 'this', 'quickly', 'is', 'exceptional.']\n",
            "Tokenized: ['[CLS]', 'well', 'we', 'have', 'been', 'working', 'with', 'ul', '##istic', 'for', '1', '.', '5', 'months', ',', 'and', 'have', '100', 'people', 'following', 'our', 'site', 'on', 'facebook', ',', 'and', 'our', 'web', 'site', ',', 'www', '.', 'design', '##of', '##ashi', '##on', '.', 'com', 'has', 'seen', 'a', '3', 'fold', 'increase', 'in', 'traffic', ',', 'which', 'is', 'significantly', 'beating', 'our', 'expectations', '-', 'seo', 'is', 'a', 'process', 'that', 'takes', 'time', 'and', 'to', 'get', 'results', 'this', 'quickly', 'is', 'exceptional', '.', '[SEP]']\n",
            "Original: ['We', 'are', 'absolutely', 'confident', 'that', \"Stuart's\", 'ethical', 'and', 'focused', 'strategies', 'will', 'see', 'these', 'trends', 'continue', 'to', 'grow,', 'and', 'our', 'business', 'will', 'reap', 'the', 'rewards', 'of', 'this', 'program.']\n",
            "Tokenized: ['[CLS]', 'we', 'are', 'absolutely', 'confident', 'that', 'stuart', \"'\", 's', 'ethical', 'and', 'focused', 'strategies', 'will', 'see', 'these', 'trends', 'continue', 'to', 'grow', ',', 'and', 'our', 'business', 'will', 're', '##ap', 'the', 'rewards', 'of', 'this', 'program', '.', '[SEP]']\n",
            "Original: ['Finally,', 'it', 'must', 'be', 'said', 'that', 'Stuart', 'is', 'a', 'fantastic', 'person', 'to', 'work', 'with,', 'because', 'of', 'his', 'solid', 'strategies', 'and', 'equally', 'as', 'importantly', 'because', 'he', 'is', 'a', 'genuinely', 'good', 'person', 'and', 'a', 'great', 'communicator.']\n",
            "Tokenized: ['[CLS]', 'finally', ',', 'it', 'must', 'be', 'said', 'that', 'stuart', 'is', 'a', 'fantastic', 'person', 'to', 'work', 'with', ',', 'because', 'of', 'his', 'solid', 'strategies', 'and', 'equally', 'as', 'importantly', 'because', 'he', 'is', 'a', 'genuinely', 'good', 'person', 'and', 'a', 'great', 'com', '##mun', '##ica', '##tor', '.', '[SEP]']\n",
            "Original: ['Great', 'Meal']\n",
            "Tokenized: ['[CLS]', 'great', 'meal', '[SEP]']\n",
            "Original: ['Happened', 'on', 'to', 'this', 'place', 'while', 'out', 'of', 'town', 'on', 'business,', 'and', 'it', 'was', 'great!']\n",
            "Tokenized: ['[CLS]', 'happened', 'on', 'to', 'this', 'place', 'while', 'out', 'of', 'town', 'on', 'business', ',', 'and', 'it', 'was', 'great', '!', '[SEP]']\n",
            "Original: ['The', 'food', 'was', 'excellent', 'and', 'the', 'service', 'was', 'terrific.']\n",
            "Tokenized: ['[CLS]', 'the', 'food', 'was', 'excellent', 'and', 'the', 'service', 'was', 'terrific', '.', '[SEP]']\n",
            "Original: ['It', 'is', 'a', 'cloth', 'napkin', 'kind', 'of', 'place,', 'but', 'I', 'thought', 'well', 'worth', 'it.']\n",
            "Tokenized: ['[CLS]', 'it', 'is', 'a', 'cloth', 'napkin', 'kind', 'of', 'place', ',', 'but', 'i', 'thought', 'well', 'worth', 'it', '.', '[SEP]']\n",
            "Original: ['Excellent', 'service,', 'close', 'to', 'the', 'morse', 'redline', 'stop.']\n",
            "Tokenized: ['[CLS]', 'excellent', 'service', ',', 'close', 'to', 'the', 'morse', 'red', '##line', 'stop', '.', '[SEP]']\n",
            "Original: ['Great', 'computer', 'repair', 'store,', 'highly', 'recommended.']\n",
            "Tokenized: ['[CLS]', 'great', 'computer', 'repair', 'store', ',', 'highly', 'recommended', '.', '[SEP]']\n",
            "Original: [\"Can't\", 'say', 'enough']\n",
            "Tokenized: ['[CLS]', 'can', \"'\", 't', 'say', 'enough', '[SEP]']\n",
            "Original: ['I', 'used', 'to', 'tan', 'down', 'the', 'street', 'before', 'I', 'was', 'referred', 'to', 'this', 'place', 'by', 'one', 'of', 'my', 'friends.']\n",
            "Tokenized: ['[CLS]', 'i', 'used', 'to', 'tan', 'down', 'the', 'street', 'before', 'i', 'was', 'referred', 'to', 'this', 'place', 'by', 'one', 'of', 'my', 'friends', '.', '[SEP]']\n",
            "Original: ['WOW!']\n",
            "Tokenized: ['[CLS]', 'wow', '!', '[SEP]']\n",
            "Original: ['I', \"didn't\", 'know', 'what', 'I', 'was', 'missing.']\n",
            "Tokenized: ['[CLS]', 'i', 'didn', \"'\", 't', 'know', 'what', 'i', 'was', 'missing', '.', '[SEP]']\n",
            "Original: ['The', 'lowest', 'bed', 'here', 'is', 'better', 'than', 'my', 'last', 'salons', 'highest', 'level.']\n",
            "Tokenized: ['[CLS]', 'the', 'lowest', 'bed', 'here', 'is', 'better', 'than', 'my', 'last', 'salon', '##s', 'highest', 'level', '.', '[SEP]']\n",
            "Original: ['Salon', 'is', 'clean', 'and', 'girls', 'are', 'nice.']\n",
            "Tokenized: ['[CLS]', 'salon', 'is', 'clean', 'and', 'girls', 'are', 'nice', '.', '[SEP]']\n",
            "Original: ['My', 'girlfriend', 'and', 'I', 'took', 'a', 'chance', 'on', 'this', 'place', 'because', 'we', \"didn't\", 'want', 'to', 'wait', 'in', 'line', 'at', 'Outback.']\n",
            "Tokenized: ['[CLS]', 'my', 'girlfriend', 'and', 'i', 'took', 'a', 'chance', 'on', 'this', 'place', 'because', 'we', 'didn', \"'\", 't', 'want', 'to', 'wait', 'in', 'line', 'at', 'out', '##back', '.', '[SEP]']\n",
            "Original: ['What', 'an', 'amazing', 'find', '-', 'this', 'restaurant', 'is', 'a', 'GEM.']\n",
            "Tokenized: ['[CLS]', 'what', 'an', 'amazing', 'find', '-', 'this', 'restaurant', 'is', 'a', 'gem', '.', '[SEP]']\n",
            "Original: ['#1', 'its', 'immaculately', 'clean.']\n",
            "Tokenized: ['[CLS]', '#', '1', 'its', 'immaculate', '##ly', 'clean', '.', '[SEP]']\n",
            "Original: ['#2', 'the', 'decor', 'is', 'tasteful', 'and', 'artistic,', 'from', 'the', 'comfortable', 'chairs', 'to', 'the', 'elegant', 'light', 'fixtures....', 'and', '(most', 'importantly)', '#3', 'the', 'food', 'is', 'FANTASTIC.']\n",
            "Tokenized: ['[CLS]', '#', '2', 'the', 'decor', 'is', 'taste', '##ful', 'and', 'artistic', ',', 'from', 'the', 'comfortable', 'chairs', 'to', 'the', 'elegant', 'light', 'fixtures', '.', '.', '.', '.', 'and', '(', 'most', 'importantly', ')', '#', '3', 'the', 'food', 'is', 'fantastic', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'authentic', 'Cuban', 'cuisine;', 'fresh', 'ingredients', 'expertly', 'prepared', 'and', 'seasoned', 'perfectly.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'authentic', 'cuban', 'cuisine', ';', 'fresh', 'ingredients', 'expert', '##ly', 'prepared', 'and', 'seasoned', 'perfectly', '.', '[SEP]']\n",
            "Original: ['The', 'portions', 'were', 'generous', 'and', 'we', 'got', 'out', 'for', 'less', 'than', 'the', 'cost', 'of', 'one', 'entree', 'at', 'some', 'chain', 'restaurant.']\n",
            "Tokenized: ['[CLS]', 'the', 'portions', 'were', 'generous', 'and', 'we', 'got', 'out', 'for', 'less', 'than', 'the', 'cost', 'of', 'one', 'en', '##tree', 'at', 'some', 'chain', 'restaurant', '.', '[SEP]']\n",
            "Original: ['TRY', 'THIS', 'PLACE', '-', \"YOU'LL\", 'LOVE', 'IT.']\n",
            "Tokenized: ['[CLS]', 'try', 'this', 'place', '-', 'you', \"'\", 'll', 'love', 'it', '.', '[SEP]']\n",
            "Original: ['Best', \"DJ's\", 'In', 'Town!']\n",
            "Tokenized: ['[CLS]', 'best', 'dj', \"'\", 's', 'in', 'town', '!', '[SEP]']\n",
            "Original: ['Wow!']\n",
            "Tokenized: ['[CLS]', 'wow', '!', '[SEP]']\n",
            "Original: ['These', 'guys', 'were', 'the', 'best.']\n",
            "Tokenized: ['[CLS]', 'these', 'guys', 'were', 'the', 'best', '.', '[SEP]']\n",
            "Original: ['They', 'were', 'thorough,', 'high', 'class,', 'and', 'went', 'above', 'and', 'beyond.']\n",
            "Tokenized: ['[CLS]', 'they', 'were', 'thorough', ',', 'high', 'class', ',', 'and', 'went', 'above', 'and', 'beyond', '.', '[SEP]']\n",
            "Original: ['We', 'never', 'had', 'to', 'worry', 'about', 'a', 'thing,', 'and', 'they', 'led', 'the', 'way', 'the', 'whole', 'time.']\n",
            "Tokenized: ['[CLS]', 'we', 'never', 'had', 'to', 'worry', 'about', 'a', 'thing', ',', 'and', 'they', 'led', 'the', 'way', 'the', 'whole', 'time', '.', '[SEP]']\n",
            "Original: ['They', 'asked', 'us', 'things', 'that', 'we', 'would', 'have', 'never', 'have', 'thought', 'of,', 'and', 'took', 'extra', 'time', 'to', 'meet', 'with', 'us', 'when', 'we', 'needed', 'it', 'before', 'the', 'wedding.']\n",
            "Tokenized: ['[CLS]', 'they', 'asked', 'us', 'things', 'that', 'we', 'would', 'have', 'never', 'have', 'thought', 'of', ',', 'and', 'took', 'extra', 'time', 'to', 'meet', 'with', 'us', 'when', 'we', 'needed', 'it', 'before', 'the', 'wedding', '.', '[SEP]']\n",
            "Original: ['Having', 'a', 'team', 'was', 'the', 'best', 'because', 'they', 'kept', 'the', 'flow', 'of', 'the', 'wedding', 'going', 'the', 'whole', 'time!']\n",
            "Tokenized: ['[CLS]', 'having', 'a', 'team', 'was', 'the', 'best', 'because', 'they', 'kept', 'the', 'flow', 'of', 'the', 'wedding', 'going', 'the', 'whole', 'time', '!', '[SEP]']\n",
            "Original: ['They', 'may', 'look', 'young', 'but', \"don't\", 'let', 'that', 'fool', 'you,', 'as', 'their', 'knowledge', 'of', 'music', 'far', 'surpassed', 'what', 'we', 'expected.']\n",
            "Tokenized: ['[CLS]', 'they', 'may', 'look', 'young', 'but', 'don', \"'\", 't', 'let', 'that', 'fool', 'you', ',', 'as', 'their', 'knowledge', 'of', 'music', 'far', 'surpassed', 'what', 'we', 'expected', '.', '[SEP]']\n",
            "Original: ['I', 'am', 'a', 'music', 'junkie', 'that', 'grew', 'up', 'in', 'the', \"80's,\", 'and', 'my', 'dad', 'worked', 'for', 'a', 'record', 'label', 'in', 'the', \"1960's.\"]\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'a', 'music', 'junk', '##ie', 'that', 'grew', 'up', 'in', 'the', '80', \"'\", 's', ',', 'and', 'my', 'dad', 'worked', 'for', 'a', 'record', 'label', 'in', 'the', '1960', \"'\", 's', '.', '[SEP]']\n",
            "Original: ['So', 'we', \"didn't\", 'expect', 'them', 'to', 'even', 'know', 'some', 'of', 'the', 'requests', 'that', 'we', 'asked', 'that', 'night.']\n",
            "Tokenized: ['[CLS]', 'so', 'we', 'didn', \"'\", 't', 'expect', 'them', 'to', 'even', 'know', 'some', 'of', 'the', 'requests', 'that', 'we', 'asked', 'that', 'night', '.', '[SEP]']\n",
            "Original: ['They', 'just', 'really', 'know', 'their', 'stuff!']\n",
            "Tokenized: ['[CLS]', 'they', 'just', 'really', 'know', 'their', 'stuff', '!', '[SEP]']\n",
            "Original: ['Good', 'Service', '-', 'Limited', 'Results']\n",
            "Tokenized: ['[CLS]', 'good', 'service', '-', 'limited', 'results', '[SEP]']\n",
            "Original: ['Andrew', 'was', 'helpful', 'and', 'knowledgeable', 'about', 'acupuncture', 're:', 'infertility.']\n",
            "Tokenized: ['[CLS]', 'andrew', 'was', 'helpful', 'and', 'knowledge', '##able', 'about', 'ac', '##up', '##un', '##cture', 're', ':', 'in', '##fer', '##tility', '.', '[SEP]']\n",
            "Original: ['He', 'was', 'willing', 'to', 'talk', 'to', 'me', 'about', 'my', 'specific', 'issues', 'and', 'develop', 'a', 'plan', 'of', 'action.']\n",
            "Tokenized: ['[CLS]', 'he', 'was', 'willing', 'to', 'talk', 'to', 'me', 'about', 'my', 'specific', 'issues', 'and', 'develop', 'a', 'plan', 'of', 'action', '.', '[SEP]']\n",
            "Original: ['The', 'office', 'is', 'shared', 'with', 'a', 'foot', 'doctor', 'and', \"it's\", 'very', 'sterile', 'and', 'medical', 'feeling,', 'which', 'I', 'liked.']\n",
            "Tokenized: ['[CLS]', 'the', 'office', 'is', 'shared', 'with', 'a', 'foot', 'doctor', 'and', 'it', \"'\", 's', 'very', 'sterile', 'and', 'medical', 'feeling', ',', 'which', 'i', 'liked', '.', '[SEP]']\n",
            "Original: ['The', 'down', 'side', 'was', 'that', 'sometimes', 'there', 'was', 'a', 'lot', 'of', 'noise', 'in', 'the', 'hallway', 'from', 'other', 'patients/doctors.']\n",
            "Tokenized: ['[CLS]', 'the', 'down', 'side', 'was', 'that', 'sometimes', 'there', 'was', 'a', 'lot', 'of', 'noise', 'in', 'the', 'hallway', 'from', 'other', 'patients', '/', 'doctors', '.', '[SEP]']\n",
            "Original: ['I', 'worked', 'with', 'Andrew', 'for', '2', 'months', 'and', 'did', 'acupuncture', 'and', 'herbs.']\n",
            "Tokenized: ['[CLS]', 'i', 'worked', 'with', 'andrew', 'for', '2', 'months', 'and', 'did', 'ac', '##up', '##un', '##cture', 'and', 'herbs', '.', '[SEP]']\n",
            "Original: ['The', 'sessions', 'were', 'nice', 'and', 'I', 'felt', 'relaxed', 'after', 'them', 'but', 'did', 'not', 'notice', 'any', 'changes', 'with', 'my', 'cycles.']\n",
            "Tokenized: ['[CLS]', 'the', 'sessions', 'were', 'nice', 'and', 'i', 'felt', 'relaxed', 'after', 'them', 'but', 'did', 'not', 'notice', 'any', 'changes', 'with', 'my', 'cycles', '.', '[SEP]']\n",
            "Original: ['I', 'know', 'it', 'can', 'take', 'awhile', 'for', 'results', 'and', \"didn't\", 'expect', 'a', 'miracle,', 'but', 'after', '2', 'months', 'I', 'felt', 'like', 'it', 'was', 'not', 'entirely', 'worth', 'the', 'cost/time.']\n",
            "Tokenized: ['[CLS]', 'i', 'know', 'it', 'can', 'take', 'awhile', 'for', 'results', 'and', 'didn', \"'\", 't', 'expect', 'a', 'miracle', ',', 'but', 'after', '2', 'months', 'i', 'felt', 'like', 'it', 'was', 'not', 'entirely', 'worth', 'the', 'cost', '/', 'time', '.', '[SEP]']\n",
            "Original: ['(I', 'am', 'also', 'a', 'little', 'suspicious', 'of', 'all', 'these', 'glowing', 'reviews...)']\n",
            "Tokenized: ['[CLS]', '(', 'i', 'am', 'also', 'a', 'little', 'suspicious', 'of', 'all', 'these', 'glowing', 'reviews', '.', '.', '.', ')', '[SEP]']\n",
            "Original: ['do', 'NOT', 'bring', 'your', 'car', 'here']\n",
            "Tokenized: ['[CLS]', 'do', 'not', 'bring', 'your', 'car', 'here', '[SEP]']\n",
            "Original: ['I', 'got', 'a', 'coupon', 'from', 'Pennysaver', 'for', 'this', 'station.']\n",
            "Tokenized: ['[CLS]', 'i', 'got', 'a', 'coup', '##on', 'from', 'penny', '##sa', '##ver', 'for', 'this', 'station', '.', '[SEP]']\n",
            "Original: ['Yes,', 'they', 'accepted', 'it.']\n",
            "Tokenized: ['[CLS]', 'yes', ',', 'they', 'accepted', 'it', '.', '[SEP]']\n",
            "Original: ['However,', 'during', 'the', 'test,', 'they', 'did', 'whatever', 'they', 'can', 'to', 'get', 'my', 'test', 'failed.']\n",
            "Tokenized: ['[CLS]', 'however', ',', 'during', 'the', 'test', ',', 'they', 'did', 'whatever', 'they', 'can', 'to', 'get', 'my', 'test', 'failed', '.', '[SEP]']\n",
            "Original: ['Then,', 'they', 'sold', 'me', 'overpriced', 'stuffs,', 'such', 'as', 'oil', 'tank', 'cap,', 'so', 'that', 'my', 'car', 'can', 'pass', 'it', 'right', 'away.']\n",
            "Tokenized: ['[CLS]', 'then', ',', 'they', 'sold', 'me', 'over', '##pr', '##ice', '##d', 'stuff', '##s', ',', 'such', 'as', 'oil', 'tank', 'cap', ',', 'so', 'that', 'my', 'car', 'can', 'pass', 'it', 'right', 'away', '.', '[SEP]']\n",
            "Original: ['I', 'ended', 'up', 'paying', 'much', 'more.']\n",
            "Tokenized: ['[CLS]', 'i', 'ended', 'up', 'paying', 'much', 'more', '.', '[SEP]']\n",
            "Original: ['Great', 'Cookies,', 'Cakes,', 'and', 'Customer', 'Service']\n",
            "Tokenized: ['[CLS]', 'great', 'cookies', ',', 'cakes', ',', 'and', 'customer', 'service', '[SEP]']\n",
            "Original: ['It', 'was', 'my', 'birthday', 'and', 'I', 'had', 'a', 'last', 'minute', 'idea', 'to', 'have', 'a', 'bakery', 'cake', 'instead', 'of', 'one', 'pre-made', 'in', 'a', 'convenience', 'store,', 'but', 'the', 'problem', 'was', 'it', 'was', 'the', 'day', 'before', 'Valentines', 'and', 'when', 'many', 'bakeries', 'turned', 'me', 'down', 'for', 'a', 'plain', 'vanilla', 'rectangle', 'cake,', 'Fiona', 'stepped', 'up', 'to', 'the', 'plate', 'and', 'was', 'able', 'to', 'make', 'a', 'fantastic', 'beautiful', 'cake.']\n",
            "Tokenized: ['[CLS]', 'it', 'was', 'my', 'birthday', 'and', 'i', 'had', 'a', 'last', 'minute', 'idea', 'to', 'have', 'a', 'bakery', 'cake', 'instead', 'of', 'one', 'pre', '-', 'made', 'in', 'a', 'convenience', 'store', ',', 'but', 'the', 'problem', 'was', 'it', 'was', 'the', 'day', 'before', 'valentine', '##s', 'and', 'when', 'many', 'baker', '##ies', 'turned', 'me', 'down', 'for', 'a', 'plain', 'vanilla', 'rec', '##tangle', 'cake', ',', 'fiona', 'stepped', 'up', 'to', 'the', 'plate', 'and', 'was', 'able', 'to', 'make', 'a', 'fantastic', 'beautiful', 'cake', '.', '[SEP]']\n",
            "Original: ['Not', 'only', 'did', 'it', 'taste', 'wonderful,', 'but', 'the', 'texture', 'was', 'unbelievable,', 'the', 'frosting', \"wasn't\", 'overly', 'sweet', 'to', 'over', 'power', 'the', 'cake,', 'and', 'the', 'cake', 'itself', 'was', 'just', 'amazingly', 'soft,', 'and', 'fluffy,', 'and', 'just', 'perfect', 'overall.']\n",
            "Tokenized: ['[CLS]', 'not', 'only', 'did', 'it', 'taste', 'wonderful', ',', 'but', 'the', 'texture', 'was', 'unbelievable', ',', 'the', 'frost', '##ing', 'wasn', \"'\", 't', 'overly', 'sweet', 'to', 'over', 'power', 'the', 'cake', ',', 'and', 'the', 'cake', 'itself', 'was', 'just', 'amazingly', 'soft', ',', 'and', 'fluffy', ',', 'and', 'just', 'perfect', 'overall', '.', '[SEP]']\n",
            "Original: ['Although', \"I'll\", 'have', 'to', 'drive', 'a', 'little', 'out', 'of', 'my', 'way', 'to', 'go', 'there,', \"I'll\", 'gladly', 'do', 'it', 'knowing', 'that', 'since', \"she's\", 'been', 'astounding', 'to', 'me', 'once', 'before', 'that', \"she'll\", 'always', 'be', 'that', 'way!']\n",
            "Tokenized: ['[CLS]', 'although', 'i', \"'\", 'll', 'have', 'to', 'drive', 'a', 'little', 'out', 'of', 'my', 'way', 'to', 'go', 'there', ',', 'i', \"'\", 'll', 'gladly', 'do', 'it', 'knowing', 'that', 'since', 'she', \"'\", 's', 'been', 'as', '##tou', '##nding', 'to', 'me', 'once', 'before', 'that', 'she', \"'\", 'll', 'always', 'be', 'that', 'way', '!', '[SEP]']\n",
            "Original: ['(Also', 'she', 'has', 'a', 'really', 'great', 'website!']\n",
            "Tokenized: ['[CLS]', '(', 'also', 'she', 'has', 'a', 'really', 'great', 'website', '!', '[SEP]']\n",
            "Original: ['And', 'we', 'bought', 'a', 'few', 'cookies', 'there', 'too,', 'they', 'were', 'fantastic', 'as', 'well!)']\n",
            "Tokenized: ['[CLS]', 'and', 'we', 'bought', 'a', 'few', 'cookies', 'there', 'too', ',', 'they', 'were', 'fantastic', 'as', 'well', '!', ')', '[SEP]']\n",
            "Original: ['She', 'deserves', 'many', '5', 'star', 'reviews!!']\n",
            "Tokenized: ['[CLS]', 'she', 'deserves', 'many', '5', 'star', 'reviews', '!', '!', '[SEP]']\n",
            "Original: ['very', 'reasonable', 'prices.']\n",
            "Tokenized: ['[CLS]', 'very', 'reasonable', 'prices', '.', '[SEP]']\n",
            "Original: ['quick', 'in', '&', 'out.']\n",
            "Tokenized: ['[CLS]', 'quick', 'in', '&', 'out', '.', '[SEP]']\n",
            "Original: ['Friendly', 'service.']\n",
            "Tokenized: ['[CLS]', 'friendly', 'service', '.', '[SEP]']\n",
            "Original: ['Hair', 'By', 'Nivine', 'in', 'eastgardens', 'fixed', 'my', 'hair', 'after', 'i', 'had', 'my', 'hair', 'cut', 'and', 'colored', 'at', 'another', 'salon', 'i', 'felt', 'more', 'confident', 'and', 'the', 'girls', 'are', 'fantastic', 'and', 'ive', 'been', 'going', 'there', 'now', 'for', '2', 'years', 'always', 'happy', 'and', 'they', 'care', 'about', 'my', 'hair', 'had', 'my', 'hair', 'done', 'for', 'my', 'wedding', 'it', 'looked', 'fabolous', '!']\n",
            "Tokenized: ['[CLS]', 'hair', 'by', 'ni', '##vine', 'in', 'east', '##gard', '##ens', 'fixed', 'my', 'hair', 'after', 'i', 'had', 'my', 'hair', 'cut', 'and', 'colored', 'at', 'another', 'salon', 'i', 'felt', 'more', 'confident', 'and', 'the', 'girls', 'are', 'fantastic', 'and', 'iv', '##e', 'been', 'going', 'there', 'now', 'for', '2', 'years', 'always', 'happy', 'and', 'they', 'care', 'about', 'my', 'hair', 'had', 'my', 'hair', 'done', 'for', 'my', 'wedding', 'it', 'looked', 'fa', '##bol', '##ous', '!', '[SEP]']\n",
            "Original: ['and', 'there', 'prices', 'are', 'really', 'good!']\n",
            "Tokenized: ['[CLS]', 'and', 'there', 'prices', 'are', 'really', 'good', '!', '[SEP]']\n",
            "Original: ['This', 'place', 'is', 'awesome']\n",
            "Tokenized: ['[CLS]', 'this', 'place', 'is', 'awesome', '[SEP]']\n",
            "Original: ['Great', 'work,', 'good', 'price.']\n",
            "Tokenized: ['[CLS]', 'great', 'work', ',', 'good', 'price', '.', '[SEP]']\n",
            "Original: ['Definetely', 'going', 'back']\n",
            "Tokenized: ['[CLS]', 'define', '##tel', '##y', 'going', 'back', '[SEP]']\n",
            "Original: ['So', 'delightful.']\n",
            "Tokenized: ['[CLS]', 'so', 'delightful', '.', '[SEP]']\n",
            "Original: ['What', 'a', 'group!']\n",
            "Tokenized: ['[CLS]', 'what', 'a', 'group', '!', '[SEP]']\n",
            "Original: ['I', \"wouldn't\", 'want', 'any', 'other', 'company', 'in', 'my', 'time', 'of', 'need.']\n",
            "Tokenized: ['[CLS]', 'i', 'wouldn', \"'\", 't', 'want', 'any', 'other', 'company', 'in', 'my', 'time', 'of', 'need', '.', '[SEP]']\n",
            "Original: ['Great', 'people!']\n",
            "Tokenized: ['[CLS]', 'great', 'people', '!', '[SEP]']\n",
            "Original: ['So', 'professional!']\n",
            "Tokenized: ['[CLS]', 'so', 'professional', '!', '[SEP]']\n",
            "Original: ['These', 'guys', 'know', 'what', \"they're\", 'doing!']\n",
            "Tokenized: ['[CLS]', 'these', 'guys', 'know', 'what', 'they', \"'\", 're', 'doing', '!', '[SEP]']\n",
            "Original: ['Way', 'to', 'go!']\n",
            "Tokenized: ['[CLS]', 'way', 'to', 'go', '!', '[SEP]']\n",
            "Original: ['This', 'is', 'not', 'your', 'usual', 'cheap', 'hotdog', 'place.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'not', 'your', 'usual', 'cheap', 'hot', '##dog', 'place', '.', '[SEP]']\n",
            "Original: ['They', 'offer', 'a', 'large', 'variety', 'of', 'quality', 'hotdogs', 'and', 'hamburgers', 'They', 'also', 'offer', 'veggie', 'dogs.']\n",
            "Tokenized: ['[CLS]', 'they', 'offer', 'a', 'large', 'variety', 'of', 'quality', 'hot', '##dog', '##s', 'and', 'hamburger', '##s', 'they', 'also', 'offer', 've', '##gg', '##ie', 'dogs', '.', '[SEP]']\n",
            "Original: ['The', 'fries', 'are', 'of', 'good', 'quality,', 'the', 'staff', 'is', 'friendly.']\n",
            "Tokenized: ['[CLS]', 'the', 'fries', 'are', 'of', 'good', 'quality', ',', 'the', 'staff', 'is', 'friendly', '.', '[SEP]']\n",
            "Original: ['The', 'atmosphere', 'is', 'your', 'typical', 'indie', 'outfit', 'with', 'old', 'movie', 'posters', 'and', 'memorabilia', 'from', 'the', \"70's\", 'and', \"80's.\"]\n",
            "Tokenized: ['[CLS]', 'the', 'atmosphere', 'is', 'your', 'typical', 'indie', 'outfit', 'with', 'old', 'movie', 'posters', 'and', 'memorabilia', 'from', 'the', '70', \"'\", 's', 'and', '80', \"'\", 's', '.', '[SEP]']\n",
            "Original: ['Kliotech']\n",
            "Tokenized: ['[CLS]', 'k', '##lio', '##tech', '[SEP]']\n",
            "Original: ['A', 'company', 'which', 'provide', 'good', 'quality', 'portals,', 'E-commerce', 'solutions,web', 'based', 'MMOG', '...', 'etc']\n",
            "Tokenized: ['[CLS]', 'a', 'company', 'which', 'provide', 'good', 'quality', 'portals', ',', 'e', '-', 'commerce', 'solutions', ',', 'web', 'based', 'mm', '##og', '.', '.', '.', 'etc', '[SEP]']\n",
            "Original: ['Poor', 'Service,', 'Lack', 'of', 'Passion:', 'Do', 'NOT', 'go']\n",
            "Tokenized: ['[CLS]', 'poor', 'service', ',', 'lack', 'of', 'passion', ':', 'do', 'not', 'go', '[SEP]']\n",
            "Original: ['I', 'began', 'seeing', 'Dr.', 'Romanick', 'back', 'in', '2000', 'and', 'have', 'seen', 'a', 'significant', 'decline', 'in', 'the', 'quality', 'of', 'care,', 'patient-doctor', 'communication,', 'and', 'just', 'the', 'overall', 'level', 'of', 'services.']\n",
            "Tokenized: ['[CLS]', 'i', 'began', 'seeing', 'dr', '.', 'romani', '##ck', 'back', 'in', '2000', 'and', 'have', 'seen', 'a', 'significant', 'decline', 'in', 'the', 'quality', 'of', 'care', ',', 'patient', '-', 'doctor', 'communication', ',', 'and', 'just', 'the', 'overall', 'level', 'of', 'services', '.', '[SEP]']\n",
            "Original: ['The', 'poor', 'quality', 'starts', 'at', 'the', 'receptionist', 'desk,', 'where', 'the', 'staff', 'is', 'very', 'impatient', 'and', 'lack', 'the', 'efficiency', 'I', 'once', 'loved', 'about', 'the', 'office.']\n",
            "Tokenized: ['[CLS]', 'the', 'poor', 'quality', 'starts', 'at', 'the', 'receptionist', 'desk', ',', 'where', 'the', 'staff', 'is', 'very', 'impatient', 'and', 'lack', 'the', 'efficiency', 'i', 'once', 'loved', 'about', 'the', 'office', '.', '[SEP]']\n",
            "Original: ['It', 'took', 'them', 'nearly', 'two', 'months', 'to', 'complete', 'a', 'simple', 'task,', 'and', 'countless', 'calls', 'from', 'my', 'part', 'due', 'to', 'their', 'lack', 'of', 'response', 'and', 'passion', '(not', 'that', \"it's\", 'a', 'requirement', 'for', 'the', 'job,', 'but', 'it', 'helps', 'to', 'at', 'least', 'pretend', 'to', 'be', 'helpful).']\n",
            "Tokenized: ['[CLS]', 'it', 'took', 'them', 'nearly', 'two', 'months', 'to', 'complete', 'a', 'simple', 'task', ',', 'and', 'countless', 'calls', 'from', 'my', 'part', 'due', 'to', 'their', 'lack', 'of', 'response', 'and', 'passion', '(', 'not', 'that', 'it', \"'\", 's', 'a', 'requirement', 'for', 'the', 'job', ',', 'but', 'it', 'helps', 'to', 'at', 'least', 'pretend', 'to', 'be', 'helpful', ')', '.', '[SEP]']\n",
            "Original: ['Also,', 'I', 'was', 'promised', 'to', 'have', 'my', 'test', 'results', 'emailed', 'to', 'me,', 'but', 'it', 'never', 'happened,', 'and', 'after', 'a', 'few', 'attempts', 'to', 'get', 'Dr.', 'Romanick', 'on', 'the', 'phone', 'to', 'brief', 'me', 'on', 'my', 'condition,', 'I', 'finally', 'gave', 'up', 'and', 'went', 'to', 'a', 'different', 'doctor.']\n",
            "Tokenized: ['[CLS]', 'also', ',', 'i', 'was', 'promised', 'to', 'have', 'my', 'test', 'results', 'email', '##ed', 'to', 'me', ',', 'but', 'it', 'never', 'happened', ',', 'and', 'after', 'a', 'few', 'attempts', 'to', 'get', 'dr', '.', 'romani', '##ck', 'on', 'the', 'phone', 'to', 'brief', 'me', 'on', 'my', 'condition', ',', 'i', 'finally', 'gave', 'up', 'and', 'went', 'to', 'a', 'different', 'doctor', '.', '[SEP]']\n",
            "Original: ['Please', 'do', 'not', 'go', 'there', 'if', \"it's\", 'professional,', 'friendly,', 'diligent', 'medical', 'services', \"you're\", 'looking', 'for.']\n",
            "Tokenized: ['[CLS]', 'please', 'do', 'not', 'go', 'there', 'if', 'it', \"'\", 's', 'professional', ',', 'friendly', ',', 'dil', '##igen', '##t', 'medical', 'services', 'you', \"'\", 're', 'looking', 'for', '.', '[SEP]']\n",
            "Original: ['Save', 'yourself', 'the', 'trouble,', 'money', 'and', 'time', 'and', 'visit', 'a', 'more', 'caring', 'facility/doctor.']\n",
            "Tokenized: ['[CLS]', 'save', 'yourself', 'the', 'trouble', ',', 'money', 'and', 'time', 'and', 'visit', 'a', 'more', 'caring', 'facility', '/', 'doctor', '.', '[SEP]']\n",
            "Original: ['Amazing', 'Experience!']\n",
            "Tokenized: ['[CLS]', 'amazing', 'experience', '!', '[SEP]']\n",
            "Original: ['My', 'experience', 'was', 'amazing', 'at', 'Providence', 'Aesthetics', 'and', 'Medical', 'Spa.']\n",
            "Tokenized: ['[CLS]', 'my', 'experience', 'was', 'amazing', 'at', 'providence', 'aesthetics', 'and', 'medical', 'spa', '.', '[SEP]']\n",
            "Original: ['Jana', 'Kueck', 'was', 'nothing', 'but', 'professional.']\n",
            "Tokenized: ['[CLS]', 'jana', 'ku', '##eck', 'was', 'nothing', 'but', 'professional', '.', '[SEP]']\n",
            "Original: ['She', 'makes', 'you', 'feel', 'like', 'you', 'are', 'the', 'most', 'important', 'person', 'in', 'the', 'world.']\n",
            "Tokenized: ['[CLS]', 'she', 'makes', 'you', 'feel', 'like', 'you', 'are', 'the', 'most', 'important', 'person', 'in', 'the', 'world', '.', '[SEP]']\n",
            "Original: ['Jana', 'made', 'me', 'feel', 'very', 'comfortable.']\n",
            "Tokenized: ['[CLS]', 'jana', 'made', 'me', 'feel', 'very', 'comfortable', '.', '[SEP]']\n",
            "Original: ['Provided', 'me', 'with', 'warm', 'blanket', 'and', 'has', 'soft', 'music', 'playing.']\n",
            "Tokenized: ['[CLS]', 'provided', 'me', 'with', 'warm', 'blanket', 'and', 'has', 'soft', 'music', 'playing', '.', '[SEP]']\n",
            "Original: ['Walking', 'in', 'the', 'door', 'you', 'are', 'made', 'to', 'feel', 'happy', 'and', 'relaxed.']\n",
            "Tokenized: ['[CLS]', 'walking', 'in', 'the', 'door', 'you', 'are', 'made', 'to', 'feel', 'happy', 'and', 'relaxed', '.', '[SEP]']\n",
            "Original: ['Equipment', 'is', 'state', 'of', 'the', 'art.']\n",
            "Tokenized: ['[CLS]', 'equipment', 'is', 'state', 'of', 'the', 'art', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'reccomend', 'anyone', 'to', 'go', 'see', 'Jana', 'Kueck', 'and', 'Robin', 'Talley', 'to', 'see', 'all', 'the', 'many', 'procedures', 'they', 'have', 'to', 'offer.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'rec', '##com', '##end', 'anyone', 'to', 'go', 'see', 'jana', 'ku', '##eck', 'and', 'robin', 'tall', '##ey', 'to', 'see', 'all', 'the', 'many', 'procedures', 'they', 'have', 'to', 'offer', '.', '[SEP]']\n",
            "Original: ['Wrong', 'Information']\n",
            "Tokenized: ['[CLS]', 'wrong', 'information', '[SEP]']\n",
            "Original: ['The', 'address', 'is', 'for', 'Noida', 'Location', 'not', 'for', 'Gurgaon', 'Location.']\n",
            "Tokenized: ['[CLS]', 'the', 'address', 'is', 'for', 'no', '##ida', 'location', 'not', 'for', 'gu', '##rga', '##on', 'location', '.', '[SEP]']\n",
            "Original: ['Please', 'update', 'this', 'listing', 'in', 'you', 'database.']\n",
            "Tokenized: ['[CLS]', 'please', 'update', 'this', 'listing', 'in', 'you', 'database', '.', '[SEP]']\n",
            "Original: ['great!', ':P']\n",
            "Tokenized: ['[CLS]', 'great', '!', ':', 'p', '[SEP]']\n",
            "Original: ['you', 'get', 'a', 'really', 'good', 'view', 'of', 'the', 'city', 'and', 'there', 'is', 'also', 'attractions', 'like', 'simulator,', 'short', 'movies.']\n",
            "Tokenized: ['[CLS]', 'you', 'get', 'a', 'really', 'good', 'view', 'of', 'the', 'city', 'and', 'there', 'is', 'also', 'attractions', 'like', 'simulator', ',', 'short', 'movies', '.', '[SEP]']\n",
            "Original: ['Try', 'the', '360', 'restraunt', 'u', 'spin', 'in', 'the', 'cn', 'tower', 'with', 'a', 'beautiful', 'view', 'the', 'sky', 'pod', 'elevator', 'is', 'about', 'an', 'hour', 'line', 'up', 'in', 'the', 'summer']\n",
            "Tokenized: ['[CLS]', 'try', 'the', '360', 'rest', '##ra', '##unt', 'u', 'spin', 'in', 'the', 'cn', 'tower', 'with', 'a', 'beautiful', 'view', 'the', 'sky', 'pod', 'elevator', 'is', 'about', 'an', 'hour', 'line', 'up', 'in', 'the', 'summer', '[SEP]']\n",
            "Original: ['Awesome', 'service', 'with', 'a', 'smile', ':)']\n",
            "Tokenized: ['[CLS]', 'awesome', 'service', 'with', 'a', 'smile', ':', ')', '[SEP]']\n",
            "Original: ['Out', 'of', 'business?']\n",
            "Tokenized: ['[CLS]', 'out', 'of', 'business', '?', '[SEP]']\n",
            "Original: ['I', 'think', 'this', 'location', 'is', 'no', 'longer', 'in', 'business.']\n",
            "Tokenized: ['[CLS]', 'i', 'think', 'this', 'location', 'is', 'no', 'longer', 'in', 'business', '.', '[SEP]']\n",
            "Original: ['If', 'you', 'check', 'RecWarehouse.com,', 'they', \"don't\", 'list', 'this', 'as', 'a', 'location.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'check', 'rec', '##ware', '##house', '.', 'com', ',', 'they', 'don', \"'\", 't', 'list', 'this', 'as', 'a', 'location', '.', '[SEP]']\n",
            "Original: [\"You'll\", 'have', 'to', 'drive', '10', 'miles', 'down', '75', 'to', 'Allen.']\n",
            "Tokenized: ['[CLS]', 'you', \"'\", 'll', 'have', 'to', 'drive', '10', 'miles', 'down', '75', 'to', 'allen', '.', '[SEP]']\n",
            "Original: ['What', 'you', 'can', 'learn', 'from', 'the', 'below', \"'bad\", \"experience'.\"]\n",
            "Tokenized: ['[CLS]', 'what', 'you', 'can', 'learn', 'from', 'the', 'below', \"'\", 'bad', 'experience', \"'\", '.', '[SEP]']\n",
            "Original: ['I', 'would', 'suggest', 'not', 'avoiding', 'Second', 'Home', 'based', 'on', 'the', \"'bad\", \"experience'\", 'review.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'suggest', 'not', 'avoiding', 'second', 'home', 'based', 'on', 'the', \"'\", 'bad', 'experience', \"'\", 'review', '.', '[SEP]']\n",
            "Original: [\"I'd\", 'probably', 'be', 'more', 'inclined', 'to', 'board', 'my', 'two', 'dogs', 'here,', 'seeing', 'that', 'they', \"don't\", 'just', 'take', 'every', 'dog', 'coming', 'in.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'd', 'probably', 'be', 'more', 'inclined', 'to', 'board', 'my', 'two', 'dogs', 'here', ',', 'seeing', 'that', 'they', 'don', \"'\", 't', 'just', 'take', 'every', 'dog', 'coming', 'in', '.', '[SEP]']\n",
            "Original: [\"I've\", 'toured', 'this', 'place', 'and', 'was', 'impressed', 'by', 'how', 'clean', 'the', 'place', 'was,', 'and', 'all', 'the', 'options', 'for', 'the', 'dogs.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 've', 'toured', 'this', 'place', 'and', 'was', 'impressed', 'by', 'how', 'clean', 'the', 'place', 'was', ',', 'and', 'all', 'the', 'options', 'for', 'the', 'dogs', '.', '[SEP]']\n",
            "Original: [\"It's\", 'unfortunate', 'that', 'bmil', 'believed', 'that', 'his', \"'perfect'\", 'dog', 'was', 'not', 'given', 'the', 'right', 'opportunity', 'to', 'prove', 'himself.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'unfortunate', 'that', 'b', '##mi', '##l', 'believed', 'that', 'his', \"'\", 'perfect', \"'\", 'dog', 'was', 'not', 'given', 'the', 'right', 'opportunity', 'to', 'prove', 'himself', '.', '[SEP]']\n",
            "Original: ['But', \"I've\", 'done', 'hundreds', 'of', 'dog', 'introductions', 'myself', '(another', 'place,', 'I', \"don't\", 'work', 'here),', 'and', 'owners', 'can', 'have', 'unrealistic', 'expectations', 'and', 'views', 'of', 'what', 'they', 'see', 'when', 'their', 'dogs', 'meet', 'other', 'dogs.']\n",
            "Tokenized: ['[CLS]', 'but', 'i', \"'\", 've', 'done', 'hundreds', 'of', 'dog', 'introductions', 'myself', '(', 'another', 'place', ',', 'i', 'don', \"'\", 't', 'work', 'here', ')', ',', 'and', 'owners', 'can', 'have', 'un', '##real', '##istic', 'expectations', 'and', 'views', 'of', 'what', 'they', 'see', 'when', 'their', 'dogs', 'meet', 'other', 'dogs', '.', '[SEP]']\n",
            "Original: ['Workers', 'who', 'do', 'these', 'introductions', 'look', 'at', 'the', 'interaction', 'objectively;', 'and', \"it's\", 'good', 'to', 'see', 'they', 'are', 'able', 'and', 'willing', 'to', 'say', 'no', 'if', 'they', 'feel', 'there', 'would', 'be', 'a', 'problem.']\n",
            "Tokenized: ['[CLS]', 'workers', 'who', 'do', 'these', 'introductions', 'look', 'at', 'the', 'interaction', 'objective', '##ly', ';', 'and', 'it', \"'\", 's', 'good', 'to', 'see', 'they', 'are', 'able', 'and', 'willing', 'to', 'say', 'no', 'if', 'they', 'feel', 'there', 'would', 'be', 'a', 'problem', '.', '[SEP]']\n",
            "Original: ['It', 'sounds', '(according', 'to', 'your', 'own', 'statement)', 'that', 'they', 'had', 'a', 'roomful', 'of', 'dogs,', 'so', 'they', 'must', 'be', 'doing', 'something', 'right', '-', 'and', 'are', 'keeping', 'those', 'dogs', 'safe', 'from', 'potential', 'problems.']\n",
            "Tokenized: ['[CLS]', 'it', 'sounds', '(', 'according', 'to', 'your', 'own', 'statement', ')', 'that', 'they', 'had', 'a', 'room', '##ful', 'of', 'dogs', ',', 'so', 'they', 'must', 'be', 'doing', 'something', 'right', '-', 'and', 'are', 'keeping', 'those', 'dogs', 'safe', 'from', 'potential', 'problems', '.', '[SEP]']\n",
            "Original: ['You', 'say', 'you', 'work', 'a', 'lot,', 'and', 'that', 'you', 'have', 'a', 'young', 'dog;', 'so', 'I', 'have', 'little', 'doubt', 'that', 'your', 'dog', 'is', 'just', 'filled', 'with', 'energy', 'to', 'burn;', 'and', 'it', 'is', 'good', 'of', 'you', 'to', 'look', 'for', 'a', 'place', 'to', 'take', 'him.']\n",
            "Tokenized: ['[CLS]', 'you', 'say', 'you', 'work', 'a', 'lot', ',', 'and', 'that', 'you', 'have', 'a', 'young', 'dog', ';', 'so', 'i', 'have', 'little', 'doubt', 'that', 'your', 'dog', 'is', 'just', 'filled', 'with', 'energy', 'to', 'burn', ';', 'and', 'it', 'is', 'good', 'of', 'you', 'to', 'look', 'for', 'a', 'place', 'to', 'take', 'him', '.', '[SEP]']\n",
            "Original: ['But', 'not', 'at', 'a', 'risk', 'to', 'other', \"people's\", 'pets.']\n",
            "Tokenized: ['[CLS]', 'but', 'not', 'at', 'a', 'risk', 'to', 'other', 'people', \"'\", 's', 'pets', '.', '[SEP]']\n",
            "Original: ['You', 'were', 'clearly', 'given', 'another', 'alternative', 'by', 'Second', 'Home,', 'to', 'board', 'him', '-', 'which', 'might', 'have', 'given', 'your', 'dog', 'a', 'chance', 'to', 'come', 'and', 'go', 'from', 'Second', 'Home', 'a', 'couple', 'times,', 'getting', 'used', 'to', 'the', 'place', 'and', 'maybe', 'facilitating', 'another', 'attempt', 'to', 'get', 'into', 'daycare', 'later.']\n",
            "Tokenized: ['[CLS]', 'you', 'were', 'clearly', 'given', 'another', 'alternative', 'by', 'second', 'home', ',', 'to', 'board', 'him', '-', 'which', 'might', 'have', 'given', 'your', 'dog', 'a', 'chance', 'to', 'come', 'and', 'go', 'from', 'second', 'home', 'a', 'couple', 'times', ',', 'getting', 'used', 'to', 'the', 'place', 'and', 'maybe', 'facilitating', 'another', 'attempt', 'to', 'get', 'into', 'day', '##care', 'later', '.', '[SEP]']\n",
            "Original: ['No', 'business', 'is', 'going', 'to', 'push', 'customers', 'away', 'without', 'good', 'reason;', 'so', \"isn't\", 'it', 'reasonable', 'to', 'think', 'they', 'might', 'know', 'what', \"they're\", 'doing?']\n",
            "Tokenized: ['[CLS]', 'no', 'business', 'is', 'going', 'to', 'push', 'customers', 'away', 'without', 'good', 'reason', ';', 'so', 'isn', \"'\", 't', 'it', 'reasonable', 'to', 'think', 'they', 'might', 'know', 'what', 'they', \"'\", 're', 'doing', '?', '[SEP]']\n",
            "Original: ['My', 'dogs', 'are', 'far', 'from', 'perfect,', 'and', 'one', 'of', 'them', 'I', 'believe', 'would', 'be', 'a', 'little', 'much', 'for', 'daycare', 'here', 'herself', '(at', 'least', 'initially).']\n",
            "Tokenized: ['[CLS]', 'my', 'dogs', 'are', 'far', 'from', 'perfect', ',', 'and', 'one', 'of', 'them', 'i', 'believe', 'would', 'be', 'a', 'little', 'much', 'for', 'day', '##care', 'here', 'herself', '(', 'at', 'least', 'initially', ')', '.', '[SEP]']\n",
            "Original: ['Be', 'a', 'little', 'more', 'reasonable', 'with', 'your', 'expectations', 'of', 'a', 'place', 'like', 'this;', 'and', 'maybe', \"don't\", 'jump', 'to', 'personal', 'attacks', 'suggesting', 'that', 'they', \"don't\", 'want', 'to', 'work', 'hard,', 'just', 'because', 'you', 'bruised', 'your', 'own', 'ego.']\n",
            "Tokenized: ['[CLS]', 'be', 'a', 'little', 'more', 'reasonable', 'with', 'your', 'expectations', 'of', 'a', 'place', 'like', 'this', ';', 'and', 'maybe', 'don', \"'\", 't', 'jump', 'to', 'personal', 'attacks', 'suggesting', 'that', 'they', 'don', \"'\", 't', 'want', 'to', 'work', 'hard', ',', 'just', 'because', 'you', 'bruised', 'your', 'own', 'ego', '.', '[SEP]']\n",
            "Original: ['Hams', 'on', 'Friendly', '...', 'RIP']\n",
            "Tokenized: ['[CLS]', 'ham', '##s', 'on', 'friendly', '.', '.', '.', 'rip', '[SEP]']\n",
            "Original: ['This', 'is', 'the', 'original', \"Ham's\", 'restaurant,', 'expanded', 'into', 'a', 'regional', 'chain', 'in', 'the', 'late', \"80's\", '--', 'but', 'this', 'one', 'is', 'no', 'more.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'the', 'original', 'ham', \"'\", 's', 'restaurant', ',', 'expanded', 'into', 'a', 'regional', 'chain', 'in', 'the', 'late', '80', \"'\", 's', '-', '-', 'but', 'this', 'one', 'is', 'no', 'more', '.', '[SEP]']\n",
            "Original: ['Victim', 'of', 'hard', 'times', 'and', 'I', 'suspect', 'failing', 'corporate', 'management.']\n",
            "Tokenized: ['[CLS]', 'victim', 'of', 'hard', 'times', 'and', 'i', 'suspect', 'failing', 'corporate', 'management', '.', '[SEP]']\n",
            "Original: ['According', 'to', 'news', 'accounts,', 'the', 'company', 'is', 'struggling.']\n",
            "Tokenized: ['[CLS]', 'according', 'to', 'news', 'accounts', ',', 'the', 'company', 'is', 'struggling', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'many', 'fond', 'memories', 'of', 'my', 'college', 'evenings', 'there', 'long', 'ago.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'many', 'fond', 'memories', 'of', 'my', 'college', 'evenings', 'there', 'long', 'ago', '.', '[SEP]']\n",
            "Original: ['So', 'long', 'Hams', '...', 'you', 'will', 'be', 'missed.']\n",
            "Tokenized: ['[CLS]', 'so', 'long', 'ham', '##s', '.', '.', '.', 'you', 'will', 'be', 'missed', '.', '[SEP]']\n",
            "Original: ['I', 'am', 'a', 'new', 'patient.']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'a', 'new', 'patient', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'visited', 'Dr.', \"Cooper's\", 'office', 'twice', 'and', 'I', 'am', 'very', 'impressed', 'with', 'how', 'friendly', 'and', 'polite', 'the', 'staff', 'is.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'visited', 'dr', '.', 'cooper', \"'\", 's', 'office', 'twice', 'and', 'i', 'am', 'very', 'impressed', 'with', 'how', 'friendly', 'and', 'polite', 'the', 'staff', 'is', '.', '[SEP]']\n",
            "Original: ['I', 'found', 'the', 'office', 'to', 'be', 'very', 'clean', 'and', 'professional-looking.']\n",
            "Tokenized: ['[CLS]', 'i', 'found', 'the', 'office', 'to', 'be', 'very', 'clean', 'and', 'professional', '-', 'looking', '.', '[SEP]']\n",
            "Original: ['Absolutely', 'the', 'best', 'little', 'motel', 'on', 'the', 'coast!']\n",
            "Tokenized: ['[CLS]', 'absolutely', 'the', 'best', 'little', 'motel', 'on', 'the', 'coast', '!', '[SEP]']\n",
            "Original: [\"I've\", 'stayed', 'at', 'this', 'fabulous', 'little', 'motel', 'two', 'years', 'running,', 'and', 'I', 'have', 'to', 'say', \"it's\", 'one', 'of', 'the', 'best', 'lodging', 'experiences', \"I've\", 'ever', 'had', 'on', 'the', 'coast...and', \"I'm\", 'even', 'comparing', 'it', 'to', 'the', 'big', 'resorts', \"I've\", 'stayed', 'at!']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 've', 'stayed', 'at', 'this', 'fabulous', 'little', 'motel', 'two', 'years', 'running', ',', 'and', 'i', 'have', 'to', 'say', 'it', \"'\", 's', 'one', 'of', 'the', 'best', 'lodging', 'experiences', 'i', \"'\", 've', 'ever', 'had', 'on', 'the', 'coast', '.', '.', '.', 'and', 'i', \"'\", 'm', 'even', 'comparing', 'it', 'to', 'the', 'big', 'resorts', 'i', \"'\", 've', 'stayed', 'at', '!', '[SEP]']\n",
            "Original: ['A', 'large', 'group', 'of', 'my', 'friends', 'and', 'I', 'rent', 'almost', 'the', 'whole', 'motel', 'every', 'year', 'for', 'a', 'weekend,', 'and', 'the', 'experience', 'and', 'stay', 'have', 'always', 'been', 'five-star.']\n",
            "Tokenized: ['[CLS]', 'a', 'large', 'group', 'of', 'my', 'friends', 'and', 'i', 'rent', 'almost', 'the', 'whole', 'motel', 'every', 'year', 'for', 'a', 'weekend', ',', 'and', 'the', 'experience', 'and', 'stay', 'have', 'always', 'been', 'five', '-', 'star', '.', '[SEP]']\n",
            "Original: ['The', 'rooms', 'are', 'SO', 'clean,', 'the', 'managers/owners', 'are', 'the', 'nicest', 'people,', 'the', 'place', 'feels', 'so', 'homey,', 'and', 'the', 'location', 'and', 'grounds', 'are', 'beautiful.']\n",
            "Tokenized: ['[CLS]', 'the', 'rooms', 'are', 'so', 'clean', ',', 'the', 'managers', '/', 'owners', 'are', 'the', 'nice', '##st', 'people', ',', 'the', 'place', 'feels', 'so', 'home', '##y', ',', 'and', 'the', 'location', 'and', 'grounds', 'are', 'beautiful', '.', '[SEP]']\n",
            "Original: [\"There's\", 'a', 'restaurant', 'nearby', '(walking', 'distance)', 'with', 'a', 'great', 'breakfast,', 'and', 'a', 'market', 'across', 'the', 'street.']\n",
            "Tokenized: ['[CLS]', 'there', \"'\", 's', 'a', 'restaurant', 'nearby', '(', 'walking', 'distance', ')', 'with', 'a', 'great', 'breakfast', ',', 'and', 'a', 'market', 'across', 'the', 'street', '.', '[SEP]']\n",
            "Original: ['The', 'motel', 'is', 'very', 'well', 'maintained,', 'and', 'the', 'managers', 'are', 'so', 'accomodating,', \"it's\", 'kind', 'of', 'like', 'visiting', 'family', 'each', 'year!', ';-)']\n",
            "Tokenized: ['[CLS]', 'the', 'motel', 'is', 'very', 'well', 'maintained', ',', 'and', 'the', 'managers', 'are', 'so', 'acc', '##omo', '##dating', ',', 'it', \"'\", 's', 'kind', 'of', 'like', 'visiting', 'family', 'each', 'year', '!', ';', '-', ')', '[SEP]']\n",
            "Original: ['I', 'honestly', \"can't\", 'rave', 'enough', 'about', 'this', \"place...it's\", 'really', 'a', 'hidden', 'gem', 'worth', 'checking', 'out!']\n",
            "Tokenized: ['[CLS]', 'i', 'honestly', 'can', \"'\", 't', 'rave', 'enough', 'about', 'this', 'place', '.', '.', '.', 'it', \"'\", 's', 'really', 'a', 'hidden', 'gem', 'worth', 'checking', 'out', '!', '[SEP]']\n",
            "Original: ['Alto', 'delivers', 'on', 'all', 'levels.']\n",
            "Tokenized: ['[CLS]', 'alto', 'delivers', 'on', 'all', 'levels', '.', '[SEP]']\n",
            "Original: ['From', 'the', 'moment', 'you', 'enter', 'the', 'restaurant,', 'you', 'know', 'you', 'are', 'some', 'place', 'special.']\n",
            "Tokenized: ['[CLS]', 'from', 'the', 'moment', 'you', 'enter', 'the', 'restaurant', ',', 'you', 'know', 'you', 'are', 'some', 'place', 'special', '.', '[SEP]']\n",
            "Original: ['The', 'service', 'is', 'impeccable,', 'and', 'the', 'food', 'is', 'even', 'better.']\n",
            "Tokenized: ['[CLS]', 'the', 'service', 'is', 'imp', '##ec', '##cable', ',', 'and', 'the', 'food', 'is', 'even', 'better', '.', '[SEP]']\n",
            "Original: ['I', 'highly', 'recommend', 'the', 'four-course', 'tasting', 'menu,', 'which', 'gives', 'you', 'plenty', 'of', 'range', 'and', 'food', 'to', 'satisfy', 'your', 'appetite.']\n",
            "Tokenized: ['[CLS]', 'i', 'highly', 'recommend', 'the', 'four', '-', 'course', 'tasting', 'menu', ',', 'which', 'gives', 'you', 'plenty', 'of', 'range', 'and', 'food', 'to', 'satisfy', 'your', 'appetite', '.', '[SEP]']\n",
            "Original: ['Also,', 'if', 'you', 'are', 'into', 'wine,', 'Alto', 'has', 'the', 'depth', 'of', 'both', 'region,', 'varietal,', 'and', 'vintage', 'to', 'satisfy', 'nearly', 'any', 'sommelier', '(or', 'after', '9pm,', 'bring', 'your', 'own', 'bottle', 'for', 'free...no', 'corking', 'fee!).']\n",
            "Tokenized: ['[CLS]', 'also', ',', 'if', 'you', 'are', 'into', 'wine', ',', 'alto', 'has', 'the', 'depth', 'of', 'both', 'region', ',', 'var', '##ie', '##tal', ',', 'and', 'vintage', 'to', 'satisfy', 'nearly', 'any', 'somme', '##lier', '(', 'or', 'after', '9', '##pm', ',', 'bring', 'your', 'own', 'bottle', 'for', 'free', '.', '.', '.', 'no', 'cork', '##ing', 'fee', '!', ')', '.', '[SEP]']\n",
            "Original: ['While', \"it's\", 'not', 'cheap,', 'Alto', 'will', 'give', 'you', 'an', 'experience', \"you'll\", 'never', 'forget.']\n",
            "Tokenized: ['[CLS]', 'while', 'it', \"'\", 's', 'not', 'cheap', ',', 'alto', 'will', 'give', 'you', 'an', 'experience', 'you', \"'\", 'll', 'never', 'forget', '.', '[SEP]']\n",
            "Original: ['DINING', 'AT', 'TEXAS', 'ROADHOUSE']\n",
            "Tokenized: ['[CLS]', 'dining', 'at', 'texas', 'road', '##house', '[SEP]']\n",
            "Original: ['TEXAS', 'ROADHOUSE', 'HAS', 'VERY', 'GOOD', 'MEALS,', 'THAT', 'THE', 'MEAT', 'COMES', 'RIGHT', 'OFF', 'THE', 'BONES.']\n",
            "Tokenized: ['[CLS]', 'texas', 'road', '##house', 'has', 'very', 'good', 'meals', ',', 'that', 'the', 'meat', 'comes', 'right', 'off', 'the', 'bones', '.', '[SEP]']\n",
            "Original: ['IT', 'HAS', 'VERY', 'GOOD', 'PRICES.']\n",
            "Tokenized: ['[CLS]', 'it', 'has', 'very', 'good', 'prices', '.', '[SEP]']\n",
            "Original: ['ON', 'A', 'BAD', 'NOTE', 'THE', 'WAITING', 'AREA', 'IS', 'NOT', 'ENJOYABLE', 'OR', 'ENOUGH', 'SEATS.']\n",
            "Tokenized: ['[CLS]', 'on', 'a', 'bad', 'note', 'the', 'waiting', 'area', 'is', 'not', 'enjoyable', 'or', 'enough', 'seats', '.', '[SEP]']\n",
            "Original: ['ALSO,', 'THERE', 'SHOULD', 'NOT', 'BE', 'PEANUTS', 'ALL', 'OVER', 'THE', 'FLOOR.']\n",
            "Tokenized: ['[CLS]', 'also', ',', 'there', 'should', 'not', 'be', 'peanuts', 'all', 'over', 'the', 'floor', '.', '[SEP]']\n",
            "Original: ['NEXT,', 'THERE', 'SHOULD', 'ONLY', 'BE', 'ONE', 'PERSON', 'BRINGING', 'YOU', 'YOUR', 'FOOD.']\n",
            "Tokenized: ['[CLS]', 'next', ',', 'there', 'should', 'only', 'be', 'one', 'person', 'bringing', 'you', 'your', 'food', '.', '[SEP]']\n",
            "Original: ['ITS', 'NOT', 'A', 'BIG', 'DEAL', 'BUT', 'I', 'HAD', 'TO', 'TAKE', 'MY', 'SALAD', 'HOME', 'BECAUSE', 'THEY', 'FORGOT', 'TO', 'BRING', 'IT.']\n",
            "Tokenized: ['[CLS]', 'its', 'not', 'a', 'big', 'deal', 'but', 'i', 'had', 'to', 'take', 'my', 'salad', 'home', 'because', 'they', 'forgot', 'to', 'bring', 'it', '.', '[SEP]']\n",
            "Original: ['I', 'HAD', 'TO', 'ASK', 'THE', 'GIRL', 'WHO', 'BROUGHT', 'MY', 'FOOD', 'AND', 'SHE', 'NEVER', 'CAME', 'BACK', 'TO', 'LET', 'ME', 'KNOW.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'to', 'ask', 'the', 'girl', 'who', 'brought', 'my', 'food', 'and', 'she', 'never', 'came', 'back', 'to', 'let', 'me', 'know', '.', '[SEP]']\n",
            "Original: ['I', 'HAD', 'TO', 'WAIT', 'FOR', 'MY', 'WAITRESS.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'to', 'wait', 'for', 'my', 'waitress', '.', '[SEP]']\n",
            "Original: ['WHEN', 'YOU', 'FIRST', 'COME', 'IN', 'THE', 'HOSTESS', 'IS', 'NOT', 'VERY', 'FRIENDLY,THERE', 'IS', 'JUST', 'A', 'BUNCH', 'OF', 'WORKERS', 'STANDING', 'THERE.']\n",
            "Tokenized: ['[CLS]', 'when', 'you', 'first', 'come', 'in', 'the', 'hostess', 'is', 'not', 'very', 'friendly', ',', 'there', 'is', 'just', 'a', 'bunch', 'of', 'workers', 'standing', 'there', '.', '[SEP]']\n",
            "Original: ['I', 'WAS', 'THERE', 'ON', 'MARCH', '6TH,', '2009.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'there', 'on', 'march', '6th', ',', '2009', '.', '[SEP]']\n",
            "Original: ['I', 'WAS', 'ALSO', 'THERE', 'OF', 'JULY', '4TH', '2008,', 'WHEN', 'MY', 'DAUGHTERS', 'BUFFALO', 'WINGS', 'CAME', 'OUT', 'WITH', 'A', 'FLY', 'ON', 'IT.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'also', 'there', 'of', 'july', '4th', '2008', ',', 'when', 'my', 'daughters', 'buffalo', 'wings', 'came', 'out', 'with', 'a', 'fly', 'on', 'it', '.', '[SEP]']\n",
            "Original: ['THE', 'MANAGER', 'CAME', 'OVER', 'AND', 'SAID', 'HE', 'WAS', 'SORRY', 'AND', 'GAVE', 'A', 'NEW', 'BATCH', 'OF', 'WINGS,', 'HE', 'SAID', 'WE', 'CANT', 'REALLY', 'DO', 'ANYTHING', 'BECAUSE', 'THE', 'DOORS', 'ARE', 'ALWAYS', 'OPENING', 'AND', 'CLOSING', '.']\n",
            "Tokenized: ['[CLS]', 'the', 'manager', 'came', 'over', 'and', 'said', 'he', 'was', 'sorry', 'and', 'gave', 'a', 'new', 'batch', 'of', 'wings', ',', 'he', 'said', 'we', 'can', '##t', 'really', 'do', 'anything', 'because', 'the', 'doors', 'are', 'always', 'opening', 'and', 'closing', '.', '[SEP]']\n",
            "Original: ['IN', 'MY', 'OPINION', 'SHOULD', 'OF', 'JUST', 'TOOK', 'OFF', 'THE', 'PRICE', 'OF', 'THE', 'WINGS', 'FROM', 'THE', 'BILL.']\n",
            "Tokenized: ['[CLS]', 'in', 'my', 'opinion', 'should', 'of', 'just', 'took', 'off', 'the', 'price', 'of', 'the', 'wings', 'from', 'the', 'bill', '.', '[SEP]']\n",
            "Original: ['BUT', 'EVERYONE', 'HAS', 'THERE', 'OWN', 'WAY!!!!!!']\n",
            "Tokenized: ['[CLS]', 'but', 'everyone', 'has', 'there', 'own', 'way', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: [\"It's\", 'now', 'called', 'Sushi', 'Lover.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'now', 'called', 'su', '##shi', 'lover', '.', '[SEP]']\n",
            "Original: ['Food', 'is', 'just', \"okay..won't\", 'crave', 'it,', 'but', \"wouldn't\", 'mind', 'coming', 'back', 'for', 'a', 'quick', 'meal.']\n",
            "Tokenized: ['[CLS]', 'food', 'is', 'just', 'okay', '.', '.', 'won', \"'\", 't', 'cr', '##ave', 'it', ',', 'but', 'wouldn', \"'\", 't', 'mind', 'coming', 'back', 'for', 'a', 'quick', 'meal', '.', '[SEP]']\n",
            "Original: ['not', 'impressive', 'enough']\n",
            "Tokenized: ['[CLS]', 'not', 'impressive', 'enough', '[SEP]']\n",
            "Original: [\"It's\", 'a', 'fine', 'place,', \"I'm\", 'just', 'a', 'little', 'mystified', 'about', 'the', 'Michelin', 'star.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'a', 'fine', 'place', ',', 'i', \"'\", 'm', 'just', 'a', 'little', 'my', '##sti', '##fied', 'about', 'the', 'michel', '##in', 'star', '.', '[SEP]']\n",
            "Original: ['Nothing', 'wrong', 'with', 'it,', 'just', 'better', 'options', 'at', 'this', 'price', 'point.']\n",
            "Tokenized: ['[CLS]', 'nothing', 'wrong', 'with', 'it', ',', 'just', 'better', 'options', 'at', 'this', 'price', 'point', '.', '[SEP]']\n",
            "Original: ['Just', 'Autos', 'Thank', 'you!']\n",
            "Tokenized: ['[CLS]', 'just', 'auto', '##s', 'thank', 'you', '!', '[SEP]']\n",
            "Original: ['Was', 'fast', 'and', 'easy,', 'Just', 'had', 'our', 'car', 'returned', 'this', 'morning,', 'I', 'would', 'recommend', 'these', 'Mobile', 'Mechanics', 'for', 'sure.']\n",
            "Tokenized: ['[CLS]', 'was', 'fast', 'and', 'easy', ',', 'just', 'had', 'our', 'car', 'returned', 'this', 'morning', ',', 'i', 'would', 'recommend', 'these', 'mobile', 'mechanics', 'for', 'sure', '.', '[SEP]']\n",
            "Original: ['They', 'certainly', 'know', 'what', 'they', 'are', 'doing.']\n",
            "Tokenized: ['[CLS]', 'they', 'certainly', 'know', 'what', 'they', 'are', 'doing', '.', '[SEP]']\n",
            "Original: ['The', 'mechanic', 'came', 'to', 'our', 'place', 'and', 'sorted', 'out', 'our', 'cars', 'problems,', 'he', 'explained', 'the', 'problem', 'and', 'was', 'very', 'up', 'front', 'and', 'honest,', 'it', 'was', 'towed', 'to', 'the', 'workshop', 'as', 'the', 'gearbox', 'was', 'not', 'working', '(he', 'explained', 'it', 'better).']\n",
            "Tokenized: ['[CLS]', 'the', 'mechanic', 'came', 'to', 'our', 'place', 'and', 'sorted', 'out', 'our', 'cars', 'problems', ',', 'he', 'explained', 'the', 'problem', 'and', 'was', 'very', 'up', 'front', 'and', 'honest', ',', 'it', 'was', 'towed', 'to', 'the', 'workshop', 'as', 'the', 'gearbox', 'was', 'not', 'working', '(', 'he', 'explained', 'it', 'better', ')', '.', '[SEP]']\n",
            "Original: ['They', 'phoned', 'the', 'same', 'day,', 'confirmed', 'it', 'was', 'the', 'gearbox', 'quoted', 'me', 'the', 'job,', 'I', 'gave', 'the', 'go', 'ahead.']\n",
            "Tokenized: ['[CLS]', 'they', 'phone', '##d', 'the', 'same', 'day', ',', 'confirmed', 'it', 'was', 'the', 'gearbox', 'quoted', 'me', 'the', 'job', ',', 'i', 'gave', 'the', 'go', 'ahead', '.', '[SEP]']\n",
            "Original: ['Now', 'my', 'cars', 'gears', 'and', 'brakes', 'have', 'never', 'run', 'so', 'well...', 'ever', 'its', 'like', 'driving', 'a', 'new', 'car.']\n",
            "Tokenized: ['[CLS]', 'now', 'my', 'cars', 'gears', 'and', 'brakes', 'have', 'never', 'run', 'so', 'well', '.', '.', '.', 'ever', 'its', 'like', 'driving', 'a', 'new', 'car', '.', '[SEP]']\n",
            "Original: ['So', 'yes,', 'I', \"wouldn't\", 'hesitate', 'in', 'recommending', 'the', 'team', 'at', 'Just', 'Autos', 'for', 'easy', 'professional', 'car', 'repairs,Thank', 'you', 'Just', 'Autos', 'for', 'your', 'help.']\n",
            "Tokenized: ['[CLS]', 'so', 'yes', ',', 'i', 'wouldn', \"'\", 't', 'hesitate', 'in', 'recommend', '##ing', 'the', 'team', 'at', 'just', 'auto', '##s', 'for', 'easy', 'professional', 'car', 'repairs', ',', 'thank', 'you', 'just', 'auto', '##s', 'for', 'your', 'help', '.', '[SEP]']\n",
            "Original: ['Skip', 'te', 'rest', '-', 'this', 'is', 'the', 'best']\n",
            "Tokenized: ['[CLS]', 'skip', 'te', 'rest', '-', 'this', 'is', 'the', 'best', '[SEP]']\n",
            "Original: ['Absolutely', 'great', '!']\n",
            "Tokenized: ['[CLS]', 'absolutely', 'great', '!', '[SEP]']\n",
            "Original: ['Clean,', 'updated', 'room,', 'friendly', 'staff,', 'safe', 'location.']\n",
            "Tokenized: ['[CLS]', 'clean', ',', 'updated', 'room', ',', 'friendly', 'staff', ',', 'safe', 'location', '.', '[SEP]']\n",
            "Original: ['Staff', 'is', 'super', 'friendly,', 'treat', 'you', 'as', 'a', 'friend.']\n",
            "Tokenized: ['[CLS]', 'staff', 'is', 'super', 'friendly', ',', 'treat', 'you', 'as', 'a', 'friend', '.', '[SEP]']\n",
            "Original: ['Cannot', 'ask', 'for', 'a', 'better', 'experience.']\n",
            "Tokenized: ['[CLS]', 'cannot', 'ask', 'for', 'a', 'better', 'experience', '.', '[SEP]']\n",
            "Original: ['Will', 'be', 'staying', 'here', 'any', 'and', 'every', 'time', 'I', 'come', 'anywhere', 'near.']\n",
            "Tokenized: ['[CLS]', 'will', 'be', 'staying', 'here', 'any', 'and', 'every', 'time', 'i', 'come', 'anywhere', 'near', '.', '[SEP]']\n",
            "Original: ['Overall,', 'Joe', 'is', 'a', 'happy', 'camper', 'who', 'has', 'found', 'a', 'great', 'spot.']\n",
            "Tokenized: ['[CLS]', 'overall', ',', 'joe', 'is', 'a', 'happy', 'camp', '##er', 'who', 'has', 'found', 'a', 'great', 'spot', '.', '[SEP]']\n",
            "Original: ['Skylight', 'repair']\n",
            "Tokenized: ['[CLS]', 'sky', '##light', 'repair', '[SEP]']\n",
            "Original: ['My', 'skylight', 'was', 'making', 'a', 'horrible', 'noise', 'when', 'the', 'wind', 'blew.']\n",
            "Tokenized: ['[CLS]', 'my', 'sky', '##light', 'was', 'making', 'a', 'horrible', 'noise', 'when', 'the', 'wind', 'blew', '.', '[SEP]']\n",
            "Original: ['James', 'Bateman', 'came', 'the', 'day', 'I', 'called', 'and', 'fixed', 'the', 'problem', 'quickly', 'and', 'efficiently.']\n",
            "Tokenized: ['[CLS]', 'james', 'bat', '##eman', 'came', 'the', 'day', 'i', 'called', 'and', 'fixed', 'the', 'problem', 'quickly', 'and', 'efficiently', '.', '[SEP]']\n",
            "Original: ['He', 'also', 'inspected', 'my', 'entie', 'roof', 'to', 'see', 'if', 'there', 'was', 'anything', 'else', 'that', 'needed', 'attention.']\n",
            "Tokenized: ['[CLS]', 'he', 'also', 'inspected', 'my', 'en', '##tie', 'roof', 'to', 'see', 'if', 'there', 'was', 'anything', 'else', 'that', 'needed', 'attention', '.', '[SEP]']\n",
            "Original: ['He', 'called', 'the', 'next', 'day', 'to', 'see', 'if', 'everything', 'was', 'to', 'my', 'satisfaction.']\n",
            "Tokenized: ['[CLS]', 'he', 'called', 'the', 'next', 'day', 'to', 'see', 'if', 'everything', 'was', 'to', 'my', 'satisfaction', '.', '[SEP]']\n",
            "Original: ['When', 'the', 'next', 'hailstorm', 'blows', 'through,', 'I', 'will', 'not', 'hesitate', 'to', 'contact', 'James', 'at', 'Team', 'Texas', 'Construction.']\n",
            "Tokenized: ['[CLS]', 'when', 'the', 'next', 'hail', '##storm', 'blows', 'through', ',', 'i', 'will', 'not', 'hesitate', 'to', 'contact', 'james', 'at', 'team', 'texas', 'construction', '.', '[SEP]']\n",
            "Original: ['Very', 'Informative', 'website', 'with', 'alot', 'of', 'good', 'work']\n",
            "Tokenized: ['[CLS]', 'very', 'inform', '##ative', 'website', 'with', 'al', '##ot', 'of', 'good', 'work', '[SEP]']\n",
            "Original: ['Close', 'to', 'my', 'house,', 'this', 'is', 'the', 'only', 'reason', 'I', 'would', 'go', 'to', 'this', 'particular', 'QT.']\n",
            "Tokenized: ['[CLS]', 'close', 'to', 'my', 'house', ',', 'this', 'is', 'the', 'only', 'reason', 'i', 'would', 'go', 'to', 'this', 'particular', 'q', '##t', '.', '[SEP]']\n",
            "Original: ['Horrible', 'tap', 'water.']\n",
            "Tokenized: ['[CLS]', 'horrible', 'tap', 'water', '.', '[SEP]']\n",
            "Original: ['But', 'no', 'other', 'complaints.']\n",
            "Tokenized: ['[CLS]', 'but', 'no', 'other', 'complaints', '.', '[SEP]']\n",
            "Original: ['Outdated', 'but', 'not', 'bad']\n",
            "Tokenized: ['[CLS]', 'outdated', 'but', 'not', 'bad', '[SEP]']\n",
            "Original: ['So', 'I', 'really', 'felt', 'like', 'this', 'place', 'was', 'extremely', 'outdated', 'especially', 'since', 'the', 'pictures', 'make', 'it', 'look', 'nice', 'and', 'modern.']\n",
            "Tokenized: ['[CLS]', 'so', 'i', 'really', 'felt', 'like', 'this', 'place', 'was', 'extremely', 'outdated', 'especially', 'since', 'the', 'pictures', 'make', 'it', 'look', 'nice', 'and', 'modern', '.', '[SEP]']\n",
            "Original: ['It', 'had', 'listed', 'that', 'there', 'was', 'a', 'hot', 'breakfast', 'but', 'all', 'this', 'meant', 'is', 'that', 'they', 'added', 'a', 'waffle', 'maker', 'to', 'the', 'common', 'continental', 'affair', 'at', 'most', 'cheap', 'hotels.']\n",
            "Tokenized: ['[CLS]', 'it', 'had', 'listed', 'that', 'there', 'was', 'a', 'hot', 'breakfast', 'but', 'all', 'this', 'meant', 'is', 'that', 'they', 'added', 'a', 'wa', '##ffle', 'maker', 'to', 'the', 'common', 'continental', 'affair', 'at', 'most', 'cheap', 'hotels', '.', '[SEP]']\n",
            "Original: ['The', 'family', 'suite', 'was', 'basically', 'two', 'rooms', 'with', 'a', 'small', 'opening', 'between', 'them', 'which', 'worked', 'great', 'for', 'us', 'because', 'we', 'were', 'two', 'families', 'traveling', 'together.']\n",
            "Tokenized: ['[CLS]', 'the', 'family', 'suite', 'was', 'basically', 'two', 'rooms', 'with', 'a', 'small', 'opening', 'between', 'them', 'which', 'worked', 'great', 'for', 'us', 'because', 'we', 'were', 'two', 'families', 'traveling', 'together', '.', '[SEP]']\n",
            "Original: ['If', 'you', 'are', 'in', 'town', 'and', 'need', 'that', 'kind', 'of', 'space', 'I', 'say', 'stay', 'here', 'but', 'if', 'you', 'are', 'looking', 'for', 'a', 'little', 'more', 'upscale', 'affair', \"don't\", 'let', 'the', 'pictures', 'fool', 'you', 'and', 'book', 'somewhere', 'else.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'are', 'in', 'town', 'and', 'need', 'that', 'kind', 'of', 'space', 'i', 'say', 'stay', 'here', 'but', 'if', 'you', 'are', 'looking', 'for', 'a', 'little', 'more', 'upscale', 'affair', 'don', \"'\", 't', 'let', 'the', 'pictures', 'fool', 'you', 'and', 'book', 'somewhere', 'else', '.', '[SEP]']\n",
            "Original: ['Terrible', 'customer', 'service']\n",
            "Tokenized: ['[CLS]', 'terrible', 'customer', 'service', '[SEP]']\n",
            "Original: ['Fried', 'rice', 'has', 'NO', 'flavor,', 'it', 'literally', 'taste', 'like', 'water.']\n",
            "Tokenized: ['[CLS]', 'fried', 'rice', 'has', 'no', 'flavor', ',', 'it', 'literally', 'taste', 'like', 'water', '.', '[SEP]']\n",
            "Original: ['When', 'I', 'tried', 'to', 'return', 'it', 'they', 'refused,', 'so', 'I', 'had', 'to', 'leave', 'without', 'a', 'refund', 'and', 'still', 'hungry.']\n",
            "Tokenized: ['[CLS]', 'when', 'i', 'tried', 'to', 'return', 'it', 'they', 'refused', ',', 'so', 'i', 'had', 'to', 'leave', 'without', 'a', 'ref', '##und', 'and', 'still', 'hungry', '.', '[SEP]']\n",
            "Original: ['Quality', 'has', 'fallen', 'over', 'the', 'years,', 'but', 'still', 'the', 'best', 'go-to', 'burger', 'place', 'on', 'the', 'East', 'Bay.']\n",
            "Tokenized: ['[CLS]', 'quality', 'has', 'fallen', 'over', 'the', 'years', ',', 'but', 'still', 'the', 'best', 'go', '-', 'to', 'burger', 'place', 'on', 'the', 'east', 'bay', '.', '[SEP]']\n",
            "Original: ['Great', 'place']\n",
            "Tokenized: ['[CLS]', 'great', 'place', '[SEP]']\n",
            "Original: ['Really', 'great', 'service', 'and', 'kind', 'staff.']\n",
            "Tokenized: ['[CLS]', 'really', 'great', 'service', 'and', 'kind', 'staff', '.', '[SEP]']\n",
            "Original: ['The', 'haircut', 'was', 'inexpensive', 'and', 'so', 'were', 'the', 'salon', 'services', '(eyebrows', 'were', 'cheap!).']\n",
            "Tokenized: ['[CLS]', 'the', 'hair', '##cut', 'was', 'inexpensive', 'and', 'so', 'were', 'the', 'salon', 'services', '(', 'eyebrows', 'were', 'cheap', '!', ')', '.', '[SEP]']\n",
            "Original: [\"It's\", 'a', 'nice,', 'relaxed', 'place', 'to', 'get', 'stuff', 'done', 'and', 'relax.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'a', 'nice', ',', 'relaxed', 'place', 'to', 'get', 'stuff', 'done', 'and', 'relax', '.', '[SEP]']\n",
            "Original: ['I', 'plan', 'on', 'going', 'again.']\n",
            "Tokenized: ['[CLS]', 'i', 'plan', 'on', 'going', 'again', '.', '[SEP]']\n",
            "Original: ['Great', 'gym', 'and', 'great', 'services.']\n",
            "Tokenized: ['[CLS]', 'great', 'gym', 'and', 'great', 'services', '.', '[SEP]']\n",
            "Original: ['A+']\n",
            "Tokenized: ['[CLS]', 'a', '+', '[SEP]']\n",
            "Original: ['Excellent', 'customer', 'service', 'and', 'honest', 'feedback.']\n",
            "Tokenized: ['[CLS]', 'excellent', 'customer', 'service', 'and', 'honest', 'feedback', '.', '[SEP]']\n",
            "Original: ['The', 'team', 'at', 'Bradley', 'Chevron', 'kept', 'my', 'car', 'running', 'for', 'well', 'past', 'its', 'expected', 'death!']\n",
            "Tokenized: ['[CLS]', 'the', 'team', 'at', 'bradley', 'che', '##vron', 'kept', 'my', 'car', 'running', 'for', 'well', 'past', 'its', 'expected', 'death', '!', '[SEP]']\n",
            "Original: ['They', 'are', 'honest', 'about', \"'immediate'\", 'concerns', 'versus', \"'recommended'\", 'repairs', 'and', 'have', 'very', 'fair', 'prices.']\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'honest', 'about', \"'\", 'immediate', \"'\", 'concerns', 'versus', \"'\", 'recommended', \"'\", 'repairs', 'and', 'have', 'very', 'fair', 'prices', '.', '[SEP]']\n",
            "Original: ['Such', 'a', 'convenient', 'location', 'as', 'well', 'with', 'coffee', 'shop', 'and', 'bradley', 'food', 'and', 'beverage', 'right', 'around', 'corner.']\n",
            "Tokenized: ['[CLS]', 'such', 'a', 'convenient', 'location', 'as', 'well', 'with', 'coffee', 'shop', 'and', 'bradley', 'food', 'and', 'beverage', 'right', 'around', 'corner', '.', '[SEP]']\n",
            "Original: ['A', 'very', 'nice', 'park.']\n",
            "Tokenized: ['[CLS]', 'a', 'very', 'nice', 'park', '.', '[SEP]']\n",
            "Original: ['The', 'architecture', 'is', 'simplz', 'splendid.']\n",
            "Tokenized: ['[CLS]', 'the', 'architecture', 'is', 'sim', '##pl', '##z', 'splendid', '.', '[SEP]']\n",
            "Original: ['Apps', '4', 'Salad', '3', 'Entree', '3.5', 'Wine', '5', '(NOV', '07)']\n",
            "Tokenized: ['[CLS]', 'apps', '4', 'salad', '3', 'en', '##tree', '3', '.', '5', 'wine', '5', '(', 'nov', '07', ')', '[SEP]']\n",
            "Original: ['Enjoyed', 'this', 'cozy', 'little', 'spot', 'with', 'a', 'group', 'of', '8', 'folks.']\n",
            "Tokenized: ['[CLS]', 'enjoyed', 'this', 'cozy', 'little', 'spot', 'with', 'a', 'group', 'of', '8', 'folks', '.', '[SEP]']\n",
            "Original: ['Service', 'was', 'excellent.']\n",
            "Tokenized: ['[CLS]', 'service', 'was', 'excellent', '.', '[SEP]']\n",
            "Original: ['Food', 'was', 'excellent.']\n",
            "Tokenized: ['[CLS]', 'food', 'was', 'excellent', '.', '[SEP]']\n",
            "Original: ['Wine', 'was', 'excellent.']\n",
            "Tokenized: ['[CLS]', 'wine', 'was', 'excellent', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'feeling', 'the', 'need', 'for', 'some', 'fish', 'so', 'I', 'had', 'the', 'salmon', 'which', 'was', 'very', 'good', 'but', 'the', 'steaks', 'looked', 'amazing', 'and', 'if', 'I', 'am', 'in', 'town', 'again,', \"I'll\", 'definitely', 'order', 'a', 'steak.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'feeling', 'the', 'need', 'for', 'some', 'fish', 'so', 'i', 'had', 'the', 'salmon', 'which', 'was', 'very', 'good', 'but', 'the', 'steak', '##s', 'looked', 'amazing', 'and', 'if', 'i', 'am', 'in', 'town', 'again', ',', 'i', \"'\", 'll', 'definitely', 'order', 'a', 'steak', '.', '[SEP]']\n",
            "Original: ['A', 'perfect', 'place', 'for', 'a', 'romantic', 'dinner.']\n",
            "Tokenized: ['[CLS]', 'a', 'perfect', 'place', 'for', 'a', 'romantic', 'dinner', '.', '[SEP]']\n",
            "Original: ['Lots', 'of', '\"pretty', 'people\"', 'dining', 'inside.']\n",
            "Tokenized: ['[CLS]', 'lots', 'of', '\"', 'pretty', 'people', '\"', 'dining', 'inside', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'a', 'great', 'facility.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'a', 'great', 'facility', '.', '[SEP]']\n",
            "Original: ['Great', 'instructors!']\n",
            "Tokenized: ['[CLS]', 'great', 'instructors', '!', '[SEP]']\n",
            "Original: ['Most', 'of', 'all,', 'my', 'daughter', 'loves', 'it!']\n",
            "Tokenized: ['[CLS]', 'most', 'of', 'all', ',', 'my', 'daughter', 'loves', 'it', '!', '[SEP]']\n",
            "Original: [\"Miami's\", 'best', 'tutoring', 'service!']\n",
            "Tokenized: ['[CLS]', 'miami', \"'\", 's', 'best', 'tutor', '##ing', 'service', '!', '[SEP]']\n",
            "Original: ['My', 'son', 'was', 'able', 'to', 'advance', 'a', 'full', 'two', 'grades', 'within', '9', 'months!']\n",
            "Tokenized: ['[CLS]', 'my', 'son', 'was', 'able', 'to', 'advance', 'a', 'full', 'two', 'grades', 'within', '9', 'months', '!', '[SEP]']\n",
            "Original: ['A', 'wonderful', 'tutoring', 'service', 'for', 'students', 'needing', 'help', 'with', 'elementary', '-', 'middle', 'school', 'work.']\n",
            "Tokenized: ['[CLS]', 'a', 'wonderful', 'tutor', '##ing', 'service', 'for', 'students', 'needing', 'help', 'with', 'elementary', '-', 'middle', 'school', 'work', '.', '[SEP]']\n",
            "Original: ['Best', 'Limo', 'Limousine', 'service', 'in', 'all', 'of', 'Dallas']\n",
            "Tokenized: ['[CLS]', 'best', 'limo', 'limousine', 'service', 'in', 'all', 'of', 'dallas', '[SEP]']\n",
            "Original: ['Great', 'Limos', 'company', 'int', 'he', 'DFW', 'fort', 'Worth', 'Metro', 'area.']\n",
            "Tokenized: ['[CLS]', 'great', 'limo', '##s', 'company', 'int', 'he', 'd', '##f', '##w', 'fort', 'worth', 'metro', 'area', '.', '[SEP]']\n",
            "Original: ['I', 'use', 'their', 'limo', 'services', 'for', 'all', 'of', 'my', 'airport', 'car', 'services', 'and', 'airport', 'transportation', 'needs']\n",
            "Tokenized: ['[CLS]', 'i', 'use', 'their', 'limo', 'services', 'for', 'all', 'of', 'my', 'airport', 'car', 'services', 'and', 'airport', 'transportation', 'needs', '[SEP]']\n",
            "Original: ['LOCATION', 'HAS', 'CLOSED.']\n",
            "Tokenized: ['[CLS]', 'location', 'has', 'closed', '.', '[SEP]']\n",
            "Original: ['HAS', 'MOVED', 'TO', '4783', 'Bay', 'Rd', 'Saginaw,', 'Michigan', '48604', '(989)755-1109']\n",
            "Tokenized: ['[CLS]', 'has', 'moved', 'to', '47', '##8', '##3', 'bay', 'rd', 'sa', '##gina', '##w', ',', 'michigan', '48', '##60', '##4', '(', '98', '##9', ')', '75', '##5', '-', '110', '##9', '[SEP]']\n",
            "Original: ['$9.62', 'excluding', 'tip', 'with', 'water', 'to', 'drink', 'for', 'the', 'buffet.']\n",
            "Tokenized: ['[CLS]', '$', '9', '.', '62', 'excluding', 'tip', 'with', 'water', 'to', 'drink', 'for', 'the', 'buffet', '.', '[SEP]']\n",
            "Original: ['Worst', 'buffet', 'period', 'by', 'far.']\n",
            "Tokenized: ['[CLS]', 'worst', 'buffet', 'period', 'by', 'far', '.', '[SEP]']\n",
            "Original: ['I', 'want', 'my', 'money', 'back!']\n",
            "Tokenized: ['[CLS]', 'i', 'want', 'my', 'money', 'back', '!', '[SEP]']\n",
            "Original: ['Internet', 'Department', 'is', 'rude', 'and', 'insulting']\n",
            "Tokenized: ['[CLS]', 'internet', 'department', 'is', 'rude', 'and', 'insulting', '[SEP]']\n",
            "Original: ['I', 'was', 'looking', 'to', 'bring', 'a', 'customer', 'to', 'their', 'lot', 'to', 'buy', 'a', 'car', 'but', 'the', 'Internet', 'salesperson', 'Last', 'name', 'is', 'Balazick', 'sent', 'me', 'this', 'email', '\"AND', 'IF', 'IT', 'WAS', 'WORTH', 'MY', 'TIME', 'I', 'WOULD', 'OF', 'BOTHERED', 'ASWERING', 'YOUR', 'QUESTIONS.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'looking', 'to', 'bring', 'a', 'customer', 'to', 'their', 'lot', 'to', 'buy', 'a', 'car', 'but', 'the', 'internet', 'sales', '##person', 'last', 'name', 'is', 'bala', '##zic', '##k', 'sent', 'me', 'this', 'email', '\"', 'and', 'if', 'it', 'was', 'worth', 'my', 'time', 'i', 'would', 'of', 'bothered', 'as', '##wer', '##ing', 'your', 'questions', '.', '[SEP]']\n",
            "Original: ['I', 'MAKE', 'MONEY', 'NOT', 'DEAL', 'WITH', 'BROKERS\"', 'Wow,', 'can', 'you', 'believe', 'in', 'todays', 'tough', 'times', 'this', 'dealership', 'would', 'be', 'looking', 'for', 'any', 'way', 'to', 'move', 'vehicles.']\n",
            "Tokenized: ['[CLS]', 'i', 'make', 'money', 'not', 'deal', 'with', 'broker', '##s', '\"', 'wow', ',', 'can', 'you', 'believe', 'in', 'today', '##s', 'tough', 'times', 'this', 'dealers', '##hip', 'would', 'be', 'looking', 'for', 'any', 'way', 'to', 'move', 'vehicles', '.', '[SEP]']\n",
            "Original: ['As', 'a', 'previous', 'Internet', 'Manager', 'I', 'would', 'deal', 'with', 'anyone', 'looking', 'to', 'buy', 'a', 'car', 'for', 'a', 'profit', 'and', 'not', 'care', 'if', 'they', 'came', 'in', 'with', 'a', 'broker.']\n",
            "Tokenized: ['[CLS]', 'as', 'a', 'previous', 'internet', 'manager', 'i', 'would', 'deal', 'with', 'anyone', 'looking', 'to', 'buy', 'a', 'car', 'for', 'a', 'profit', 'and', 'not', 'care', 'if', 'they', 'came', 'in', 'with', 'a', 'broker', '.', '[SEP]']\n",
            "Original: ['Stay', 'away', 'from', 'this', 'dealership!!!']\n",
            "Tokenized: ['[CLS]', 'stay', 'away', 'from', 'this', 'dealers', '##hip', '!', '!', '!', '[SEP]']\n",
            "Original: ['Great', 'Place!']\n",
            "Tokenized: ['[CLS]', 'great', 'place', '!', '[SEP]']\n",
            "Original: ['This', 'is', 'one', 'of', 'my', 'favorite', 'places', 'to', 'eat', 'for', 'lunch.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'one', 'of', 'my', 'favorite', 'places', 'to', 'eat', 'for', 'lunch', '.', '[SEP]']\n",
            "Original: ['They', 'offer', 'a', 'good', 'portions', 'at', 'a', 'great', 'price,', \"it's\", 'enough', 'food', 'to', 'fill', 'you', 'up,', 'but', 'you', \"don't\", 'ever', 'feel', 'like', 'you', 'ate', 'too', 'much.']\n",
            "Tokenized: ['[CLS]', 'they', 'offer', 'a', 'good', 'portions', 'at', 'a', 'great', 'price', ',', 'it', \"'\", 's', 'enough', 'food', 'to', 'fill', 'you', 'up', ',', 'but', 'you', 'don', \"'\", 't', 'ever', 'feel', 'like', 'you', 'ate', 'too', 'much', '.', '[SEP]']\n",
            "Original: ['Plus,', \"it's\", 'super', 'healthy!']\n",
            "Tokenized: ['[CLS]', 'plus', ',', 'it', \"'\", 's', 'super', 'healthy', '!', '[SEP]']\n",
            "Original: ['I', 'called', 'them', 'for', 'an', 'estimate', 'and', 'they', 'INSULTED', 'ME', 'WHEN', 'I', 'ASK', 'THEM', 'QUESTIONS.']\n",
            "Tokenized: ['[CLS]', 'i', 'called', 'them', 'for', 'an', 'estimate', 'and', 'they', 'insulted', 'me', 'when', 'i', 'ask', 'them', 'questions', '.', '[SEP]']\n",
            "Original: ['THEY', 'ARE', 'VERY', 'RUDE', 'AND', 'NASTY.']\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'very', 'rude', 'and', 'nasty', '.', '[SEP]']\n",
            "Original: ['PLEASE', \"DON'T\", 'USE', 'THIS', 'MOVING', 'COMPANY', 'IF', 'YOU', \"DON'T\", 'WANT', 'TO:', 'CRY,', 'HAVE', 'TROUBLE', 'AND', 'A', 'BAD', 'EXPERIENCE', 'ON', 'THE', 'DAY', 'OF', 'YOUR', 'MOVE.']\n",
            "Tokenized: ['[CLS]', 'please', 'don', \"'\", 't', 'use', 'this', 'moving', 'company', 'if', 'you', 'don', \"'\", 't', 'want', 'to', ':', 'cry', ',', 'have', 'trouble', 'and', 'a', 'bad', 'experience', 'on', 'the', 'day', 'of', 'your', 'move', '.', '[SEP]']\n",
            "Original: ['THEY', 'WILL', 'GIVE', 'YOU', 'A', 'LOW', 'PRICE', 'OVER', 'THE', 'PHONE', 'AND', 'ON', 'THE', 'DAY', 'OF', 'YOUR', 'MOVE', 'THEY', 'WILL', 'CHANGE', 'THE', 'PRICE,', 'I', 'GUARANTEE', 'IT.']\n",
            "Tokenized: ['[CLS]', 'they', 'will', 'give', 'you', 'a', 'low', 'price', 'over', 'the', 'phone', 'and', 'on', 'the', 'day', 'of', 'your', 'move', 'they', 'will', 'change', 'the', 'price', ',', 'i', 'guarantee', 'it', '.', '[SEP]']\n",
            "Original: ['love', 'this', 'park']\n",
            "Tokenized: ['[CLS]', 'love', 'this', 'park', '[SEP]']\n",
            "Original: ['this', 'is', 'a', 'great', 'park', 'to', 'have', 'kids', 'birthday', 'parties', 'at!!']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'a', 'great', 'park', 'to', 'have', 'kids', 'birthday', 'parties', 'at', '!', '!', '[SEP]']\n",
            "Original: ['slow', 'service']\n",
            "Tokenized: ['[CLS]', 'slow', 'service', '[SEP]']\n",
            "Original: ['I', 'used', 'to', 'take', 'my', 'cars', 'there', 'all', 'thetime,', 'but', 'management', 'changes', 'hands', 'too', 'frequently,', 'the', 'service', 'has', 'been', 'slow,', 'and', 'they', 'often', 'try', 'to', '\"add', 'on\"', 'extra', 'services,', 'which', 'sometimes', 'is', 'not', 'needed.']\n",
            "Tokenized: ['[CLS]', 'i', 'used', 'to', 'take', 'my', 'cars', 'there', 'all', 'the', '##time', ',', 'but', 'management', 'changes', 'hands', 'too', 'frequently', ',', 'the', 'service', 'has', 'been', 'slow', ',', 'and', 'they', 'often', 'try', 'to', '\"', 'add', 'on', '\"', 'extra', 'services', ',', 'which', 'sometimes', 'is', 'not', 'needed', '.', '[SEP]']\n",
            "Original: ['I', 'dont', 'go', 'there', 'anymore']\n",
            "Tokenized: ['[CLS]', 'i', 'don', '##t', 'go', 'there', 'anymore', '[SEP]']\n",
            "Original: ['Natasha', 'is', 'the', 'BEST', 'photographer', 'we', 'have', 'ever', 'worked', 'with.']\n",
            "Tokenized: ['[CLS]', 'natasha', 'is', 'the', 'best', 'photographer', 'we', 'have', 'ever', 'worked', 'with', '.', '[SEP]']\n",
            "Original: ['She', 'has', 'a', 'great', 'way', 'with', 'children', 'and', 'is', 'able', 'to', 'capture', 'their', 'personality', 'as', 'well', 'as', 'many', 'spontaneous', 'images.']\n",
            "Tokenized: ['[CLS]', 'she', 'has', 'a', 'great', 'way', 'with', 'children', 'and', 'is', 'able', 'to', 'capture', 'their', 'personality', 'as', 'well', 'as', 'many', 'spontaneous', 'images', '.', '[SEP]']\n",
            "Original: ['You', 'will', 'not', 'be', 'disappointed', 'with', 'her', 'work!!']\n",
            "Tokenized: ['[CLS]', 'you', 'will', 'not', 'be', 'disappointed', 'with', 'her', 'work', '!', '!', '[SEP]']\n",
            "Original: ['Gone', 'downhill', 'since', 'change', 'in', 'ownership']\n",
            "Tokenized: ['[CLS]', 'gone', 'downhill', 'since', 'change', 'in', 'ownership', '[SEP]']\n",
            "Original: ['Sigh.']\n",
            "Tokenized: ['[CLS]', 'sigh', '.', '[SEP]']\n",
            "Original: ['I', 'used', 'to', 'LOVE', 'this', 'place.']\n",
            "Tokenized: ['[CLS]', 'i', 'used', 'to', 'love', 'this', 'place', '.', '[SEP]']\n",
            "Original: ['But', 'now', 'that', 'they', 'are', 'part', 'of', 'a', 'chain,', 'service', 'has', 'slipped.']\n",
            "Tokenized: ['[CLS]', 'but', 'now', 'that', 'they', 'are', 'part', 'of', 'a', 'chain', ',', 'service', 'has', 'slipped', '.', '[SEP]']\n",
            "Original: ['I', 'am', 'waiting', 'longer', 'at', 'BNA', 'for', 'my', 'pickups', 'and', 'last', 'time', 'I', 'parked', 'with', 'them,', 'they', 'lost', 'my', 'car', 'key.']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'waiting', 'longer', 'at', 'bn', '##a', 'for', 'my', 'pickup', '##s', 'and', 'last', 'time', 'i', 'parked', 'with', 'them', ',', 'they', 'lost', 'my', 'car', 'key', '.', '[SEP]']\n",
            "Original: ['Lucky', 'I', 'had', 'a', 'spare', 'with', 'me!']\n",
            "Tokenized: ['[CLS]', 'lucky', 'i', 'had', 'a', 'spare', 'with', 'me', '!', '[SEP]']\n",
            "Original: ['To', 'date', 'they', 'have', 'not', 'made', 'good', 'by', 'either', 'finding', 'the', 'key', 'or', 'paying', 'for', 'a', 'new', 'one.']\n",
            "Tokenized: ['[CLS]', 'to', 'date', 'they', 'have', 'not', 'made', 'good', 'by', 'either', 'finding', 'the', 'key', 'or', 'paying', 'for', 'a', 'new', 'one', '.', '[SEP]']\n",
            "Original: ['(These', 'new', 'car', 'keys', 'are', 'EXPENSIVE', 'to', 'copy,', 'you', \"can't\", 'just', 'go', 'to', 'WalMart.)']\n",
            "Tokenized: ['[CLS]', '(', 'these', 'new', 'car', 'keys', 'are', 'expensive', 'to', 'copy', ',', 'you', 'can', \"'\", 't', 'just', 'go', 'to', 'wal', '##mart', '.', ')', '[SEP]']\n",
            "Original: ['Go', 'to', 'the', 'website', 'for', 'coupons', 'and', 'join', 'the', 'club', '-you', 'can', 'get', 'free', 'parking.']\n",
            "Tokenized: ['[CLS]', 'go', 'to', 'the', 'website', 'for', 'coup', '##ons', 'and', 'join', 'the', 'club', '-', 'you', 'can', 'get', 'free', 'parking', '.', '[SEP]']\n",
            "Original: ['However,', 'with', 'BNA', 'offering', 'one', 'day', 'of', 'free', 'parking', 'due', 'to', 'construction', 'of', 'the', 'new', 'rental', 'car', 'facility,', 'it', 'may', 'be', 'cheaper', 'to', 'park', 'at', 'BNA.']\n",
            "Tokenized: ['[CLS]', 'however', ',', 'with', 'bn', '##a', 'offering', 'one', 'day', 'of', 'free', 'parking', 'due', 'to', 'construction', 'of', 'the', 'new', 'rental', 'car', 'facility', ',', 'it', 'may', 'be', 'cheaper', 'to', 'park', 'at', 'bn', '##a', '.', '[SEP]']\n",
            "Original: ['Consistantly', 'poor']\n",
            "Tokenized: ['[CLS]', 'consist', '##antly', 'poor', '[SEP]']\n",
            "Original: ['A', 'lack', 'of', 'organisation,', 'coupled', 'with', 'the', 'distain', 'for', 'its', 'customers,', 'makes', 'this', 'the', 'worst', 'rental', 'agency', 'I', 'have', 'used.']\n",
            "Tokenized: ['[CLS]', 'a', 'lack', 'of', 'organisation', ',', 'coupled', 'with', 'the', 'di', '##sta', '##in', 'for', 'its', 'customers', ',', 'makes', 'this', 'the', 'worst', 'rental', 'agency', 'i', 'have', 'used', '.', '[SEP]']\n",
            "Original: ['Chasing', 'them', 'on', 'issues', 'from', 'the', 'day', 'I', 'moved', 'in', '(many', 'of', 'them', 'still', 'unresolved', 'as', 'I', 'left)', 'to', 'all', 'sorts', 'of', 'farcical', 'issues', 'with', 'funds,', 'after', 'I', 'left.', '.', '.', '.', 'as', 'soon', 'as', 'I', 'could.']\n",
            "Tokenized: ['[CLS]', 'chasing', 'them', 'on', 'issues', 'from', 'the', 'day', 'i', 'moved', 'in', '(', 'many', 'of', 'them', 'still', 'un', '##res', '##olved', 'as', 'i', 'left', ')', 'to', 'all', 'sorts', 'of', 'far', '##cic', '##al', 'issues', 'with', 'funds', ',', 'after', 'i', 'left', '.', '.', '.', '.', 'as', 'soon', 'as', 'i', 'could', '.', '[SEP]']\n",
            "Original: ['If', 'you', 'must', 'use', 'them,', 'be', 'vigilant', 'and', 'be', 'ready', 'to', 'push,', 'if', 'you', 'can', 'go', 'elsewhere', 'then', 'I', 'would.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'must', 'use', 'them', ',', 'be', 'vi', '##gil', '##ant', 'and', 'be', 'ready', 'to', 'push', ',', 'if', 'you', 'can', 'go', 'elsewhere', 'then', 'i', 'would', '.', '[SEP]']\n",
            "Original: ['Not', 'so', 'great']\n",
            "Tokenized: ['[CLS]', 'not', 'so', 'great', '[SEP]']\n",
            "Original: ['My', 'husband', 'just', 'got', 'a', 'bike', 'there', 'as', 'a', 'gift', \"he's\", 'only', 'had', 'it', 'a', 'month.']\n",
            "Tokenized: ['[CLS]', 'my', 'husband', 'just', 'got', 'a', 'bike', 'there', 'as', 'a', 'gift', 'he', \"'\", 's', 'only', 'had', 'it', 'a', 'month', '.', '[SEP]']\n",
            "Original: [\"It's\", 'a', 'nice', 'bike', 'and', 'it', 'cost', 'a', 'lot', 'of', 'money.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'a', 'nice', 'bike', 'and', 'it', 'cost', 'a', 'lot', 'of', 'money', '.', '[SEP]']\n",
            "Original: ['But', 'just', 'this', 'week', 'a', 'peddle', 'broke.']\n",
            "Tokenized: ['[CLS]', 'but', 'just', 'this', 'week', 'a', 'pe', '##ddle', 'broke', '.', '[SEP]']\n",
            "Original: ['He', 'took', 'it', 'back', 'and', 'they', 'would', 'not', 'honor', 'a', 'warranty', 'and', 'said', 'it', 'was', 'his', 'fault', 'because', 'of', 'his', 'shoes??']\n",
            "Tokenized: ['[CLS]', 'he', 'took', 'it', 'back', 'and', 'they', 'would', 'not', 'honor', 'a', 'warrant', '##y', 'and', 'said', 'it', 'was', 'his', 'fault', 'because', 'of', 'his', 'shoes', '?', '?', '[SEP]']\n",
            "Original: ['So', 'they', 'were', 'going', 'to', 'charge', 'him', 'for', 'new', 'peddles', 'and', 'proceeded', 'to', 'put', 'them', 'on', 'without', 'even', 'telling', 'him', 'the', 'price.']\n",
            "Tokenized: ['[CLS]', 'so', 'they', 'were', 'going', 'to', 'charge', 'him', 'for', 'new', 'pe', '##ddle', '##s', 'and', 'proceeded', 'to', 'put', 'them', 'on', 'without', 'even', 'telling', 'him', 'the', 'price', '.', '[SEP]']\n",
            "Original: ['Which', 'was', 'very', 'expensive.']\n",
            "Tokenized: ['[CLS]', 'which', 'was', 'very', 'expensive', '.', '[SEP]']\n",
            "Original: ['He', 'asked', 'for', 'a', 'different', 'pair', 'instead', 'and', 'they', 'only', 'gave', 'him', 'five', 'dollars', 'off??']\n",
            "Tokenized: ['[CLS]', 'he', 'asked', 'for', 'a', 'different', 'pair', 'instead', 'and', 'they', 'only', 'gave', 'him', 'five', 'dollars', 'off', '?', '?', '[SEP]']\n",
            "Original: ['Trek', 'is', 'not', 'so', 'great']\n",
            "Tokenized: ['[CLS]', 'trek', 'is', 'not', 'so', 'great', '[SEP]']\n",
            "Original: ['I', 'gave', 'Dr.', 'Rohatgi', '2', 'stars', 'because', 'her', 'assistant', 'was', 'very', 'pleasant.']\n",
            "Tokenized: ['[CLS]', 'i', 'gave', 'dr', '.', 'ro', '##hat', '##gi', '2', 'stars', 'because', 'her', 'assistant', 'was', 'very', 'pleasant', '.', '[SEP]']\n",
            "Original: ['However,', 'I', 'did', 'not', 'find', 'her', 'very', 'helpful', 'and', 'her', 'receptionist', 'was', 'rude.']\n",
            "Tokenized: ['[CLS]', 'however', ',', 'i', 'did', 'not', 'find', 'her', 'very', 'helpful', 'and', 'her', 'receptionist', 'was', 'rude', '.', '[SEP]']\n",
            "Original: ['Dentist', 'you', 'can', 'trust']\n",
            "Tokenized: ['[CLS]', 'dentist', 'you', 'can', 'trust', '[SEP]']\n",
            "Original: ['If', 'possible', 'I', 'try', 'the', 'services', 'on', 'myself', 'before', 'I', 'bring', 'in', 'my', 'son.']\n",
            "Tokenized: ['[CLS]', 'if', 'possible', 'i', 'try', 'the', 'services', 'on', 'myself', 'before', 'i', 'bring', 'in', 'my', 'son', '.', '[SEP]']\n",
            "Original: ['Drs.', 'Ali', 'work', 'wonders.']\n",
            "Tokenized: ['[CLS]', 'dr', '##s', '.', 'ali', 'work', 'wonders', '.', '[SEP]']\n",
            "Original: ['Neither', 'me', 'nor', 'my', 'son', \"haven't\", 'had', 'a', 'single', 'cavity', 'since', 'we', 'started', 'dental', 'care', 'there.']\n",
            "Tokenized: ['[CLS]', 'neither', 'me', 'nor', 'my', 'son', 'haven', \"'\", 't', 'had', 'a', 'single', 'cavity', 'since', 'we', 'started', 'dental', 'care', 'there', '.', '[SEP]']\n",
            "Original: ['The', 'team', 'focus', 'is', 'prevention', 'and', 'education.']\n",
            "Tokenized: ['[CLS]', 'the', 'team', 'focus', 'is', 'prevention', 'and', 'education', '.', '[SEP]']\n",
            "Original: ['That', 'alone', 'makes', 'them', 'unique.']\n",
            "Tokenized: ['[CLS]', 'that', 'alone', 'makes', 'them', 'unique', '.', '[SEP]']\n",
            "Original: ['Make', 'You', 'Feel', 'Like', 'a', 'Number']\n",
            "Tokenized: ['[CLS]', 'make', 'you', 'feel', 'like', 'a', 'number', '[SEP]']\n",
            "Original: ['Stay', 'away', 'from', 'Kids', 'First', 'West', 'Chester.']\n",
            "Tokenized: ['[CLS]', 'stay', 'away', 'from', 'kids', 'first', 'west', 'chester', '.', '[SEP]']\n",
            "Original: ['You', 'NEVER', 'get', 'a', 'human', 'on', 'the', 'phone.']\n",
            "Tokenized: ['[CLS]', 'you', 'never', 'get', 'a', 'human', 'on', 'the', 'phone', '.', '[SEP]']\n",
            "Original: [\"It's\", 'impossible', 'to', 'get', 'an', 'appointment.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'impossible', 'to', 'get', 'an', 'appointment', '.', '[SEP]']\n",
            "Original: ['If', 'you', 'get', 'caught', 'in', 'traffic', 'and', 'are', 'a', 'couple', 'minutes', 'late', 'they', 'make', 'you', 're-schedule...', 'for', '6', 'weeks', 'later.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'get', 'caught', 'in', 'traffic', 'and', 'are', 'a', 'couple', 'minutes', 'late', 'they', 'make', 'you', 're', '-', 'schedule', '.', '.', '.', 'for', '6', 'weeks', 'later', '.', '[SEP]']\n",
            "Original: ['And', 'worst', 'of', 'all', 'your', 'child', 'feels', 'like', 'a', 'number', 'not', 'a', 'patient.']\n",
            "Tokenized: ['[CLS]', 'and', 'worst', 'of', 'all', 'your', 'child', 'feels', 'like', 'a', 'number', 'not', 'a', 'patient', '.', '[SEP]']\n",
            "Original: ['Go', 'somewhere', 'else.']\n",
            "Tokenized: ['[CLS]', 'go', 'somewhere', 'else', '.', '[SEP]']\n",
            "Original: ['Perfect', 'Location', 'plus']\n",
            "Tokenized: ['[CLS]', 'perfect', 'location', 'plus', '[SEP]']\n",
            "Original: ['I', 'moved', 'into', 'the', 'Tanglewood', 'Apartments', 'in', 'late', '2008', 'and', \"it's\", 'been', 'a', 'refreshing', 'change.']\n",
            "Tokenized: ['[CLS]', 'i', 'moved', 'into', 'the', 'tangle', '##wood', 'apartments', 'in', 'late', '2008', 'and', 'it', \"'\", 's', 'been', 'a', 'refreshing', 'change', '.', '[SEP]']\n",
            "Original: ['I', 'used', 'to', 'live', 'at', 'Meadowrun', 'and', 'that', 'was', 'a', 'nightmare.']\n",
            "Tokenized: ['[CLS]', 'i', 'used', 'to', 'live', 'at', 'meadow', '##run', 'and', 'that', 'was', 'a', 'nightmare', '.', '[SEP]']\n",
            "Original: ['The', 'manager', '-', 'Tiffany', '-', 'is', 'actually', 'very', 'nice', 'so', \"I'ma\", 'bit', 'surprised', 'by', 'other', 'comments.']\n",
            "Tokenized: ['[CLS]', 'the', 'manager', '-', 'tiffany', '-', 'is', 'actually', 'very', 'nice', 'so', 'i', \"'\", 'ma', 'bit', 'surprised', 'by', 'other', 'comments', '.', '[SEP]']\n",
            "Original: [\"She's\", 'very', 'reachable', 'and', 'she', 'has', 'always', 'responded', 'quickly', 'to', 'any', 'questions', 'or', 'requests.']\n",
            "Tokenized: ['[CLS]', 'she', \"'\", 's', 'very', 'reach', '##able', 'and', 'she', 'has', 'always', 'responded', 'quickly', 'to', 'any', 'questions', 'or', 'requests', '.', '[SEP]']\n",
            "Original: ['Plus', 'she', 'plans', 'a', 'monthly', 'breakfasts', 'and', 'other', 'events', 'at', 'the', 'clubhouse', 'which', 'is', 'a', 'nice', 'added', 'benefit.']\n",
            "Tokenized: ['[CLS]', 'plus', 'she', 'plans', 'a', 'monthly', 'breakfast', '##s', 'and', 'other', 'events', 'at', 'the', 'clubhouse', 'which', 'is', 'a', 'nice', 'added', 'benefit', '.', '[SEP]']\n",
            "Original: ['Old', 'time', 'grocery,', 'best', 'steaks', 'I', 'have', 'ever', 'had!']\n",
            "Tokenized: ['[CLS]', 'old', 'time', 'grocery', ',', 'best', 'steak', '##s', 'i', 'have', 'ever', 'had', '!', '[SEP]']\n",
            "Original: ['Great', 'meats', 'that', 'are', 'already', 'cooked,', 'easy', 'to', 'take', 'home', 'for', 'dinner.']\n",
            "Tokenized: ['[CLS]', 'great', 'meat', '##s', 'that', 'are', 'already', 'cooked', ',', 'easy', 'to', 'take', 'home', 'for', 'dinner', '.', '[SEP]']\n",
            "Original: ['Dr.', 'White', 'is', 'the', 'best!']\n",
            "Tokenized: ['[CLS]', 'dr', '.', 'white', 'is', 'the', 'best', '!', '[SEP]']\n",
            "Original: ['I', 'am', 'in', 'love', 'with', 'the', 'giant', 'plate', 'of', 'nachos!']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'in', 'love', 'with', 'the', 'giant', 'plate', 'of', 'na', '##cho', '##s', '!', '[SEP]']\n",
            "Original: ['Last', 'time', 'I', 'went', 'however,', 'my', 'beer', 'was', 'warm', 'and', 'the', 'service', 'was', 'so-so.']\n",
            "Tokenized: ['[CLS]', 'last', 'time', 'i', 'went', 'however', ',', 'my', 'beer', 'was', 'warm', 'and', 'the', 'service', 'was', 'so', '-', 'so', '.', '[SEP]']\n",
            "Original: ['I', 'get', 'that', 'careless', 'teenager', 'kind', 'of', 'treatment', 'from', 'some', 'of', 'their', 'staff...perhaps', 'they', 'should', 'hire', 'more', 'serious', 'adults', 'to', 'help', 'serve/cook.']\n",
            "Tokenized: ['[CLS]', 'i', 'get', 'that', 'careless', 'teenager', 'kind', 'of', 'treatment', 'from', 'some', 'of', 'their', 'staff', '.', '.', '.', 'perhaps', 'they', 'should', 'hire', 'more', 'serious', 'adults', 'to', 'help', 'serve', '/', 'cook', '.', '[SEP]']\n",
            "Original: ['The', 'best', 'pilates', 'on', 'the', 'Gold', 'Coast!']\n",
            "Tokenized: ['[CLS]', 'the', 'best', 'pi', '##lates', 'on', 'the', 'gold', 'coast', '!', '[SEP]']\n",
            "Original: ['My', 'Favorite', 'in', 'McLean']\n",
            "Tokenized: ['[CLS]', 'my', 'favorite', 'in', 'mclean', '[SEP]']\n",
            "Original: ['My', 'family', 'loves', 'coming', 'to', 'Endo', 'Sushi.They', 'are', 'very', 'nice,', 'it', 'is', 'never', 'crowded,', 'and', 'the', 'food', 'is', 'wonderful,', 'very', 'delicious', 'and', 'fresh!']\n",
            "Tokenized: ['[CLS]', 'my', 'family', 'loves', 'coming', 'to', 'end', '##o', 'su', '##shi', '.', 'they', 'are', 'very', 'nice', ',', 'it', 'is', 'never', 'crowded', ',', 'and', 'the', 'food', 'is', 'wonderful', ',', 'very', 'delicious', 'and', 'fresh', '!', '[SEP]']\n",
            "Original: ['Sometimes', 'it', 'is', 'hard', 'to', 'get', 'parking', 'in', 'the', 'lot', 'in', 'front', 'of', 'the', 'storefront,', 'and', 'it', 'is', 'on', 'a', 'one-way', 'street,', 'but', 'the', 'restaurant', 'itself', 'is', 'NEVER', 'overcrowded.']\n",
            "Tokenized: ['[CLS]', 'sometimes', 'it', 'is', 'hard', 'to', 'get', 'parking', 'in', 'the', 'lot', 'in', 'front', 'of', 'the', 'store', '##front', ',', 'and', 'it', 'is', 'on', 'a', 'one', '-', 'way', 'street', ',', 'but', 'the', 'restaurant', 'itself', 'is', 'never', 'over', '##crow', '##ded', '.', '[SEP]']\n",
            "Original: ['If', 'you', 'cannot', 'park', 'in', 'the', 'lot,', 'then', 'you', 'can', 'park', 'in', 'the', 'shopping', \"center's\", 'garage,', 'and', 'walk', 'up', 'to', 'Endo', 'Sushi.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'cannot', 'park', 'in', 'the', 'lot', ',', 'then', 'you', 'can', 'park', 'in', 'the', 'shopping', 'center', \"'\", 's', 'garage', ',', 'and', 'walk', 'up', 'to', 'end', '##o', 'su', '##shi', '.', '[SEP]']\n",
            "Original: ['The', 'service', 'is', 'fast.']\n",
            "Tokenized: ['[CLS]', 'the', 'service', 'is', 'fast', '.', '[SEP]']\n",
            "Original: ['Be', 'sure', 'to', 'ring', 'the', 'little', 'bell', 'on', 'your', 'way', 'out', 'if', 'you', 'enjoyed', 'your', 'meal!']\n",
            "Tokenized: ['[CLS]', 'be', 'sure', 'to', 'ring', 'the', 'little', 'bell', 'on', 'your', 'way', 'out', 'if', 'you', 'enjoyed', 'your', 'meal', '!', '[SEP]']\n",
            "Original: [\"(it's\", 'by', 'the', 'door,', 'on', 'the', 'hostess', 'stand)']\n",
            "Tokenized: ['[CLS]', '(', 'it', \"'\", 's', 'by', 'the', 'door', ',', 'on', 'the', 'hostess', 'stand', ')', '[SEP]']\n",
            "Original: ['Love', 'my', 'home', 'at', 'Creekside']\n",
            "Tokenized: ['[CLS]', 'love', 'my', 'home', 'at', 'creeks', '##ide', '[SEP]']\n",
            "Original: ['I', 'moved', 'to', 'Creekside', 'Apartments', 'in', 'August', '2008', 'with', 'a', '6-month', 'lease', 'and', 'have', 'just', 'extended', 'it', 'for', 'another', '13', 'months!']\n",
            "Tokenized: ['[CLS]', 'i', 'moved', 'to', 'creeks', '##ide', 'apartments', 'in', 'august', '2008', 'with', 'a', '6', '-', 'month', 'lease', 'and', 'have', 'just', 'extended', 'it', 'for', 'another', '13', 'months', '!', '[SEP]']\n",
            "Original: ['The', 'management', 'from', 'Julie', 'and', 'Janice', 'to', 'the', 'work', 'staff,', 'esp.', 'Edwin,', 'are', 'just', 'wonderful.']\n",
            "Tokenized: ['[CLS]', 'the', 'management', 'from', 'julie', 'and', 'janice', 'to', 'the', 'work', 'staff', ',', 'es', '##p', '.', 'edwin', ',', 'are', 'just', 'wonderful', '.', '[SEP]']\n",
            "Original: ['They', 'have', 'been', 'extremely', 'helpful', 'whenever', 'I', 'have', 'asked', 'for', 'help.']\n",
            "Tokenized: ['[CLS]', 'they', 'have', 'been', 'extremely', 'helpful', 'whenever', 'i', 'have', 'asked', 'for', 'help', '.', '[SEP]']\n",
            "Original: ['Although', 'the', 'apartments', 'are', 'facing', 'Stokes', 'Street', 'and', 'close', 'to', 'the', 'Bascom', 'Light', 'Rail,', 'the', 'location', 'is', 'surprisingly', 'quiet.']\n",
            "Tokenized: ['[CLS]', 'although', 'the', 'apartments', 'are', 'facing', 'stokes', 'street', 'and', 'close', 'to', 'the', 'bas', '##com', 'light', 'rail', ',', 'the', 'location', 'is', 'surprisingly', 'quiet', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'never', 'had', 'any', 'problems', 'with', 'loud', 'neighbors', 'or', 'concerns', 'about', 'safety.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'never', 'had', 'any', 'problems', 'with', 'loud', 'neighbors', 'or', 'concerns', 'about', 'safety', '.', '[SEP]']\n",
            "Original: ['The', 'apartments', 'are', 'within', 'walking', 'distance', 'to', 'Trader', \"Joe's,\", 'Whole', 'Foods,', 'and', 'other', 'stores.']\n",
            "Tokenized: ['[CLS]', 'the', 'apartments', 'are', 'within', 'walking', 'distance', 'to', 'trader', 'joe', \"'\", 's', ',', 'whole', 'foods', ',', 'and', 'other', 'stores', '.', '[SEP]']\n",
            "Original: ['The', 'well-equipped,', 'clean', 'gym', 'is', 'a', 'plus!']\n",
            "Tokenized: ['[CLS]', 'the', 'well', '-', 'equipped', ',', 'clean', 'gym', 'is', 'a', 'plus', '!', '[SEP]']\n",
            "Original: ['Great', 'place', 'to', 'live', 'if', 'you', 'work', 'in', 'and', 'around', 'downtown', 'San', 'Jose!']\n",
            "Tokenized: ['[CLS]', 'great', 'place', 'to', 'live', 'if', 'you', 'work', 'in', 'and', 'around', 'downtown', 'san', 'jose', '!', '[SEP]']\n",
            "Original: ['Midtown', 'Reston', 'has', 'great', 'location', 'and', 'luxurious', 'environment.']\n",
            "Tokenized: ['[CLS]', 'midtown', 'rest', '##on', 'has', 'great', 'location', 'and', 'luxurious', 'environment', '.', '[SEP]']\n",
            "Original: ['Really', 'enjoyed', 'it.']\n",
            "Tokenized: ['[CLS]', 'really', 'enjoyed', 'it', '.', '[SEP]']\n",
            "Original: ['Great', 'atmosphere,', 'great', 'food.']\n",
            "Tokenized: ['[CLS]', 'great', 'atmosphere', ',', 'great', 'food', '.', '[SEP]']\n",
            "Original: ['Definitely', 'a', 'must.']\n",
            "Tokenized: ['[CLS]', 'definitely', 'a', 'must', '.', '[SEP]']\n",
            "Original: ['Great', 'Dude', 'Cut!']\n",
            "Tokenized: ['[CLS]', 'great', 'dude', 'cut', '!', '[SEP]']\n",
            "Original: ['Great', 'service,', 'cool', 'vibe,', 'impeccable', 'style.']\n",
            "Tokenized: ['[CLS]', 'great', 'service', ',', 'cool', 'vibe', ',', 'imp', '##ec', '##cable', 'style', '.', '[SEP]']\n",
            "Original: [\"I'ma\", 'guy', 'with', 'tricky', 'hair', 'so', 'getting', 'that', 'right', 'is', 'job', '#1.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'ma', 'guy', 'with', 'tricky', 'hair', 'so', 'getting', 'that', 'right', 'is', 'job', '#', '1', '.', '[SEP]']\n",
            "Original: ['After', 'going', 'through', '5', 'other', 'places', 'I', 'finally', 'found', 'Janice', 'at', 'Alta', 'Moda.']\n",
            "Tokenized: ['[CLS]', 'after', 'going', 'through', '5', 'other', 'places', 'i', 'finally', 'found', 'janice', 'at', 'alta', 'mod', '##a', '.', '[SEP]']\n",
            "Original: ['Not', 'only', 'was', 'it', 'a', 'good', 'cut', 'but', 'my', 'wife', 'and', 'friends', 'comment', 'on', 'my', 'hair', 'every', 'time', 'I', 'leave...saying', \"it's\", 'the', 'best', 'look', \"I've\", 'ever', 'had.']\n",
            "Tokenized: ['[CLS]', 'not', 'only', 'was', 'it', 'a', 'good', 'cut', 'but', 'my', 'wife', 'and', 'friends', 'comment', 'on', 'my', 'hair', 'every', 'time', 'i', 'leave', '.', '.', '.', 'saying', 'it', \"'\", 's', 'the', 'best', 'look', 'i', \"'\", 've', 'ever', 'had', '.', '[SEP]']\n",
            "Original: ['Either', 'I', 'suck', 'at', 'my', 'own', 'style', 'or', 'Janice', 'is', 'a', 'genius.']\n",
            "Tokenized: ['[CLS]', 'either', 'i', 'suck', 'at', 'my', 'own', 'style', 'or', 'janice', 'is', 'a', 'genius', '.', '[SEP]']\n",
            "Original: ['Looks', 'like', \"there's\", 'a', 'lot', 'of', 'talent', 'in', 'this', 'place.']\n",
            "Tokenized: ['[CLS]', 'looks', 'like', 'there', \"'\", 's', 'a', 'lot', 'of', 'talent', 'in', 'this', 'place', '.', '[SEP]']\n",
            "Original: ['Worth', 'every', 'penny.']\n",
            "Tokenized: ['[CLS]', 'worth', 'every', 'penny', '.', '[SEP]']\n",
            "Original: ['Great', 'Sanwiches,', 'Great', 'Prices']\n",
            "Tokenized: ['[CLS]', 'great', 'san', '##wich', '##es', ',', 'great', 'prices', '[SEP]']\n",
            "Original: ['I', 'used', 'to', 'go', 'here', 'almost', 'everyday', 'since', 'I', 'work', 'in', 'the', 'neighbourhood', 'and', 'loved', 'their', 'turkey', 'and', 'meatball', 'sandwiches.']\n",
            "Tokenized: ['[CLS]', 'i', 'used', 'to', 'go', 'here', 'almost', 'everyday', 'since', 'i', 'work', 'in', 'the', 'neighbourhood', 'and', 'loved', 'their', 'turkey', 'and', 'meat', '##ball', 'sandwiches', '.', '[SEP]']\n",
            "Original: ['Chicken', 'salad', 'salad', 'is', 'great', 'too.']\n",
            "Tokenized: ['[CLS]', 'chicken', 'salad', 'salad', 'is', 'great', 'too', '.', '[SEP]']\n",
            "Original: ['Best', 'of', 'all,', 'the', 'staff', 'is', 'quick', 'on', 'their', 'feet', 'and', 'even', 'with', 'long', 'lines,', 'usually', 'serve', 'you', 'in', '5', 'minutes', 'or', 'less.']\n",
            "Tokenized: ['[CLS]', 'best', 'of', 'all', ',', 'the', 'staff', 'is', 'quick', 'on', 'their', 'feet', 'and', 'even', 'with', 'long', 'lines', ',', 'usually', 'serve', 'you', 'in', '5', 'minutes', 'or', 'less', '.', '[SEP]']\n",
            "Original: ['For', 'the', 'quality,', 'the', 'prices', '($4-$6)', 'have', 'to', 'be', 'the', 'best', 'in', 'town.']\n",
            "Tokenized: ['[CLS]', 'for', 'the', 'quality', ',', 'the', 'prices', '(', '$', '4', '-', '$', '6', ')', 'have', 'to', 'be', 'the', 'best', 'in', 'town', '.', '[SEP]']\n",
            "Original: ['The', 'staff', 'get', 'to', 'know', 'regulars', 'and', 'do', 'their', 'job', 'very', 'well.']\n",
            "Tokenized: ['[CLS]', 'the', 'staff', 'get', 'to', 'know', 'regulars', 'and', 'do', 'their', 'job', 'very', 'well', '.', '[SEP]']\n",
            "Original: ['Tourists', 'like', 'the', 'other', 'reviewer', 'might', 'not', 'appreciate', 'their', 'efficiency', 'or', 'quality,', 'but', 'I', 'certainly', 'do.']\n",
            "Tokenized: ['[CLS]', 'tourists', 'like', 'the', 'other', 'reviewer', 'might', 'not', 'appreciate', 'their', 'efficiency', 'or', 'quality', ',', 'but', 'i', 'certainly', 'do', '.', '[SEP]']\n",
            "Original: ['This', \"isn'ta\", 'TGIF', 'or', 'Cafe,', 'its', 'a', 'lunch', 'sandwich', 'place', 'and', 'a', 'good', 'one', 'at', 'that.']\n",
            "Tokenized: ['[CLS]', 'this', 'isn', \"'\", 'ta', 't', '##gi', '##f', 'or', 'cafe', ',', 'its', 'a', 'lunch', 'sandwich', 'place', 'and', 'a', 'good', 'one', 'at', 'that', '.', '[SEP]']\n",
            "Original: ['AWFUL', 'SERVICE!']\n",
            "Tokenized: ['[CLS]', 'awful', 'service', '!', '[SEP]']\n",
            "Original: ['After', 'happily', 'visiting', \"Sear's\", 'Automotives', 'in', 'the', 'past,', 'I', 'was', 'shocked', 'at', 'the', 'horrible', 'service', 'received', 'at', 'their', 'Greensboro', 'location.']\n",
            "Tokenized: ['[CLS]', 'after', 'happily', 'visiting', 'sea', '##r', \"'\", 's', 'automotive', '##s', 'in', 'the', 'past', ',', 'i', 'was', 'shocked', 'at', 'the', 'horrible', 'service', 'received', 'at', 'their', 'greensboro', 'location', '.', '[SEP]']\n",
            "Original: ['I', 'brought', 'my', 'car', 'in', 'on', 'a', 'Sunday', 'to', 'replace', 'a', 'shredded', 'tire.']\n",
            "Tokenized: ['[CLS]', 'i', 'brought', 'my', 'car', 'in', 'on', 'a', 'sunday', 'to', 'replace', 'a', 'shredded', 'tire', '.', '[SEP]']\n",
            "Original: ['I', 'waited', 'about', '20', 'minutes', 'in', 'the', 'store', 'part', 'before', 'anyone', 'was', 'able', 'to', 'assist', 'me', 'and', 'was', 'then', 'told', 'to', 'pull', 'my', 'car', 'into', 'the', 'shop', '(that', 'is', 'apparently', 'what', 'you', 'are', 'supposed', 'to', 'do,', 'but', 'the', 'big', 'signs', 'pointing', 'you', 'that', 'way', 'are', 'for', 'some', 'reason', 'kept', 'inside', 'the', 'garage,', 'so', 'you', \"don't\", 'see', 'them', 'drving', 'up,', 'and', 'they', 'purposely', 'block', 'the', 'front', 'pull-up', 'that', 'all', 'other', \"Sear's\", 'use).']\n",
            "Tokenized: ['[CLS]', 'i', 'waited', 'about', '20', 'minutes', 'in', 'the', 'store', 'part', 'before', 'anyone', 'was', 'able', 'to', 'assist', 'me', 'and', 'was', 'then', 'told', 'to', 'pull', 'my', 'car', 'into', 'the', 'shop', '(', 'that', 'is', 'apparently', 'what', 'you', 'are', 'supposed', 'to', 'do', ',', 'but', 'the', 'big', 'signs', 'pointing', 'you', 'that', 'way', 'are', 'for', 'some', 'reason', 'kept', 'inside', 'the', 'garage', ',', 'so', 'you', 'don', \"'\", 't', 'see', 'them', 'dr', '##ving', 'up', ',', 'and', 'they', 'purposely', 'block', 'the', 'front', 'pull', '-', 'up', 'that', 'all', 'other', 'sea', '##r', \"'\", 's', 'use', ')', '.', '[SEP]']\n",
            "Original: ['Once', 'inside,', 'I', 'had', 'to', 'stand', 'around', 'for', 'at', 'least', '10', 'more', 'minutes', 'before--FINALLY--', 'a', 'technician', 'got', 'to', 'me.']\n",
            "Tokenized: ['[CLS]', 'once', 'inside', ',', 'i', 'had', 'to', 'stand', 'around', 'for', 'at', 'least', '10', 'more', 'minutes', 'before', '-', '-', 'finally', '-', '-', 'a', 'technician', 'got', 'to', 'me', '.', '[SEP]']\n",
            "Original: ['Once', 'I', 'returned', 'to', 'pick', 'up', 'my', 'car,', 'you', 'can', 'believe', 'I', 'spent', 'quite', 'a', 'bit', 'MORE', 'time', 'standing', 'around', 'waiting.']\n",
            "Tokenized: ['[CLS]', 'once', 'i', 'returned', 'to', 'pick', 'up', 'my', 'car', ',', 'you', 'can', 'believe', 'i', 'spent', 'quite', 'a', 'bit', 'more', 'time', 'standing', 'around', 'waiting', '.', '[SEP]']\n",
            "Original: ['I', 'had', 'wanted', 'to', 'split', 'the', 'total', 'between', 'a', 'credit', 'card', 'and', 'check', 'card', 'since', 'I', 'was', 'being', 'reimbursed', 'for', 'the', 'tire', 'but', 'was', 'told', 'this', \"wasn't\", 'possible.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'wanted', 'to', 'split', 'the', 'total', 'between', 'a', 'credit', 'card', 'and', 'check', 'card', 'since', 'i', 'was', 'being', 'rei', '##mb', '##urse', '##d', 'for', 'the', 'tire', 'but', 'was', 'told', 'this', 'wasn', \"'\", 't', 'possible', '.', '[SEP]']\n",
            "Original: ['Once', 'I', 'actually', 'got', 'back', 'in', 'my', 'car,', 'it', 'was', 'dirty', 'and', 'had', 'grease', 'all', 'over', 'the', 'steering', 'wheel.']\n",
            "Tokenized: ['[CLS]', 'once', 'i', 'actually', 'got', 'back', 'in', 'my', 'car', ',', 'it', 'was', 'dirty', 'and', 'had', 'grease', 'all', 'over', 'the', 'steering', 'wheel', '.', '[SEP]']\n",
            "Original: ['OK,', 'one', 'bad', 'experience...fine.']\n",
            "Tokenized: ['[CLS]', 'ok', ',', 'one', 'bad', 'experience', '.', '.', '.', 'fine', '.', '[SEP]']\n",
            "Original: ['The', 'following', 'Friday,', 'I', 'returned', 'with', 'my', 'car', 'to', 'go', 'ahead', 'and', 'replace', 'the', 'other', '3', 'tires,', 'which', 'were', 'worn.']\n",
            "Tokenized: ['[CLS]', 'the', 'following', 'friday', ',', 'i', 'returned', 'with', 'my', 'car', 'to', 'go', 'ahead', 'and', 'replace', 'the', 'other', '3', 'tires', ',', 'which', 'were', 'worn', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'not', 'have', 'gone', 'back,', 'but', 'I', \"couldn't\", 'find', 'the', 'particular', 'tire', \"they'd\", 'used', 'in', 'stock', 'anywhere', 'else.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'not', 'have', 'gone', 'back', ',', 'but', 'i', 'couldn', \"'\", 't', 'find', 'the', 'particular', 'tire', 'they', \"'\", 'd', 'used', 'in', 'stock', 'anywhere', 'else', '.', '[SEP]']\n",
            "Original: ['Once', 'again,', 'I', 'waited', 'for', 'quite', 'a', 'bit', 'before', 'being', 'attended', 'to.']\n",
            "Tokenized: ['[CLS]', 'once', 'again', ',', 'i', 'waited', 'for', 'quite', 'a', 'bit', 'before', 'being', 'attended', 'to', '.', '[SEP]']\n",
            "Original: ['I', 'got', 'the', 'order', 'completed,', 'and', 'then', 'questioned', 'the', 'technician', 'since', 'it', 'came', 'out', 'about', '$40', 'less', 'than', 'I', 'expected.']\n",
            "Tokenized: ['[CLS]', 'i', 'got', 'the', 'order', 'completed', ',', 'and', 'then', 'questioned', 'the', 'technician', 'since', 'it', 'came', 'out', 'about', '$', '40', 'less', 'than', 'i', 'expected', '.', '[SEP]']\n",
            "Original: ['He', 'said', 'it', 'was', 'the', 'same', 'tire,', 'and', 'verified', 'this,', 'after', 'checking', 'both', 'the', 'actual', 'tire', 'on', 'my', 'car', 'and', 'my', 'service', 'papers', 'from', 'earlier', 'in', 'the', 'week.']\n",
            "Tokenized: ['[CLS]', 'he', 'said', 'it', 'was', 'the', 'same', 'tire', ',', 'and', 'verified', 'this', ',', 'after', 'checking', 'both', 'the', 'actual', 'tire', 'on', 'my', 'car', 'and', 'my', 'service', 'papers', 'from', 'earlier', 'in', 'the', 'week', '.', '[SEP]']\n",
            "Original: ['However,', 'when', 'he', 'printed', 'out', 'the', 'service', 'quote,', 'I', 'could', 'see', 'that', 'it', 'was', 'NOT', 'the', 'correct', 'tire,', 'and', 'was', 'not', 'even', 'an', 'appropriate', 'tire', 'for', 'my', 'car', 'model.']\n",
            "Tokenized: ['[CLS]', 'however', ',', 'when', 'he', 'printed', 'out', 'the', 'service', 'quote', ',', 'i', 'could', 'see', 'that', 'it', 'was', 'not', 'the', 'correct', 'tire', ',', 'and', 'was', 'not', 'even', 'an', 'appropriate', 'tire', 'for', 'my', 'car', 'model', '.', '[SEP]']\n",
            "Original: ['So', 'I', 'pointed', 'this', 'out', 'to', 'him,', 'at', 'which', 'point', 'he', 'said', 'they', 'only', 'had', 'one', 'of', 'the', 'correct', 'tires', 'in', 'stock.']\n",
            "Tokenized: ['[CLS]', 'so', 'i', 'pointed', 'this', 'out', 'to', 'him', ',', 'at', 'which', 'point', 'he', 'said', 'they', 'only', 'had', 'one', 'of', 'the', 'correct', 'tires', 'in', 'stock', '.', '[SEP]']\n",
            "Original: ['Ok--fine.']\n",
            "Tokenized: ['[CLS]', 'ok', '-', '-', 'fine', '.', '[SEP]']\n",
            "Original: ['So', 'I', 'got', 'just', 'my', 'other', 'rear', 'tire', 'replaced.']\n",
            "Tokenized: ['[CLS]', 'so', 'i', 'got', 'just', 'my', 'other', 'rear', 'tire', 'replaced', '.', '[SEP]']\n",
            "Original: ['They', 'promised', \"it'd\", 'be', 'done', 'within', 'an', 'hour,', 'so', 'I', 'waited', 'in', 'the', 'lobby.']\n",
            "Tokenized: ['[CLS]', 'they', 'promised', 'it', \"'\", 'd', 'be', 'done', 'within', 'an', 'hour', ',', 'so', 'i', 'waited', 'in', 'the', 'lobby', '.', '[SEP]']\n",
            "Original: ['Over', 'two', 'hours', 'later', '(and', 'ten', 'minutes', 'before', 'they', 'closed)', 'my', 'car', 'was', 'finally', 'finished.']\n",
            "Tokenized: ['[CLS]', 'over', 'two', 'hours', 'later', '(', 'and', 'ten', 'minutes', 'before', 'they', 'closed', ')', 'my', 'car', 'was', 'finally', 'finished', '.', '[SEP]']\n",
            "Original: ['A', 'few', 'minutes', 'after', 'I', 'left,', 'I', 'was', 'called', 'and', 'informed', 'that', '\"I\"', 'left', 'my', 'wheel', 'lock', '(which', 'they', 'should', 'have', 'left', 'in', 'the', 'car).']\n",
            "Tokenized: ['[CLS]', 'a', 'few', 'minutes', 'after', 'i', 'left', ',', 'i', 'was', 'called', 'and', 'informed', 'that', '\"', 'i', '\"', 'left', 'my', 'wheel', 'lock', '(', 'which', 'they', 'should', 'have', 'left', 'in', 'the', 'car', ')', '.', '[SEP]']\n",
            "Original: ['Of', 'course,', 'they', 'would', 'be', 'closing', 'in', '5', 'minutes,', 'so', 'I', 'would', 'have', 'to', 'hurry', 'up', 'or', 'get', 'it', 'the', 'next', 'day.']\n",
            "Tokenized: ['[CLS]', 'of', 'course', ',', 'they', 'would', 'be', 'closing', 'in', '5', 'minutes', ',', 'so', 'i', 'would', 'have', 'to', 'hurry', 'up', 'or', 'get', 'it', 'the', 'next', 'day', '.', '[SEP]']\n",
            "Original: ['Of', 'course', 'I', \"couldn't\", 'make', 'it', 'back', 'in', 'time', '(and', 'they', 'apparently', 'could', 'not', 'stay', '5', 'extra', 'minutes', 'to', 'wait', 'for', 'me).']\n",
            "Tokenized: ['[CLS]', 'of', 'course', 'i', 'couldn', \"'\", 't', 'make', 'it', 'back', 'in', 'time', '(', 'and', 'they', 'apparently', 'could', 'not', 'stay', '5', 'extra', 'minutes', 'to', 'wait', 'for', 'me', ')', '.', '[SEP]']\n",
            "Original: ['The', 'next', 'day,', 'no', 'one', 'could', 'find', 'my', 'wheel', 'lock', 'and', 'that', 'particular', 'technician', 'was', 'not', 'in.']\n",
            "Tokenized: ['[CLS]', 'the', 'next', 'day', ',', 'no', 'one', 'could', 'find', 'my', 'wheel', 'lock', 'and', 'that', 'particular', 'technician', 'was', 'not', 'in', '.', '[SEP]']\n",
            "Original: ['Of', 'course,', 'they', \"couldn't\", 'call', 'him', 'either', 'to', 'ask', 'about', 'it', 'because', 'apparently', 'they', \"don't\", 'keep', 'their', \"employees'\", 'phone', 'numbers', '(riiight),', 'so', 'I', 'would', 'have', 'to', 'return', 'on', 'Monday', '(driving', 'for', '3', 'days', 'now', 'with', 'no', 'wheel', 'lock', 'should', 'I', 'get', 'a', 'flat).']\n",
            "Tokenized: ['[CLS]', 'of', 'course', ',', 'they', 'couldn', \"'\", 't', 'call', 'him', 'either', 'to', 'ask', 'about', 'it', 'because', 'apparently', 'they', 'don', \"'\", 't', 'keep', 'their', 'employees', \"'\", 'phone', 'numbers', '(', 'ri', '##ii', '##ght', ')', ',', 'so', 'i', 'would', 'have', 'to', 'return', 'on', 'monday', '(', 'driving', 'for', '3', 'days', 'now', 'with', 'no', 'wheel', 'lock', 'should', 'i', 'get', 'a', 'flat', ')', '.', '[SEP]']\n",
            "Original: ['On', 'Monday', 'I', 'called', 'and', 'again', 'it', 'was', 'a', 'big', 'to-do', 'to', 'find', 'anyone', 'who', 'knew', 'anything', 'about', 'it.']\n",
            "Tokenized: ['[CLS]', 'on', 'monday', 'i', 'called', 'and', 'again', 'it', 'was', 'a', 'big', 'to', '-', 'do', 'to', 'find', 'anyone', 'who', 'knew', 'anything', 'about', 'it', '.', '[SEP]']\n",
            "Original: ['Supposedly', 'they', 'will', 'be', 'holding', 'it', 'for', 'me', 'this', 'evening,', 'but', \"I'm\", 'sure', 'that', 'will', 'also', 'be', 'a', 'huge', 'ordeal.']\n",
            "Tokenized: ['[CLS]', 'supposedly', 'they', 'will', 'be', 'holding', 'it', 'for', 'me', 'this', 'evening', ',', 'but', 'i', \"'\", 'm', 'sure', 'that', 'will', 'also', 'be', 'a', 'huge', 'ordeal', '.', '[SEP]']\n",
            "Original: ['The', 'employees', 'at', 'this', \"Sear's\", 'are', 'completely', 'apathetic', 'and', 'there', \"didn't\", 'seem', 'to', 'be', 'any', 'sort', 'of', 'management', 'that', 'I', 'could', 'see.']\n",
            "Tokenized: ['[CLS]', 'the', 'employees', 'at', 'this', 'sea', '##r', \"'\", 's', 'are', 'completely', 'ap', '##ath', '##etic', 'and', 'there', 'didn', \"'\", 't', 'seem', 'to', 'be', 'any', 'sort', 'of', 'management', 'that', 'i', 'could', 'see', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'never', 'return', 'there', 'again', '(and', 'now', 'have', 'some', 'serious', 'doubts', 'about', 'the', 'quality', 'of', 'work', 'they', 'actually', 'performed', 'on', 'my', 'car).']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'never', 'return', 'there', 'again', '(', 'and', 'now', 'have', 'some', 'serious', 'doubts', 'about', 'the', 'quality', 'of', 'work', 'they', 'actually', 'performed', 'on', 'my', 'car', ')', '.', '[SEP]']\n",
            "Original: ['Great', 'Wine', '&', 'Service']\n",
            "Tokenized: ['[CLS]', 'great', 'wine', '&', 'service', '[SEP]']\n",
            "Original: ['This', 'place', 'is', 'so', 'great!']\n",
            "Tokenized: ['[CLS]', 'this', 'place', 'is', 'so', 'great', '!', '[SEP]']\n",
            "Original: ['They', 'have', 'a', 'great', 'selection', 'of', 'wine', 'from', 'all', 'over', 'the', 'world', 'with', 'all', 'different', 'prices.']\n",
            "Tokenized: ['[CLS]', 'they', 'have', 'a', 'great', 'selection', 'of', 'wine', 'from', 'all', 'over', 'the', 'world', 'with', 'all', 'different', 'prices', '.', '[SEP]']\n",
            "Original: ['The', 'employees', 'make', 'you', 'feel', 'very', 'comfortable', 'and', 'are', 'very', 'helpful,', 'whether', 'you', 'are', 'very', 'knowledgeable', 'or', \"don't\", 'know', 'anything', 'at', 'all', 'about', 'wine.']\n",
            "Tokenized: ['[CLS]', 'the', 'employees', 'make', 'you', 'feel', 'very', 'comfortable', 'and', 'are', 'very', 'helpful', ',', 'whether', 'you', 'are', 'very', 'knowledge', '##able', 'or', 'don', \"'\", 't', 'know', 'anything', 'at', 'all', 'about', 'wine', '.', '[SEP]']\n",
            "Original: ['Check', 'out', 'their', 'wine', 'tastings', 'every', 'Friday', 'night!']\n",
            "Tokenized: ['[CLS]', 'check', 'out', 'their', 'wine', 'tasting', '##s', 'every', 'friday', 'night', '!', '[SEP]']\n",
            "Original: ['Summary:', 'Not', 'cheep,', 'but', 'very', 'fast,', 'and', 'super', 'friendly', 'service.']\n",
            "Tokenized: ['[CLS]', 'summary', ':', 'not', 'che', '##ep', ',', 'but', 'very', 'fast', ',', 'and', 'super', 'friendly', 'service', '.', '[SEP]']\n",
            "Original: ['Quality', 'of', 'work', 'is', 'sufficient', 'but', 'not', 'outstanding.']\n",
            "Tokenized: ['[CLS]', 'quality', 'of', 'work', 'is', 'sufficient', 'but', 'not', 'outstanding', '.', '[SEP]']\n",
            "Original: ['Like', 'all', 'oil', 'place', 'changes,', 'ask/recommend', 'the', '100', 'other', 'services', 'they', 'have.']\n",
            "Tokenized: ['[CLS]', 'like', 'all', 'oil', 'place', 'changes', ',', 'ask', '/', 'recommend', 'the', '100', 'other', 'services', 'they', 'have', '.', '[SEP]']\n",
            "Original: ['Will', 'be', 'a', 'repeat', 'customer', 'with', 'discount', 'coupons.']\n",
            "Tokenized: ['[CLS]', 'will', 'be', 'a', 'repeat', 'customer', 'with', 'discount', 'coup', '##ons', '.', '[SEP]']\n",
            "Original: ['Finest??']\n",
            "Tokenized: ['[CLS]', 'finest', '?', '?', '[SEP]']\n",
            "Original: ['Really??']\n",
            "Tokenized: ['[CLS]', 'really', '?', '?', '[SEP]']\n",
            "Original: ['I', 'beg', 'to', 'differ.']\n",
            "Tokenized: ['[CLS]', 'i', 'beg', 'to', 'differ', '.', '[SEP]']\n",
            "Original: ['This', 'place', 'is', 'marginal', 'at', 'best.']\n",
            "Tokenized: ['[CLS]', 'this', 'place', 'is', 'marginal', 'at', 'best', '.', '[SEP]']\n",
            "Original: ['Not', 'very', 'welcoming', 'and', 'focused', 'mostly', 'on', 'keeping', 'little', 'kids', 'entertained.']\n",
            "Tokenized: ['[CLS]', 'not', 'very', 'welcoming', 'and', 'focused', 'mostly', 'on', 'keeping', 'little', 'kids', 'entertained', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'not', 'impressed,', 'and', 'quite', 'frustrated', 'at', 'their', 'lack', 'of', 'rating', 'for', 'their', 'courses.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'not', 'impressed', ',', 'and', 'quite', 'frustrated', 'at', 'their', 'lack', 'of', 'rating', 'for', 'their', 'courses', '.', '[SEP]']\n",
            "Original: ['I', 'understand', 'not', 'wanting', 'to', 'put', 'labels', 'like', '5.10', 'on', 'an', 'indoor', 'course,', 'because', 'yes,', 'it', 'is', 'not', 'the', 'same,', 'but', 'some', 'clear', 'understanding', 'of', 'the', 'difficulty', 'of', 'one', 'course', 'to', 'another', 'is', 'nice', 'when', 'you', 'are', 'an', 'intermediate', 'climber', 'looking', 'to', 'improve.']\n",
            "Tokenized: ['[CLS]', 'i', 'understand', 'not', 'wanting', 'to', 'put', 'labels', 'like', '5', '.', '10', 'on', 'an', 'indoor', 'course', ',', 'because', 'yes', ',', 'it', 'is', 'not', 'the', 'same', ',', 'but', 'some', 'clear', 'understanding', 'of', 'the', 'difficulty', 'of', 'one', 'course', 'to', 'another', 'is', 'nice', 'when', 'you', 'are', 'an', 'intermediate', 'climb', '##er', 'looking', 'to', 'improve', '.', '[SEP]']\n",
            "Original: ['I', \"don't\", 'want', 'to', 'waste', 'my', 'time', 'on', 'routes', 'set', 'for', 'children,', 'but', 'I', \"don't\", 'want', 'to', 'take', 'on', 'something', 'I', \"can't\", 'handle', 'just', 'to', 'strain', 'myself', 'to', 'exhaustion.']\n",
            "Tokenized: ['[CLS]', 'i', 'don', \"'\", 't', 'want', 'to', 'waste', 'my', 'time', 'on', 'routes', 'set', 'for', 'children', ',', 'but', 'i', 'don', \"'\", 't', 'want', 'to', 'take', 'on', 'something', 'i', 'can', \"'\", 't', 'handle', 'just', 'to', 'strain', 'myself', 'to', 'exhaustion', '.', '[SEP]']\n",
            "Original: ['Rate', 'the', 'routes,', 'with', 'understandable', 'markings', 'and', 'a', 'more', 'detailed', 'system', 'than', 'easy,', 'moderate,', 'and', 'hard.']\n",
            "Tokenized: ['[CLS]', 'rate', 'the', 'routes', ',', 'with', 'understand', '##able', 'markings', 'and', 'a', 'more', 'detailed', 'system', 'than', 'easy', ',', 'moderate', ',', 'and', 'hard', '.', '[SEP]']\n",
            "Original: ['Again,', 'a', 'great', 'outing', 'for', 'the', 'kids,', 'a', 'frustration', 'for', 'an', 'out', 'of', 'town', 'climber.']\n",
            "Tokenized: ['[CLS]', 'again', ',', 'a', 'great', 'outing', 'for', 'the', 'kids', ',', 'a', 'frustration', 'for', 'an', 'out', 'of', 'town', 'climb', '##er', '.', '[SEP]']\n",
            "Original: ['DONT', 'ever', 'go', 'there,', 'not', 'even', 'if', 'your', 'car', 'flips.']\n",
            "Tokenized: ['[CLS]', 'don', '##t', 'ever', 'go', 'there', ',', 'not', 'even', 'if', 'your', 'car', 'flip', '##s', '.', '[SEP]']\n",
            "Original: ['Their', 'service', 'sucks', 'to', 'start', 'off', 'with,', 'people', 'are', 'cruel', 'and', 'ignorant.']\n",
            "Tokenized: ['[CLS]', 'their', 'service', 'sucks', 'to', 'start', 'off', 'with', ',', 'people', 'are', 'cruel', 'and', 'ignorant', '.', '[SEP]']\n",
            "Original: [\"I'm\", 'compeltly', 'dissatisfied', 'with', 'their', 'service', 'and', 'their', 'products.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'm', 'com', '##pel', '##tly', 'dissatisfied', 'with', 'their', 'service', 'and', 'their', 'products', '.', '[SEP]']\n",
            "Original: ['They', 'DO', 'NOT', 'have', 'a', 'return', 'policy', 'and', 'even', 'if', 'their', 'product', 'SUCKS,', 'they', 'will', 'NOT', 'take', 'it', 'back!']\n",
            "Tokenized: ['[CLS]', 'they', 'do', 'not', 'have', 'a', 'return', 'policy', 'and', 'even', 'if', 'their', 'product', 'sucks', ',', 'they', 'will', 'not', 'take', 'it', 'back', '!', '[SEP]']\n",
            "Original: ['DO', 'NOT', 'EVER', 'GO', 'HERE.']\n",
            "Tokenized: ['[CLS]', 'do', 'not', 'ever', 'go', 'here', '.', '[SEP]']\n",
            "Original: ['I', 'prefer', 'Advanced', 'auto', 'parts', 'over', 'this', 'crappy', 'place', 'with', 'the', 'meanest', 'people.']\n",
            "Tokenized: ['[CLS]', 'i', 'prefer', 'advanced', 'auto', 'parts', 'over', 'this', 'crap', '##py', 'place', 'with', 'the', 'mean', '##est', 'people', '.', '[SEP]']\n",
            "Original: ['And', 'they', 'THRIVE', 'to', 'get', 'a', 'customer.']\n",
            "Tokenized: ['[CLS]', 'and', 'they', 'thrive', 'to', 'get', 'a', 'customer', '.', '[SEP]']\n",
            "Original: ['DO', 'NOT', 'go', 'here..thank', 'you']\n",
            "Tokenized: ['[CLS]', 'do', 'not', 'go', 'here', '.', '.', 'thank', 'you', '[SEP]']\n",
            "Original: ['Food', 'was', 'cold']\n",
            "Tokenized: ['[CLS]', 'food', 'was', 'cold', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'here', '3', 'to', '4', 'times', 'and', 'every', 'time', 'food', 'they', 'served', 'seems', 'warmed', 'up', 'not', 'cooked', 'after', 'you', 'order', 'it.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'here', '3', 'to', '4', 'times', 'and', 'every', 'time', 'food', 'they', 'served', 'seems', 'warmed', 'up', 'not', 'cooked', 'after', 'you', 'order', 'it', '.', '[SEP]']\n",
            "Original: ['I', 'like', 'my', 'food', 'hot', 'both', 'ways', 'not', 'warm', 'or', 'cold.']\n",
            "Tokenized: ['[CLS]', 'i', 'like', 'my', 'food', 'hot', 'both', 'ways', 'not', 'warm', 'or', 'cold', '.', '[SEP]']\n",
            "Original: ['Price', 'and', 'taste', 'is', 'good.']\n",
            "Tokenized: ['[CLS]', 'price', 'and', 'taste', 'is', 'good', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'be', 'happy', 'if', 'they', 'can', 'serve', 'when', 'food', 'is', 'piping', 'hot.']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'be', 'happy', 'if', 'they', 'can', 'serve', 'when', 'food', 'is', 'pip', '##ing', 'hot', '.', '[SEP]']\n",
            "Original: ['KB']\n",
            "Tokenized: ['[CLS]', 'kb', '[SEP]']\n",
            "Original: ['Not', 'impressed.']\n",
            "Tokenized: ['[CLS]', 'not', 'impressed', '.', '[SEP]']\n",
            "Original: ['Overpriced', 'and', 'the', 'doctor', 'acted', 'arrogant', 'and', 'rushed', 'at', 'a', 'time', 'when', 'there', 'was', 'very', 'few', 'clients', 'in', 'the', 'facility.']\n",
            "Tokenized: ['[CLS]', 'over', '##pr', '##ice', '##d', 'and', 'the', 'doctor', 'acted', 'arrogant', 'and', 'rushed', 'at', 'a', 'time', 'when', 'there', 'was', 'very', 'few', 'clients', 'in', 'the', 'facility', '.', '[SEP]']\n",
            "Original: ['I', \"won't\", 'return.']\n",
            "Tokenized: ['[CLS]', 'i', 'won', \"'\", 't', 'return', '.', '[SEP]']\n",
            "Original: ['Wonderful', 'staff', 'and', 'great', 'service', '!!']\n",
            "Tokenized: ['[CLS]', 'wonderful', 'staff', 'and', 'great', 'service', '!', '!', '[SEP]']\n",
            "Original: ['Class', 'act.']\n",
            "Tokenized: ['[CLS]', 'class', 'act', '.', '[SEP]']\n",
            "Original: ['It', 'was', 'late', 'in', 'the', 'day', 'and', 'I', 'was', 'worried', 'I', 'would', 'get', 'charged', 'an', 'arm', 'and', 'a', 'leg', 'and', 'have', 'to', 'wait', 'forever.']\n",
            "Tokenized: ['[CLS]', 'it', 'was', 'late', 'in', 'the', 'day', 'and', 'i', 'was', 'worried', 'i', 'would', 'get', 'charged', 'an', 'arm', 'and', 'a', 'leg', 'and', 'have', 'to', 'wait', 'forever', '.', '[SEP]']\n",
            "Original: ['They', 'picked', 'my', 'car', 'up', 'in', 'Yarmouth', 'and', 'towed', 'to', 'Bath', 'for', 'a', 'great', 'price.']\n",
            "Tokenized: ['[CLS]', 'they', 'picked', 'my', 'car', 'up', 'in', 'yarmouth', 'and', 'towed', 'to', 'bath', 'for', 'a', 'great', 'price', '.', '[SEP]']\n",
            "Original: ['Would', 'do', 'business', 'with', 'them', 'again.']\n",
            "Tokenized: ['[CLS]', 'would', 'do', 'business', 'with', 'them', 'again', '.', '[SEP]']\n",
            "Original: ['PAT', 'testing', 'quick', '&', 'efficient']\n",
            "Tokenized: ['[CLS]', 'pat', 'testing', 'quick', '&', 'efficient', '[SEP]']\n",
            "Original: ['I', 'rang', 'SRD', 'PAT', 'testing', 'and', 'within', '3', 'hours', 'Scot', 'had', 'come', 'to', 'my', 'premises', 'and', 'PAT', 'tested', 'all', 'my', 'our', 'Spill', 'The', 'Whisky', 'barn', 'dance', 'band', 'equipment,', 'and', 'supplied', 'a', 'certificate', 'for', 'only', '70p', 'per', 'unit.']\n",
            "Tokenized: ['[CLS]', 'i', 'rang', 'sr', '##d', 'pat', 'testing', 'and', 'within', '3', 'hours', 'sc', '##ot', 'had', 'come', 'to', 'my', 'premises', 'and', 'pat', 'tested', 'all', 'my', 'our', 'spill', 'the', 'whisky', 'barn', 'dance', 'band', 'equipment', ',', 'and', 'supplied', 'a', 'certificate', 'for', 'only', '70', '##p', 'per', 'unit', '.', '[SEP]']\n",
            "Original: ['Fantastic,', 'quick', 'and', 'efficient', 'service.']\n",
            "Tokenized: ['[CLS]', 'fantastic', ',', 'quick', 'and', 'efficient', 'service', '.', '[SEP]']\n",
            "Original: ['How', 'rare!']\n",
            "Tokenized: ['[CLS]', 'how', 'rare', '!', '[SEP]']\n",
            "Original: ['Well', 'done.']\n",
            "Tokenized: ['[CLS]', 'well', 'done', '.', '[SEP]']\n",
            "Original: ['HEAVEN', 'ON', 'EARTHHHHHHH!!!!']\n",
            "Tokenized: ['[CLS]', 'heaven', 'on', 'earth', '##hh', '##hh', '##hh', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['MUST', 'TRY!!!']\n",
            "Tokenized: ['[CLS]', 'must', 'try', '!', '!', '!', '[SEP]']\n",
            "Original: ['A+++']\n",
            "Tokenized: ['[CLS]', 'a', '+', '+', '+', '[SEP]']\n",
            "Original: ['THIS', 'PLACE', 'IS', 'THE', 'BEST.']\n",
            "Tokenized: ['[CLS]', 'this', 'place', 'is', 'the', 'best', '.', '[SEP]']\n",
            "Original: ['I', 'HATED', 'SUSHI', 'BEFORE', 'BUT', 'NOW', 'I', 'CANT', 'STOP', 'EATTING', 'IT!!!!!!!']\n",
            "Tokenized: ['[CLS]', 'i', 'hated', 'su', '##shi', 'before', 'but', 'now', 'i', 'can', '##t', 'stop', 'eat', '##ting', 'it', '!', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['NICE', 'SERVICE,', 'AND', 'EXCELLENT', 'FOOD.']\n",
            "Tokenized: ['[CLS]', 'nice', 'service', ',', 'and', 'excellent', 'food', '.', '[SEP]']\n",
            "Original: ['EVERYTHING', 'IN', 'HERE', 'SEEMS', 'TO', 'AMAZE', 'ME!!!']\n",
            "Tokenized: ['[CLS]', 'everything', 'in', 'here', 'seems', 'to', 'ama', '##ze', 'me', '!', '!', '!', '[SEP]']\n",
            "Original: ['THEIR', 'GRILL', 'DISHES', 'ARE', 'OUTTA', 'THIS', 'WORLD', 'AND', 'SUSHI', 'IS', 'JUST', 'FABULOUS!!!!']\n",
            "Tokenized: ['[CLS]', 'their', 'grill', 'dishes', 'are', 'outta', 'this', 'world', 'and', 'su', '##shi', 'is', 'just', 'fabulous', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['I', 'EAT', 'HERE', 'AT', 'LEAST', '5', 'DAYS', 'A', 'WEEK.']\n",
            "Tokenized: ['[CLS]', 'i', 'eat', 'here', 'at', 'least', '5', 'days', 'a', 'week', '.', '[SEP]']\n",
            "Original: ['THEY', 'HAVE', 'EXCELLENT', 'SUSHI', 'CHEF', 'SPECIAL', 'ROLLS', 'FOR', 'A', 'FAIR', 'PRICE', 'AND', 'SO', 'IS', 'THE', 'GRILL', 'ORDERS.']\n",
            "Tokenized: ['[CLS]', 'they', 'have', 'excellent', 'su', '##shi', 'chef', 'special', 'rolls', 'for', 'a', 'fair', 'price', 'and', 'so', 'is', 'the', 'grill', 'orders', '.', '[SEP]']\n",
            "Original: ['A++++', '!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!']\n",
            "Tokenized: ['[CLS]', 'a', '+', '+', '+', '+', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['JUNO', 'AND', 'OPEN', 'I', 'LOVE', 'YOU', 'GUYS!!!!!!!!!!!!!!']\n",
            "Tokenized: ['[CLS]', 'juno', 'and', 'open', 'i', 'love', 'you', 'guys', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['[no', 'homo]', ':D']\n",
            "Tokenized: ['[CLS]', '[', 'no', 'homo', ']', ':', 'd', '[SEP]']\n",
            "Original: ['Trust', 'The', 'Midas', 'Touch']\n",
            "Tokenized: ['[CLS]', 'trust', 'the', 'mid', '##as', 'touch', '[SEP]']\n",
            "Original: ['I', 'personally', 'trust', 'this', 'Midas', 'store', 'with', 'all', 'my', 'vehicles,', 'I', 'have', 'been', 'going', 'there', 'for', 'years', '&', 'would', 'never', 'go', 'anywhere', 'else!']\n",
            "Tokenized: ['[CLS]', 'i', 'personally', 'trust', 'this', 'mid', '##as', 'store', 'with', 'all', 'my', 'vehicles', ',', 'i', 'have', 'been', 'going', 'there', 'for', 'years', '&', 'would', 'never', 'go', 'anywhere', 'else', '!', '[SEP]']\n",
            "Original: ['the', 'staff', 'is', 'very', 'personable', '&', 'actually', 'care', 'about', 'the', 'customers', 'safety', 'rather', 'than', 'taking', 'there', 'money.']\n",
            "Tokenized: ['[CLS]', 'the', 'staff', 'is', 'very', 'persona', '##ble', '&', 'actually', 'care', 'about', 'the', 'customers', 'safety', 'rather', 'than', 'taking', 'there', 'money', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'however', 'a', 'very', 'busy', 'shop', 'but', 'there', 'are', 'appointments', 'available', '&', 'the', 'staff', 'up', 'front', 'will', 'surely', 'make', 'sure', 'you', 'get', 'back', 'in', 'a', 'timely', 'manner.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'however', 'a', 'very', 'busy', 'shop', 'but', 'there', 'are', 'appointments', 'available', '&', 'the', 'staff', 'up', 'front', 'will', 'surely', 'make', 'sure', 'you', 'get', 'back', 'in', 'a', 'timely', 'manner', '.', '[SEP]']\n",
            "Original: ['I', 'look', 'at', 'some', 'of', 'these', 'other', 'comments', '&', 'laugh', 'because', 'people', 'think', 'that', 'the', 'world', 'revolves', 'around', 'them!']\n",
            "Tokenized: ['[CLS]', 'i', 'look', 'at', 'some', 'of', 'these', 'other', 'comments', '&', 'laugh', 'because', 'people', 'think', 'that', 'the', 'world', 'revolves', 'around', 'them', '!', '[SEP]']\n",
            "Original: ['Like', 'the', 'girl', 'with', 'the', 'fuse', 'problem...']\n",
            "Tokenized: ['[CLS]', 'like', 'the', 'girl', 'with', 'the', 'fuse', 'problem', '.', '.', '.', '[SEP]']\n",
            "Original: ['Midas', 'has', 'the', 'most', 'high', 'tech', 'equipment', 'in', 'town', '&', 'I', 'guarantee', 'you', 'if', 'they', 'told', 'you', 'it', 'was', 'electrical', 'then', 'in', 'deed', 'its', 'electrical!']\n",
            "Tokenized: ['[CLS]', 'mid', '##as', 'has', 'the', 'most', 'high', 'tech', 'equipment', 'in', 'town', '&', 'i', 'guarantee', 'you', 'if', 'they', 'told', 'you', 'it', 'was', 'electrical', 'then', 'in', 'deed', 'its', 'electrical', '!', '[SEP]']\n",
            "Original: ['maybe', 'you', 'should', 'understand', 'how', 'the', 'world', 'works', '&', 'realize', 'you', 'are', 'just', 'like', 'any', 'other', 'person', '&', 'not', 'put', 'yourself', 'on', 'a', 'pedestal.']\n",
            "Tokenized: ['[CLS]', 'maybe', 'you', 'should', 'understand', 'how', 'the', 'world', 'works', '&', 'realize', 'you', 'are', 'just', 'like', 'any', 'other', 'person', '&', 'not', 'put', 'yourself', 'on', 'a', 'pedestal', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'continue', 'going', 'to', 'Dave', 'at', 'Midas', 'because', 'he', 'is', 'one', 'of', 'the', 'most', 'honest', 'business', 'owners', 'in', 'this', 'town!']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'continue', 'going', 'to', 'dave', 'at', 'mid', '##as', 'because', 'he', 'is', 'one', 'of', 'the', 'most', 'honest', 'business', 'owners', 'in', 'this', 'town', '!', '[SEP]']\n",
            "Original: ['When', 'having', 'your', 'car', 'worked', 'on', 'you', 'have', 'to', 'trust', 'the', 'mechanic', '&', 'this', 'Midas', 'is', 'truly', 'someone', 'you', 'can', 'trust!']\n",
            "Tokenized: ['[CLS]', 'when', 'having', 'your', 'car', 'worked', 'on', 'you', 'have', 'to', 'trust', 'the', 'mechanic', '&', 'this', 'mid', '##as', 'is', 'truly', 'someone', 'you', 'can', 'trust', '!', '[SEP]']\n",
            "Original: ['Checked', 'in', 'real', 'late,', 'but', 'staff', 'was', 'very', 'kind', 'and', 'helpful.']\n",
            "Tokenized: ['[CLS]', 'checked', 'in', 'real', 'late', ',', 'but', 'staff', 'was', 'very', 'kind', 'and', 'helpful', '.', '[SEP]']\n",
            "Original: ['Rooms', 'very', 'clean', 'and', 'smelled', 'very', 'fresh.']\n",
            "Tokenized: ['[CLS]', 'rooms', 'very', 'clean', 'and', 'smelled', 'very', 'fresh', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'recommend', 'this', 'hotel', 'to', 'anyone.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'recommend', 'this', 'hotel', 'to', 'anyone', '.', '[SEP]']\n",
            "Original: ['I', 'loved', 'my', 'stay', 'here', 'and', 'if', 'ever', 'back', 'in', 'the', 'area,', 'I', 'will', 'be', 'staying', 'here', 'again.']\n",
            "Tokenized: ['[CLS]', 'i', 'loved', 'my', 'stay', 'here', 'and', 'if', 'ever', 'back', 'in', 'the', 'area', ',', 'i', 'will', 'be', 'staying', 'here', 'again', '.', '[SEP]']\n",
            "Original: ['Good', 'clean', 'store', 'nice', 'car', 'wash']\n",
            "Tokenized: ['[CLS]', 'good', 'clean', 'store', 'nice', 'car', 'wash', '[SEP]']\n",
            "Original: ['Terrible', 'service!!!']\n",
            "Tokenized: ['[CLS]', 'terrible', 'service', '!', '!', '!', '[SEP]']\n",
            "Original: ['Made', 'an', 'appointment', 'to', 'have', 'them', 'come', 'to', 'the', 'house', 'to', 'discuss', 'curtain', 'options', 'and', 'give', 'an', 'estimate.']\n",
            "Tokenized: ['[CLS]', 'made', 'an', 'appointment', 'to', 'have', 'them', 'come', 'to', 'the', 'house', 'to', 'discuss', 'curtain', 'options', 'and', 'give', 'an', 'estimate', '.', '[SEP]']\n",
            "Original: ['They', 'sent', 'over', 'someone', 'who', 'said', 'he', 'knows', 'nothing', 'about', 'curtains', 'and', 'could', 'not', 'show', 'me', 'fabric', 'options', 'or', 'give', 'an', 'estimate.']\n",
            "Tokenized: ['[CLS]', 'they', 'sent', 'over', 'someone', 'who', 'said', 'he', 'knows', 'nothing', 'about', 'curtains', 'and', 'could', 'not', 'show', 'me', 'fabric', 'options', 'or', 'give', 'an', 'estimate', '.', '[SEP]']\n",
            "Original: ['When', 'I', 'called', 'the', 'manager', 'to', 'complain,', 'she', 'said', 'she', 'KNEW', 'the', 'guy', \"didn't\", 'know', 'about', 'curtains', 'and', 'that', 'the', 'usual', 'lady', 'called', 'in', 'sick', 'hours', 'earlier!']\n",
            "Tokenized: ['[CLS]', 'when', 'i', 'called', 'the', 'manager', 'to', 'complain', ',', 'she', 'said', 'she', 'knew', 'the', 'guy', 'didn', \"'\", 't', 'know', 'about', 'curtains', 'and', 'that', 'the', 'usual', 'lady', 'called', 'in', 'sick', 'hours', 'earlier', '!', '[SEP]']\n",
            "Original: ['Instead', 'of', 'rescheduling', 'they', 'chose', 'to', 'waste', 'my', 'time', 'instead.']\n",
            "Tokenized: ['[CLS]', 'instead', 'of', 'res', '##ched', '##ulin', '##g', 'they', 'chose', 'to', 'waste', 'my', 'time', 'instead', '.', '[SEP]']\n",
            "Original: ['So', 'what', 'was', 'the', 'point', 'of', 'the', 'appointment!?!']\n",
            "Tokenized: ['[CLS]', 'so', 'what', 'was', 'the', 'point', 'of', 'the', 'appointment', '!', '?', '!', '[SEP]']\n",
            "Original: ['To', 'just', 'come', 'over', 'and', 'hang', 'out?!?']\n",
            "Tokenized: ['[CLS]', 'to', 'just', 'come', 'over', 'and', 'hang', 'out', '?', '!', '?', '[SEP]']\n",
            "Original: ['I', 'will', 'NEVER', 'do', 'business', 'with', 'this', 'company!']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'never', 'do', 'business', 'with', 'this', 'company', '!', '[SEP]']\n",
            "Original: ['EVER!!!']\n",
            "Tokenized: ['[CLS]', 'ever', '!', '!', '!', '[SEP]']\n",
            "Original: ['Instructor', 'never', 'showed', 'up!']\n",
            "Tokenized: ['[CLS]', 'instructor', 'never', 'showed', 'up', '!', '[SEP]']\n",
            "Original: ['January', '15th', '--', 'We', 'were', 'signed', 'up', 'for', \"Saturday's\", '2', 'PM', 'class', '\"Beginning', 'Yoga', 'with', 'Brittany.\"']\n",
            "Tokenized: ['[CLS]', 'january', '15th', '-', '-', 'we', 'were', 'signed', 'up', 'for', 'saturday', \"'\", 's', '2', 'pm', 'class', '\"', 'beginning', 'yoga', 'with', 'brittany', '.', '\"', '[SEP]']\n",
            "Original: ['We', 'even', 'arrived', '10', 'minutes', 'early', 'as', 'the', 'website', 'suggests.']\n",
            "Tokenized: ['[CLS]', 'we', 'even', 'arrived', '10', 'minutes', 'early', 'as', 'the', 'website', 'suggests', '.', '[SEP]']\n",
            "Original: ['The', 'instructor', 'did', 'not', 'show', 'up!']\n",
            "Tokenized: ['[CLS]', 'the', 'instructor', 'did', 'not', 'show', 'up', '!', '[SEP]']\n",
            "Original: ['We', 'waited', 'until', '2:25', 'PM', 'and', 'then', 'left.']\n",
            "Tokenized: ['[CLS]', 'we', 'waited', 'until', '2', ':', '25', 'pm', 'and', 'then', 'left', '.', '[SEP]']\n",
            "Original: ['There', 'were', '2', 'in', 'our', 'group,', 'and', 'a', '3rd', 'person', 'was', 'also', 'in', 'the', 'parking', 'lot', 'waiting', 'for', 'this', 'class.']\n",
            "Tokenized: ['[CLS]', 'there', 'were', '2', 'in', 'our', 'group', ',', 'and', 'a', '3rd', 'person', 'was', 'also', 'in', 'the', 'parking', 'lot', 'waiting', 'for', 'this', 'class', '.', '[SEP]']\n",
            "Original: ['Well,', 'not', 'much', 'I', 'can', 'say', 'except', \"I'm\", 'very', 'disappointed', 'with', 'this', 'experience.']\n",
            "Tokenized: ['[CLS]', 'well', ',', 'not', 'much', 'i', 'can', 'say', 'except', 'i', \"'\", 'm', 'very', 'disappointed', 'with', 'this', 'experience', '.', '[SEP]']\n",
            "Original: ['This', 'was', 'our', 'first', 'visit', 'to', 'your', 'studio.']\n",
            "Tokenized: ['[CLS]', 'this', 'was', 'our', 'first', 'visit', 'to', 'your', 'studio', '.', '[SEP]']\n",
            "Original: ['In', \"today's\", 'instant', 'world,', \"there's\", 'no', 'reason', 'for', 'the', 'instructor', 'not', 'to', 'even', 'have', 'given', 'us', 'a', 'phone', 'call', 'or', 'e-mail', 'if', 'she', 'was', 'going', 'to', 'be', 'late.']\n",
            "Tokenized: ['[CLS]', 'in', 'today', \"'\", 's', 'instant', 'world', ',', 'there', \"'\", 's', 'no', 'reason', 'for', 'the', 'instructor', 'not', 'to', 'even', 'have', 'given', 'us', 'a', 'phone', 'call', 'or', 'e', '-', 'mail', 'if', 'she', 'was', 'going', 'to', 'be', 'late', '.', '[SEP]']\n",
            "Original: ['As', 'a', 'yoga', 'studio,', \"I'm\", 'sure', \"you're\", 'all', 'aware', 'that', 'all', 'actions', 'generate', 'karma', '…']\n",
            "Tokenized: ['[CLS]', 'as', 'a', 'yoga', 'studio', ',', 'i', \"'\", 'm', 'sure', 'you', \"'\", 're', 'all', 'aware', 'that', 'all', 'actions', 'generate', 'karma', '…', '[SEP]']\n",
            "Original: ['and', 'sometimes', 'karma', 'can', 'manifest', 'itself', 'on', 'a', 'bad', 'review', 'on', 'Google.']\n",
            "Tokenized: ['[CLS]', 'and', 'sometimes', 'karma', 'can', 'manifest', 'itself', 'on', 'a', 'bad', 'review', 'on', 'google', '.', '[SEP]']\n",
            "Original: ['Best', 'Electrician', 'in', 'Florence']\n",
            "Tokenized: ['[CLS]', 'best', 'electric', '##ian', 'in', 'florence', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'using', 'Steele', 'Electric', 'for', 'years.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'using', 'steele', 'electric', 'for', 'years', '.', '[SEP]']\n",
            "Original: ['They', 'have', 'always', 'done', 'a', 'great', 'job', 'at', 'a', 'reasonable', 'price.']\n",
            "Tokenized: ['[CLS]', 'they', 'have', 'always', 'done', 'a', 'great', 'job', 'at', 'a', 'reasonable', 'price', '.', '[SEP]']\n",
            "Original: ['Highly', 'recommended.']\n",
            "Tokenized: ['[CLS]', 'highly', 'recommended', '.', '[SEP]']\n",
            "Original: ['I', 'googled', 'restaurants', 'in', 'the', 'area', 'and', 'Fuji', 'Sushi', 'came', 'up', 'and', 'reviews', 'were', 'great', 'so', 'I', 'made', 'a', 'carry', 'out', 'order', 'of', ':', 'L', '17.']\n",
            "Tokenized: ['[CLS]', 'i', 'google', '##d', 'restaurants', 'in', 'the', 'area', 'and', 'fuji', 'su', '##shi', 'came', 'up', 'and', 'reviews', 'were', 'great', 'so', 'i', 'made', 'a', 'carry', 'out', 'order', 'of', ':', 'l', '17', '.', '[SEP]']\n",
            "Original: ['Mixed', 'Tempura.....................8.25', 'Shrimp', 'or', 'vegetable', 'tempura', '&', 'salad.']\n",
            "Tokenized: ['[CLS]', 'mixed', 'te', '##mp', '##ura', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '8', '.', '25', 'shrimp', 'or', 'vegetable', 'te', '##mp', '##ura', '&', 'salad', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'very', 'happy', 'with', 'the', 'customer', 'service', 'and', 'even', 'more', 'please', 'with', 'the', 'portion', 'size,', 'to', 'go', 'box', 'set', 'up', 'and', 'quality', 'of', 'the', 'food', 'for', 'the', 'price.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'very', 'happy', 'with', 'the', 'customer', 'service', 'and', 'even', 'more', 'please', 'with', 'the', 'portion', 'size', ',', 'to', 'go', 'box', 'set', 'up', 'and', 'quality', 'of', 'the', 'food', 'for', 'the', 'price', '.', '[SEP]']\n",
            "Original: [\"I'm\", 'very', 'happy', 'and', 'will', 'definitely', 'dine', 'in', 'and', 'carry', 'out', 'again.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'm', 'very', 'happy', 'and', 'will', 'definitely', 'din', '##e', 'in', 'and', 'carry', 'out', 'again', '.', '[SEP]']\n",
            "Original: ['Some', 'of', 'the', 'nicest', 'people', 'and', 'very', 'good', 'work', 'standards']\n",
            "Tokenized: ['[CLS]', 'some', 'of', 'the', 'nice', '##st', 'people', 'and', 'very', 'good', 'work', 'standards', '[SEP]']\n",
            "Original: ['Room', 'ok.']\n",
            "Tokenized: ['[CLS]', 'room', 'ok', '.', '[SEP]']\n",
            "Original: ['Service', 'and', 'Client', 'base', 'not', 'ok.']\n",
            "Tokenized: ['[CLS]', 'service', 'and', 'client', 'base', 'not', 'ok', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'stayed', 'in', 'this', 'hotel', 'many', 'times,', 'and', 'while', 'it', 'typically', 'offers', 'a', 'decent', 'bang', 'for', 'the', 'buck,', 'its', 'client', 'base', 'largely', 'consists', 'of', 'troubled', 'youngsteers', 'and', 'evictees', 'from', 'the', 'local,', 'not', 'so', 'pleasant', 'hood.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'stayed', 'in', 'this', 'hotel', 'many', 'times', ',', 'and', 'while', 'it', 'typically', 'offers', 'a', 'decent', 'bang', 'for', 'the', 'buck', ',', 'its', 'client', 'base', 'largely', 'consists', 'of', 'troubled', 'young', '##ste', '##ers', 'and', 'ev', '##ic', '##tees', 'from', 'the', 'local', ',', 'not', 'so', 'pleasant', 'hood', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'never', 'considered', 'this', 'a', 'real', 'problem', 'as', 'I', 'travel', 'without', 'kids', 'and', 'can', 'fend', 'for', 'myself,', 'but', 'when', 'I', 'had', 'to', 'listen', 'to', 'a', '(non-violent)', 'domestic', 'fight', 'that', 'lasted', 'from', '1', 'AM', 'to', '5', 'AM', 'during', 'my', 'last', 'stay', 'and', 'I', 'found', 'out', 'that', 'the', 'front', 'desk', 'was', 'unmanned', 'during', 'night', 'hours,', 'my', 'choice', 'was', 'to', 'either', 'waste', 'tax', 'payers', 'money', 'by', 'calling', 'for', 'a', 'yet', 'another', 'police', 'dispatch', 'to', 'this', 'hotel,', 'or', 'just', 'get', 'over', 'it.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'never', 'considered', 'this', 'a', 'real', 'problem', 'as', 'i', 'travel', 'without', 'kids', 'and', 'can', 'fen', '##d', 'for', 'myself', ',', 'but', 'when', 'i', 'had', 'to', 'listen', 'to', 'a', '(', 'non', '-', 'violent', ')', 'domestic', 'fight', 'that', 'lasted', 'from', '1', 'am', 'to', '5', 'am', 'during', 'my', 'last', 'stay', 'and', 'i', 'found', 'out', 'that', 'the', 'front', 'desk', 'was', 'unmanned', 'during', 'night', 'hours', ',', 'my', 'choice', 'was', 'to', 'either', 'waste', 'tax', 'pay', '##ers', 'money', 'by', 'calling', 'for', 'a', 'yet', 'another', 'police', 'dispatch', 'to', 'this', 'hotel', ',', 'or', 'just', 'get', 'over', 'it', '.', '[SEP]']\n",
            "Original: ['I', 'chose', 'the', 'later,', 'but', 'approached', 'the', 'front', 'desk', 'about', 'the', 'hotel', 'policy', 'to', 'push', 'over', 'their', 'responsibilities', 'on', 'local', 'authorities,', 'not', 'to', 'mention', 'the', 'good', 'nights', 'sleep', 'i', 'paid', 'for', 'but', 'didnt', 'get.']\n",
            "Tokenized: ['[CLS]', 'i', 'chose', 'the', 'later', ',', 'but', 'approached', 'the', 'front', 'desk', 'about', 'the', 'hotel', 'policy', 'to', 'push', 'over', 'their', 'responsibilities', 'on', 'local', 'authorities', ',', 'not', 'to', 'mention', 'the', 'good', 'nights', 'sleep', 'i', 'paid', 'for', 'but', 'didn', '##t', 'get', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'told', 'management', 'would', 'call', 'me', 'back', 'but', 'still', 'waiting', 'for', 'that', 'call.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'told', 'management', 'would', 'call', 'me', 'back', 'but', 'still', 'waiting', 'for', 'that', 'call', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'probably', 'stay', 'here', 'again', 'because', 'its', 'cheap', 'and', 'I', 'cant', 'afford', 'a', 'better', 'hotel,', 'but', 'do', 'not', 'look', 'forward', 'to', 'it', 'and', 'would', 'definitely', 'not', 'recommend', 'this', 'hotel', 'for', 'families', 'or', 'single', 'female', 'travellers.']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'probably', 'stay', 'here', 'again', 'because', 'its', 'cheap', 'and', 'i', 'can', '##t', 'afford', 'a', 'better', 'hotel', ',', 'but', 'do', 'not', 'look', 'forward', 'to', 'it', 'and', 'would', 'definitely', 'not', 'recommend', 'this', 'hotel', 'for', 'families', 'or', 'single', 'female', 'travellers', '.', '[SEP]']\n",
            "Original: ['Nice', 'selection,', 'very', 'clean,', 'friendly', 'staff!']\n",
            "Tokenized: ['[CLS]', 'nice', 'selection', ',', 'very', 'clean', ',', 'friendly', 'staff', '!', '[SEP]']\n",
            "Original: ['Hot', 'Iron', 'has', 'become', 'a', 'favorite', 'of', 'our', 'family.']\n",
            "Tokenized: ['[CLS]', 'hot', 'iron', 'has', 'become', 'a', 'favorite', 'of', 'our', 'family', '.', '[SEP]']\n",
            "Original: ['Everyone', 'can', 'get', 'the', 'signature', 'dish', 'with', 'ingredients', 'and', 'spices', 'that', 'they', 'want', 'and', 'have', 'the', 'fun', 'of', 'watching', 'it', 'cook.']\n",
            "Tokenized: ['[CLS]', 'everyone', 'can', 'get', 'the', 'signature', 'dish', 'with', 'ingredients', 'and', 'spices', 'that', 'they', 'want', 'and', 'have', 'the', 'fun', 'of', 'watching', 'it', 'cook', '.', '[SEP]']\n",
            "Original: ['The', 'staff', 'are', 'very', 'friendly', 'and', 'conscientious.']\n",
            "Tokenized: ['[CLS]', 'the', 'staff', 'are', 'very', 'friendly', 'and', 'con', '##sc', '##ient', '##ious', '.', '[SEP]']\n",
            "Original: ['They', 'always', 'ask', 'if', 'you', 'have', 'meat', 'in', 'your', 'dish,', '(for', 'vegetarians', 'like', 'me),', 'they', 'scrub', 'an', 'area', 'of', 'the', 'grill', 'and', 'use', 'separate', 'utensils', 'to', 'cook.']\n",
            "Tokenized: ['[CLS]', 'they', 'always', 'ask', 'if', 'you', 'have', 'meat', 'in', 'your', 'dish', ',', '(', 'for', 'vegetarian', '##s', 'like', 'me', ')', ',', 'they', 'scrub', 'an', 'area', 'of', 'the', 'grill', 'and', 'use', 'separate', 'ut', '##ens', '##ils', 'to', 'cook', '.', '[SEP]']\n",
            "Original: ['They', \"don't\", 'just', 'dump', 'your', 'ingredients', 'on', 'and', 'cook', 'either,', 'but', 'carefully', 'separate', 'meat', 'to', 'the', 'hotter', 'parts', 'of', 'the', 'grill', 'and', 'still', 'manage', 'to', 'keep', 'the', 'veggies', 'from', 'turning', 'to', 'mush.']\n",
            "Tokenized: ['[CLS]', 'they', 'don', \"'\", 't', 'just', 'dump', 'your', 'ingredients', 'on', 'and', 'cook', 'either', ',', 'but', 'carefully', 'separate', 'meat', 'to', 'the', 'hotter', 'parts', 'of', 'the', 'grill', 'and', 'still', 'manage', 'to', 'keep', 'the', 've', '##gg', '##ies', 'from', 'turning', 'to', 'mu', '##sh', '.', '[SEP]']\n",
            "Original: ['The', 'selection', 'of', 'meats,', 'veggies', 'and', 'sauces', 'is', 'awesome', 'too!']\n",
            "Tokenized: ['[CLS]', 'the', 'selection', 'of', 'meat', '##s', ',', 've', '##gg', '##ies', 'and', 'sauce', '##s', 'is', 'awesome', 'too', '!', '[SEP]']\n",
            "Original: ['Every', 'time', 'we', 'go', 'they', 'seem', 'to', 'have', 'a', 'different', 'ingredient', 'or', 'two', 'which', 'keeps', 'things', 'interesting.']\n",
            "Tokenized: ['[CLS]', 'every', 'time', 'we', 'go', 'they', 'seem', 'to', 'have', 'a', 'different', 'ingredient', 'or', 'two', 'which', 'keeps', 'things', 'interesting', '.', '[SEP]']\n",
            "Original: ['Meats', 'are', 'kept', 'VERY', 'cold,', 'seafood', 'smells', 'fresh', 'and', 'the', 'serving', 'bar', 'is', 'VERY', 'clean.']\n",
            "Tokenized: ['[CLS]', 'meat', '##s', 'are', 'kept', 'very', 'cold', ',', 'seafood', 'smells', 'fresh', 'and', 'the', 'serving', 'bar', 'is', 'very', 'clean', '.', '[SEP]']\n",
            "Original: ['Prices', 'are', 'reasonable,', '(Kids', 'meals', 'are', 'around', '4.99)', 'and', \"Wednesday's\", 'they', 'have', 'discount', 'dinner', 'prices,', 'I', 'believe.']\n",
            "Tokenized: ['[CLS]', 'prices', 'are', 'reasonable', ',', '(', 'kids', 'meals', 'are', 'around', '4', '.', '99', ')', 'and', 'wednesday', \"'\", 's', 'they', 'have', 'discount', 'dinner', 'prices', ',', 'i', 'believe', '.', '[SEP]']\n",
            "Original: ['Would', 'not', 'recommend', 'I', 'Was', 'in', 'a', 'fair', 'amount', 'of', 'pain', 'for', 'several', 'weeks.']\n",
            "Tokenized: ['[CLS]', 'would', 'not', 'recommend', 'i', 'was', 'in', 'a', 'fair', 'amount', 'of', 'pain', 'for', 'several', 'weeks', '.', '[SEP]']\n",
            "Original: ['his', 'clinic', 'is', 'very', 'very', 'dirty', 'he', 'is', 'a', 'real', 'disaster', 'to', 'go', 'totally', 'not', 'organized', 'for', 'every', 'step', 'he', 'take', '.']\n",
            "Tokenized: ['[CLS]', 'his', 'clinic', 'is', 'very', 'very', 'dirty', 'he', 'is', 'a', 'real', 'disaster', 'to', 'go', 'totally', 'not', 'organized', 'for', 'every', 'step', 'he', 'take', '.', '[SEP]']\n",
            "Original: ['My', 'wife', 'and', 'I', 'avoided', 'doing', 'some', 'fairly', 'simple', 'electrical', 're-wiring', 'in', 'our', 'home', 'for', 'several', 'years', 'due', 'to', 'overall', 'hassle', 'and', 'cost', 'involved.']\n",
            "Tokenized: ['[CLS]', 'my', 'wife', 'and', 'i', 'avoided', 'doing', 'some', 'fairly', 'simple', 'electrical', 're', '-', 'wiring', 'in', 'our', 'home', 'for', 'several', 'years', 'due', 'to', 'overall', 'has', '##sle', 'and', 'cost', 'involved', '.', '[SEP]']\n",
            "Original: ['I', 'finally', 'called', 'Matt', 'from', 'Bonafide', 'and', 'he', 'made', 'the', 'project', 'both', 'easy', 'for', 'us', 'and', 'reasonable.']\n",
            "Tokenized: ['[CLS]', 'i', 'finally', 'called', 'matt', 'from', 'bon', '##af', '##ide', 'and', 'he', 'made', 'the', 'project', 'both', 'easy', 'for', 'us', 'and', 'reasonable', '.', '[SEP]']\n",
            "Original: ['He', 'was', 'prompt,', 'knowledgable,', 'friendly,', 'clean', 'and', 'just', 'an', 'overall', 'great', 'guy', 'who', 'obviously', 'cares', 'about', 'his', 'business.']\n",
            "Tokenized: ['[CLS]', 'he', 'was', 'prompt', ',', 'know', '##led', '##ga', '##ble', ',', 'friendly', ',', 'clean', 'and', 'just', 'an', 'overall', 'great', 'guy', 'who', 'obviously', 'cares', 'about', 'his', 'business', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'reccommend', 'his', 'services', 'however/whenever', 'possible!']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'rec', '##com', '##men', '##d', 'his', 'services', 'however', '/', 'whenever', 'possible', '!', '[SEP]']\n",
            "Original: ['Amazing', 'Pictures', 'at', 'an', 'Amazing', 'Price']\n",
            "Tokenized: ['[CLS]', 'amazing', 'pictures', 'at', 'an', 'amazing', 'price', '[SEP]']\n",
            "Original: ['Rendy', 'is', 'totally', 'amazing.']\n",
            "Tokenized: ['[CLS]', 'ren', '##dy', 'is', 'totally', 'amazing', '.', '[SEP]']\n",
            "Original: ['She', 'gave', 'me', 'amazing', 'pictures', 'at', 'an', 'amazing', 'price', 'and', 'made', 'my', 'wedding', 'day', 'so', 'memorable.']\n",
            "Tokenized: ['[CLS]', 'she', 'gave', 'me', 'amazing', 'pictures', 'at', 'an', 'amazing', 'price', 'and', 'made', 'my', 'wedding', 'day', 'so', 'memorable', '.', '[SEP]']\n",
            "Original: ['She', 'was', 'way', 'easy', 'to', 'work', 'with', 'and', 'made', 'my', 'wedding', 'day', 'so', 'easy', 'and', 'she', 'got', 'some', 'amazing', 'pictures,', 'not', 'only', 'of', 'me,', 'but', 'of', 'my', 'family', 'and', 'friends.']\n",
            "Tokenized: ['[CLS]', 'she', 'was', 'way', 'easy', 'to', 'work', 'with', 'and', 'made', 'my', 'wedding', 'day', 'so', 'easy', 'and', 'she', 'got', 'some', 'amazing', 'pictures', ',', 'not', 'only', 'of', 'me', ',', 'but', 'of', 'my', 'family', 'and', 'friends', '.', '[SEP]']\n",
            "Original: ['She', 'is', 'amazing.']\n",
            "Tokenized: ['[CLS]', 'she', 'is', 'amazing', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'recommend', 'her', 'to', 'anyone!']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'recommend', 'her', 'to', 'anyone', '!', '[SEP]']\n",
            "Original: ['Thanks', 'for', 'following', 'me', 'around', 'the', 'store']\n",
            "Tokenized: ['[CLS]', 'thanks', 'for', 'following', 'me', 'around', 'the', 'store', '[SEP]']\n",
            "Original: ['Enough', 'said.']\n",
            "Tokenized: ['[CLS]', 'enough', 'said', '.', '[SEP]']\n",
            "Original: ['I', \"don't\", 'steal,', 'I', \"wasn't\", 'acting', 'suspiciously.']\n",
            "Tokenized: ['[CLS]', 'i', 'don', \"'\", 't', 'steal', ',', 'i', 'wasn', \"'\", 't', 'acting', 'suspiciously', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'ready', 'to', 'buy', 'a', 'new', 'jacket,', 'a', 'new', 'sweater', 'and', 'a', 'couple', 'of', 'your', 'overpriced', 'belts', 'and', 'I', 'walked', 'out', 'because', 'of', 'your', 'obvious', 'lurking']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'ready', 'to', 'buy', 'a', 'new', 'jacket', ',', 'a', 'new', 'sweater', 'and', 'a', 'couple', 'of', 'your', 'over', '##pr', '##ice', '##d', 'belts', 'and', 'i', 'walked', 'out', 'because', 'of', 'your', 'obvious', 'lurking', '[SEP]']\n",
            "Original: ['The', 'best', 'pizza', 'ever', 'im', 'fat', 'so', 'ive', 'had', 'a', 'ton', 'of', 'pizza', 'other', 'than', 'pizza', 'from', 'chicago', 'its', 'the', 'best']\n",
            "Tokenized: ['[CLS]', 'the', 'best', 'pizza', 'ever', 'im', 'fat', 'so', 'iv', '##e', 'had', 'a', 'ton', 'of', 'pizza', 'other', 'than', 'pizza', 'from', 'chicago', 'its', 'the', 'best', '[SEP]']\n",
            "Original: ['Very', 'unhappy', '...']\n",
            "Tokenized: ['[CLS]', 'very', 'unhappy', '.', '.', '.', '[SEP]']\n",
            "Original: ['Working', 'with', 'Rod', 'Jacobsen', 'was', 'my', 'first', 'experience', 'working', 'with', 'a', 'CPA,', 'so', 'I', 'did', 'not', 'know', 'what', 'to', 'expect.']\n",
            "Tokenized: ['[CLS]', 'working', 'with', 'rod', 'jacobs', '##en', 'was', 'my', 'first', 'experience', 'working', 'with', 'a', 'cp', '##a', ',', 'so', 'i', 'did', 'not', 'know', 'what', 'to', 'expect', '.', '[SEP]']\n",
            "Original: ['That', 'said', '-', 'he', 'seemed', 'to', 'be', 'doing', 'well', 'enough.']\n",
            "Tokenized: ['[CLS]', 'that', 'said', '-', 'he', 'seemed', 'to', 'be', 'doing', 'well', 'enough', '.', '[SEP]']\n",
            "Original: ['However,', 'then', 'I', 'asked', 'to', 'amend', 'my', 'return', 'to', 'apply', 'a', 'credit', 'I', 'had', 'just', 'become', 'eligible', 'for.']\n",
            "Tokenized: ['[CLS]', 'however', ',', 'then', 'i', 'asked', 'to', 'amend', 'my', 'return', 'to', 'apply', 'a', 'credit', 'i', 'had', 'just', 'become', 'eligible', 'for', '.', '[SEP]']\n",
            "Original: ['I', 'expected', 'to', 'pay', 'for', 'this', 'service,', 'but', 'imagine', 'my', 'surprise', 'when', 'I', 'received', 'a', 'bill', 'for', 'MORE', 'than', 'what', 'I', 'paid', 'to', 'have', 'the', 'original', 'return', 'prepared.']\n",
            "Tokenized: ['[CLS]', 'i', 'expected', 'to', 'pay', 'for', 'this', 'service', ',', 'but', 'imagine', 'my', 'surprise', 'when', 'i', 'received', 'a', 'bill', 'for', 'more', 'than', 'what', 'i', 'paid', 'to', 'have', 'the', 'original', 'return', 'prepared', '.', '[SEP]']\n",
            "Original: ['Asked', 'why,', 'Rod', 'simply', 'told', 'me', 'that', 'he', 'had', 'to', 'research', 'how', 'to', 'do', 'the', 'amendment', '(it', 'was', 'an', 'amended', 'to', 'show', 'that', 'I', 'had', 'purchased', 'a', 'home', '-', 'nothing', 'out', 'of', 'the', 'ordinary,', 'one', 'would', 'think)', 'and', 'that', 'took', 'time', 'to', 'figure', 'out.']\n",
            "Tokenized: ['[CLS]', 'asked', 'why', ',', 'rod', 'simply', 'told', 'me', 'that', 'he', 'had', 'to', 'research', 'how', 'to', 'do', 'the', 'amendment', '(', 'it', 'was', 'an', 'amended', 'to', 'show', 'that', 'i', 'had', 'purchased', 'a', 'home', '-', 'nothing', 'out', 'of', 'the', 'ordinary', ',', 'one', 'would', 'think', ')', 'and', 'that', 'took', 'time', 'to', 'figure', 'out', '.', '[SEP]']\n",
            "Original: ['Well,', 'that', 'was', 'strike', 'one.']\n",
            "Tokenized: ['[CLS]', 'well', ',', 'that', 'was', 'strike', 'one', '.', '[SEP]']\n",
            "Original: ['I', \"wasn't\", 'going', 'to', 'use', 'them', 'again,', 'but', 'I', 'was', 'going', 'to', 'leave', 'it', 'at', 'that.']\n",
            "Tokenized: ['[CLS]', 'i', 'wasn', \"'\", 't', 'going', 'to', 'use', 'them', 'again', ',', 'but', 'i', 'was', 'going', 'to', 'leave', 'it', 'at', 'that', '.', '[SEP]']\n",
            "Original: ['However,', 'now', 'that', 'I', 'have', 'come', 'to', 'realize', 'that', 'I', 'am', 'going', 'to', 'owe', 'the', 'IRS', '$6,000+', 'despite', 'doing', 'exactly', 'what', 'Rod', 'told', 'me', 'to', 'do,', 'I', 'feel', 'I', 'have', 'to', 'voice', 'my', 'opinion.']\n",
            "Tokenized: ['[CLS]', 'however', ',', 'now', 'that', 'i', 'have', 'come', 'to', 'realize', 'that', 'i', 'am', 'going', 'to', 'owe', 'the', 'irs', '$', '6', ',', '000', '+', 'despite', 'doing', 'exactly', 'what', 'rod', 'told', 'me', 'to', 'do', ',', 'i', 'feel', 'i', 'have', 'to', 'voice', 'my', 'opinion', '.', '[SEP]']\n",
            "Original: ['Last', 'year,', 'after', 'all', 'was', 'said', 'and', 'done,', 'I', 'asked', 'Rod', 'whether', 'my', 'payment', 'structure', 'would', 'leave', 'me', 'with', 'no/little', 'tax', 'liability', 'at', 'the', 'end', 'of', 'the', 'year.']\n",
            "Tokenized: ['[CLS]', 'last', 'year', ',', 'after', 'all', 'was', 'said', 'and', 'done', ',', 'i', 'asked', 'rod', 'whether', 'my', 'payment', 'structure', 'would', 'leave', 'me', 'with', 'no', '/', 'little', 'tax', 'liability', 'at', 'the', 'end', 'of', 'the', 'year', '.', '[SEP]']\n",
            "Original: ['He', 'said', 'yes.']\n",
            "Tokenized: ['[CLS]', 'he', 'said', 'yes', '.', '[SEP]']\n",
            "Original: ['Well,', 'again,', 'I', 'am', 'now', 'faced', 'with', 'a', 'tax', 'bill', 'of', '$6,000+,', 'all', 'due', 'on', 'April', '15,', '2010', 'and', 'all', 'that', 'Rod', 'has', 'to', 'say', 'to', 'the', 'matter', 'is', \"'well,\", 'you', \"won't\", 'have', 'to', 'pay', 'a', \"penalty.'\"]\n",
            "Tokenized: ['[CLS]', 'well', ',', 'again', ',', 'i', 'am', 'now', 'faced', 'with', 'a', 'tax', 'bill', 'of', '$', '6', ',', '000', '+', ',', 'all', 'due', 'on', 'april', '15', ',', '2010', 'and', 'all', 'that', 'rod', 'has', 'to', 'say', 'to', 'the', 'matter', 'is', \"'\", 'well', ',', 'you', 'won', \"'\", 't', 'have', 'to', 'pay', 'a', 'penalty', '.', \"'\", '[SEP]']\n",
            "Original: ['I', 'may', 'not', 'have', 'to', 'pay', 'a', 'penalty,', 'yet,', 'but', 'this', 'is', 'NOT', 'what', 'I', 'had', 'in', 'mind', 'when', 'hired', 'these', 'guys.']\n",
            "Tokenized: ['[CLS]', 'i', 'may', 'not', 'have', 'to', 'pay', 'a', 'penalty', ',', 'yet', ',', 'but', 'this', 'is', 'not', 'what', 'i', 'had', 'in', 'mind', 'when', 'hired', 'these', 'guys', '.', '[SEP]']\n",
            "Original: ['In', 'the', 'words', 'of', 'my', 'new', 'accountant,', 'THEY', 'LET', 'ME', 'DOWN!']\n",
            "Tokenized: ['[CLS]', 'in', 'the', 'words', 'of', 'my', 'new', 'accountant', ',', 'they', 'let', 'me', 'down', '!', '[SEP]']\n",
            "Original: ['WHAT', 'A', 'GREAT', 'DEAL', 'THANK', 'YOU']\n",
            "Tokenized: ['[CLS]', 'what', 'a', 'great', 'deal', 'thank', 'you', '[SEP]']\n",
            "Original: ['Best', 'Car', 'Dealer', 'in', 'TX']\n",
            "Tokenized: ['[CLS]', 'best', 'car', 'dealer', 'in', 'tx', '[SEP]']\n",
            "Original: ['I', 'purchased', 'a', 'nissan', 'from', 'this', 'dealship.']\n",
            "Tokenized: ['[CLS]', 'i', 'purchased', 'a', 'nissan', 'from', 'this', 'deals', '##hip', '.', '[SEP]']\n",
            "Original: ['The', 'sales', 'men', 'were', 'very', 'knowledgeable', 'about', 'every', 'aspect', 'of', 'every', 'car', 'we', 'looked', 'at.']\n",
            "Tokenized: ['[CLS]', 'the', 'sales', 'men', 'were', 'very', 'knowledge', '##able', 'about', 'every', 'aspect', 'of', 'every', 'car', 'we', 'looked', 'at', '.', '[SEP]']\n",
            "Original: ['They', 'were', 'very', 'patient', 'and', 'helpful', 'from', 'showing', 'the', 'cars', 'to', 'doing', 'the', 'paperwork.']\n",
            "Tokenized: ['[CLS]', 'they', 'were', 'very', 'patient', 'and', 'helpful', 'from', 'showing', 'the', 'cars', 'to', 'doing', 'the', 'paperwork', '.', '[SEP]']\n",
            "Original: ['The', 'paperwork', 'was', 'a', 'very', 'easy', 'and', 'smooth.']\n",
            "Tokenized: ['[CLS]', 'the', 'paperwork', 'was', 'a', 'very', 'easy', 'and', 'smooth', '.', '[SEP]']\n",
            "Original: ['They', 'tried', 'to', 'run', 'my', 'credit', 'score', 'as', 'less', 'as', 'possible', 'so', 'it', \"won't\", 'hurt', 'my', 'score.']\n",
            "Tokenized: ['[CLS]', 'they', 'tried', 'to', 'run', 'my', 'credit', 'score', 'as', 'less', 'as', 'possible', 'so', 'it', 'won', \"'\", 't', 'hurt', 'my', 'score', '.', '[SEP]']\n",
            "Original: ['Overall,', 'I', 'was', 'very', 'happy', 'with', 'the', 'customer', 'service', 'and', 'my', 'purchase.']\n",
            "Tokenized: ['[CLS]', 'overall', ',', 'i', 'was', 'very', 'happy', 'with', 'the', 'customer', 'service', 'and', 'my', 'purchase', '.', '[SEP]']\n",
            "Original: ['If', \"you're\", 'looking', 'to', 'buy', 'a', 'car,', 'definitely', 'give', 'them', 'a', 'call.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', \"'\", 're', 'looking', 'to', 'buy', 'a', 'car', ',', 'definitely', 'give', 'them', 'a', 'call', '.', '[SEP]']\n",
            "Original: ['They', 'have', 'a', 'huge', 'inventory.']\n",
            "Tokenized: ['[CLS]', 'they', 'have', 'a', 'huge', 'inventory', '.', '[SEP]']\n",
            "Original: ['No', 'way.']\n",
            "Tokenized: ['[CLS]', 'no', 'way', '.', '[SEP]']\n",
            "Original: [\"I'm\", 'certainly', 'no', 'expert', 'on', 'asian', 'food', 'in', 'fact', 'not', 'even', 'a', 'lover', 'of', 'Vietnamese', 'food', 'but', 'I', 'wanted', 'to', 'try', 'the', 'real', 'things', 'here', 'at', 'A', 'Dong.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'm', 'certainly', 'no', 'expert', 'on', 'asian', 'food', 'in', 'fact', 'not', 'even', 'a', 'lover', 'of', 'vietnamese', 'food', 'but', 'i', 'wanted', 'to', 'try', 'the', 'real', 'things', 'here', 'at', 'a', 'dong', '.', '[SEP]']\n",
            "Original: ['Cold,', 'slimy,', 'tasteless', 'however', 'is', 'the', 'same', 'in', 'all', 'languages', 'and', 'foods.']\n",
            "Tokenized: ['[CLS]', 'cold', ',', 'slim', '##y', ',', 'taste', '##less', 'however', 'is', 'the', 'same', 'in', 'all', 'languages', 'and', 'foods', '.', '[SEP]']\n",
            "Original: ['Not', 'good,', 'not', 'great,', 'and', 'again', 'another', 'disappointment', 'in', 'Central', 'Iowa.']\n",
            "Tokenized: ['[CLS]', 'not', 'good', ',', 'not', 'great', ',', 'and', 'again', 'another', 'disappointment', 'in', 'central', 'iowa', '.', '[SEP]']\n",
            "Original: ['How', 'do', 'these', 'places', 'stay', 'in', 'business.']\n",
            "Tokenized: ['[CLS]', 'how', 'do', 'these', 'places', 'stay', 'in', 'business', '.', '[SEP]']\n",
            "Original: ['This', 'town', 'needs', 'some', 'food', 'soul', 'and', 'this', 'is', 'not', 'it.']\n",
            "Tokenized: ['[CLS]', 'this', 'town', 'needs', 'some', 'food', 'soul', 'and', 'this', 'is', 'not', 'it', '.', '[SEP]']\n",
            "Original: ['Great', 'deals,', 'great', 'pizza!']\n",
            "Tokenized: ['[CLS]', 'great', 'deals', ',', 'great', 'pizza', '!', '[SEP]']\n",
            "Original: ['One', 'of', 'the', 'better', 'vegetarian', 'sandwiches', \"I've\", 'had', 'in', 'Seattle.']\n",
            "Tokenized: ['[CLS]', 'one', 'of', 'the', 'better', 'vegetarian', 'sandwiches', 'i', \"'\", 've', 'had', 'in', 'seattle', '.', '[SEP]']\n",
            "Original: ['And', 'from', 'a', 'place', 'that', 'specializes', 'in', 'high', 'quality', 'meat,', 'too.']\n",
            "Tokenized: ['[CLS]', 'and', 'from', 'a', 'place', 'that', 'specializes', 'in', 'high', 'quality', 'meat', ',', 'too', '.', '[SEP]']\n",
            "Original: ['Best', 'Salsa', '(hehehe)']\n",
            "Tokenized: ['[CLS]', 'best', 'salsa', '(', 'he', '##he', '##he', ')', '[SEP]']\n",
            "Original: ['After', 'my', 'trees', 'were', 'cleaned', 'up,', 'they', 'gave', 'me', 'a', 'jar', 'of', 'salsa.']\n",
            "Tokenized: ['[CLS]', 'after', 'my', 'trees', 'were', 'cleaned', 'up', ',', 'they', 'gave', 'me', 'a', 'jar', 'of', 'salsa', '.', '[SEP]']\n",
            "Original: ['The', 'owner', 'warned', 'me', 'that', 'it', 'was', 'the', 'best', 'salsa', 'I', 'would', 'ever', 'had,', 'and', 'he', 'was', 'right.']\n",
            "Tokenized: ['[CLS]', 'the', 'owner', 'warned', 'me', 'that', 'it', 'was', 'the', 'best', 'salsa', 'i', 'would', 'ever', 'had', ',', 'and', 'he', 'was', 'right', '.', '[SEP]']\n",
            "Original: ['No', 'joke!']\n",
            "Tokenized: ['[CLS]', 'no', 'joke', '!', '[SEP]']\n",
            "Original: ['I', 'hate', 'to', 'say', 'check', 'them', 'out', 'just', 'for', 'the', 'salsa,', 'but', 'James,', 'I', 'NEED', 'another', 'jar', 'badly', ':)', 'All', 'kidding', 'aside,', 'they', 'are', 'a', 'very', 'good', 'company,', 'I', 'have', 'a', 'hard', 'time', 'giving', 'any', 'service', 'biz', 'a', '5', 'star', 'review', 'but', 'they', 'came', 'close.']\n",
            "Tokenized: ['[CLS]', 'i', 'hate', 'to', 'say', 'check', 'them', 'out', 'just', 'for', 'the', 'salsa', ',', 'but', 'james', ',', 'i', 'need', 'another', 'jar', 'badly', ':', ')', 'all', 'kidding', 'aside', ',', 'they', 'are', 'a', 'very', 'good', 'company', ',', 'i', 'have', 'a', 'hard', 'time', 'giving', 'any', 'service', 'bi', '##z', 'a', '5', 'star', 'review', 'but', 'they', 'came', 'close', '.', '[SEP]']\n",
            "Original: ['Very', 'affordable', \"(don't\", 'call', 'it', 'cheap)', 'and', 'their', 'trimmers', 'were', 'quick', 'and', 'courteous', 'when', 'I', 'got', 'home', 'from', 'work.']\n",
            "Tokenized: ['[CLS]', 'very', 'affordable', '(', 'don', \"'\", 't', 'call', 'it', 'cheap', ')', 'and', 'their', 'trim', '##mers', 'were', 'quick', 'and', 'court', '##eous', 'when', 'i', 'got', 'home', 'from', 'work', '.', '[SEP]']\n",
            "Original: ['And', 'the', 'salsa,', 'be', 'sure', 'to', 'ask', 'for', 'a', 'jar', 'and', 'have', 'plenty', 'of', 'chips', 'around,', 'you', 'will', 'need', 'them.....']\n",
            "Tokenized: ['[CLS]', 'and', 'the', 'salsa', ',', 'be', 'sure', 'to', 'ask', 'for', 'a', 'jar', 'and', 'have', 'plenty', 'of', 'chips', 'around', ',', 'you', 'will', 'need', 'them', '.', '.', '.', '.', '.', '[SEP]']\n",
            "Original: ['Cleanest', 'guesthouse', 'i', 'have', 'been', 'to']\n",
            "Tokenized: ['[CLS]', 'clean', '##est', 'guest', '##house', 'i', 'have', 'been', 'to', '[SEP]']\n",
            "Original: ['Stayed', 'here', 'for', '2', 'nights.']\n",
            "Tokenized: ['[CLS]', 'stayed', 'here', 'for', '2', 'nights', '.', '[SEP]']\n",
            "Original: ['The', 'owner', 'was', 'very', 'friendly', 'and', 'helpful.']\n",
            "Tokenized: ['[CLS]', 'the', 'owner', 'was', 'very', 'friendly', 'and', 'helpful', '.', '[SEP]']\n",
            "Original: ['The', 'rooms', 'were', 'very', 'clean', 'and', 'the', 'breakfast', 'was', 'excellent.']\n",
            "Tokenized: ['[CLS]', 'the', 'rooms', 'were', 'very', 'clean', 'and', 'the', 'breakfast', 'was', 'excellent', '.', '[SEP]']\n",
            "Original: ['Good', 'location', 'and', 'off', 'road', 'parking', 'made', 'our', 'stay', 'very', 'convenient.']\n",
            "Tokenized: ['[CLS]', 'good', 'location', 'and', 'off', 'road', 'parking', 'made', 'our', 'stay', 'very', 'convenient', '.', '[SEP]']\n",
            "Original: ['Very', 'poor', 'customer', 'service.']\n",
            "Tokenized: ['[CLS]', 'very', 'poor', 'customer', 'service', '.', '[SEP]']\n",
            "Original: ['There', 'are', 'a', 'couple', 'decent', 'people', 'working', 'there,', 'but', 'the', 'rest', 'are', 'VERY', 'dishonest,', 'as', 'well', 'as', 'rude,', 'I', 'have', 'yet', 'to', 'hear', 'the', 'truth', 'come', 'out', 'of', 'their', 'mouths.']\n",
            "Tokenized: ['[CLS]', 'there', 'are', 'a', 'couple', 'decent', 'people', 'working', 'there', ',', 'but', 'the', 'rest', 'are', 'very', 'dish', '##ones', '##t', ',', 'as', 'well', 'as', 'rude', ',', 'i', 'have', 'yet', 'to', 'hear', 'the', 'truth', 'come', 'out', 'of', 'their', 'mouths', '.', '[SEP]']\n",
            "Original: ['Not', 'what', 'i', 'expected!']\n",
            "Tokenized: ['[CLS]', 'not', 'what', 'i', 'expected', '!', '[SEP]']\n",
            "Original: ['We', 'read', 'the', 'good', 'reviews', 'before', 'going', 'and', 'had', 'high', 'hopes..', 'but', 'to', 'our', 'dismay', 'it', 'didnt', 'turn', 'out', 'that', 'way!']\n",
            "Tokenized: ['[CLS]', 'we', 'read', 'the', 'good', 'reviews', 'before', 'going', 'and', 'had', 'high', 'hopes', '.', '.', 'but', 'to', 'our', 'dismay', 'it', 'didn', '##t', 'turn', 'out', 'that', 'way', '!', '[SEP]']\n",
            "Original: ['~It', 'took', 'over', '40', 'mins', 'to', 'be', 'taken', 'to', 'our', 'table,', 'once', 'there', 'it', 'took', 'another', '20', 'mins', 'to', 'get', 'our', 'orders', 'and', 'a', 'further', '45', 'mins', 'till', 'our', 'starters', 'landed', 'on', 'our', 'table.']\n",
            "Tokenized: ['[CLS]', '~', 'it', 'took', 'over', '40', 'min', '##s', 'to', 'be', 'taken', 'to', 'our', 'table', ',', 'once', 'there', 'it', 'took', 'another', '20', 'min', '##s', 'to', 'get', 'our', 'orders', 'and', 'a', 'further', '45', 'min', '##s', 'till', 'our', 'starters', 'landed', 'on', 'our', 'table', '.', '[SEP]']\n",
            "Original: ['Very', 'frustrating', 'for', 'a', 'restaurant', 'that', 'has', '1', 'rosette', 'and', 'is', 'supposedly', 'renowned', 'for', 'the', 'service...', 'hmm', 'There', 'was', 'no', \"canape's\", 'or', 'amuse', 'to', 'keep', 'us', 'occupied,', 'once', 'we', 'complained', 'about', 'the', 'wait', 'they', 'took', 'us', 'something', 'to', 'nibble', 'on', 'but', 'that', 'took', 'us', 'getting', 'out', 'our', 'chairs', 'and', 'wondering', 'round', 'to', 'do', 'that', 'and', 'it', 'was', 'impossible', 'to', 'get', 'anyones', 'attention.']\n",
            "Tokenized: ['[CLS]', 'very', 'frustrating', 'for', 'a', 'restaurant', 'that', 'has', '1', 'rose', '##tte', 'and', 'is', 'supposedly', 'renowned', 'for', 'the', 'service', '.', '.', '.', 'hmm', 'there', 'was', 'no', 'can', '##ape', \"'\", 's', 'or', 'am', '##use', 'to', 'keep', 'us', 'occupied', ',', 'once', 'we', 'complained', 'about', 'the', 'wait', 'they', 'took', 'us', 'something', 'to', 'ni', '##bble', 'on', 'but', 'that', 'took', 'us', 'getting', 'out', 'our', 'chairs', 'and', 'wondering', 'round', 'to', 'do', 'that', 'and', 'it', 'was', 'impossible', 'to', 'get', 'anyone', '##s', 'attention', '.', '[SEP]']\n",
            "Original: ['Scallops', 'were', 'overcooked', 'and', 'the', 'foie', 'gras', 'was', 'cold', 'but', 'the', 'rest', 'of', 'the', 'food', 'was', 'lovely.']\n",
            "Tokenized: ['[CLS]', 'sc', '##all', '##ops', 'were', 'over', '##co', '##oked', 'and', 'the', 'f', '##oi', '##e', 'gr', '##as', 'was', 'cold', 'but', 'the', 'rest', 'of', 'the', 'food', 'was', 'lovely', '.', '[SEP]']\n",
            "Original: ['On', 'top', 'of', 'that', 'though', 'they', 'tried', 'to', 'charge', 'us', 'service', 'charge', 'just', 'to', 'rub', 'it', 'it....']\n",
            "Tokenized: ['[CLS]', 'on', 'top', 'of', 'that', 'though', 'they', 'tried', 'to', 'charge', 'us', 'service', 'charge', 'just', 'to', 'rub', 'it', 'it', '.', '.', '.', '.', '[SEP]']\n",
            "Original: [\"Wouldn't\", 'go', 'back', 'as', 'there', 'are', 'a', 'lot', 'of', 'places', 'A', 'LOT', 'better', 'and', 'cheaper.']\n",
            "Tokenized: ['[CLS]', 'wouldn', \"'\", 't', 'go', 'back', 'as', 'there', 'are', 'a', 'lot', 'of', 'places', 'a', 'lot', 'better', 'and', 'cheaper', '.', '[SEP]']\n",
            "Original: ['What', 'a', 'Preschool!']\n",
            "Tokenized: ['[CLS]', 'what', 'a', 'preschool', '!', '[SEP]']\n",
            "Original: ['If', 'you', 'want', 'the', 'best', 'for', 'your', 'child,', \"don't\", 'hesitate', 'in', 'visiting', 'this', 'wornderful', 'school.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'want', 'the', 'best', 'for', 'your', 'child', ',', 'don', \"'\", 't', 'hesitate', 'in', 'visiting', 'this', 'worn', '##der', '##ful', 'school', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'the', 'very', 'best', 'in', 'the', 'Gables.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'the', 'very', 'best', 'in', 'the', 'gables', '.', '[SEP]']\n",
            "Original: ['hard', 'to', 'forgive', 'such', 'an', 'awful', 'margarita', 'and', 'steep', 'prices', 'but', 'the', 'food', 'can', 'be', 'good']\n",
            "Tokenized: ['[CLS]', 'hard', 'to', 'forgive', 'such', 'an', 'awful', 'margarita', 'and', 'steep', 'prices', 'but', 'the', 'food', 'can', 'be', 'good', '[SEP]']\n",
            "Original: ['Great', 'graphic', 'design', 'work!']\n",
            "Tokenized: ['[CLS]', 'great', 'graphic', 'design', 'work', '!', '[SEP]']\n",
            "Original: ['Fresh', 'Design', 'Studio', 'helped', 'jump-start', 'my', 'own', 'business', 'by', 'providing', 'affordable', 'and', 'effective', 'marketing', 'materials:', 'logo,', 'flyers,', 'posters', 'ad', 'design,', 'and', 'more.']\n",
            "Tokenized: ['[CLS]', 'fresh', 'design', 'studio', 'helped', 'jump', '-', 'start', 'my', 'own', 'business', 'by', 'providing', 'affordable', 'and', 'effective', 'marketing', 'materials', ':', 'logo', ',', 'flyers', ',', 'posters', 'ad', 'design', ',', 'and', 'more', '.', '[SEP]']\n",
            "Original: ['They', 'have', 'unbeatable', 'price', 'in', 'town', 'and', 'deliver', 'on', 'time.']\n",
            "Tokenized: ['[CLS]', 'they', 'have', 'un', '##beat', '##able', 'price', 'in', 'town', 'and', 'deliver', 'on', 'time', '.', '[SEP]']\n",
            "Original: ['I', 'enjoy', 'working', 'with', 'this', 'architectural', 'and', 'graphic', 'design', 'firm', 'and', 'will', 'recommend', 'to', 'anyone.']\n",
            "Tokenized: ['[CLS]', 'i', 'enjoy', 'working', 'with', 'this', 'architectural', 'and', 'graphic', 'design', 'firm', 'and', 'will', 'recommend', 'to', 'anyone', '.', '[SEP]']\n",
            "Original: ['Identity', 'Theft']\n",
            "Tokenized: ['[CLS]', 'identity', 'theft', '[SEP]']\n",
            "Original: ['Myself', 'and', 'my', \"fiance's\", 'identity', 'was', 'stolen', 'from', 'the', 'office', 'staff.']\n",
            "Tokenized: ['[CLS]', 'myself', 'and', 'my', 'fiance', \"'\", 's', 'identity', 'was', 'stolen', 'from', 'the', 'office', 'staff', '.', '[SEP]']\n",
            "Original: ['We', 'were', 'told', 'by', 'a', 'detective', 'and', 'asked', 'to', 'check', 'our', 'credit', 'for', 'anything', 'unusual.']\n",
            "Tokenized: ['[CLS]', 'we', 'were', 'told', 'by', 'a', 'detective', 'and', 'asked', 'to', 'check', 'our', 'credit', 'for', 'anything', 'unusual', '.', '[SEP]']\n",
            "Original: ['Luckily', 'they', 'caught', 'the', 'crooks', 'before', 'they', 'did', 'one', 'on', 'us.']\n",
            "Tokenized: ['[CLS]', 'luckily', 'they', 'caught', 'the', 'crook', '##s', 'before', 'they', 'did', 'one', 'on', 'us', '.', '[SEP]']\n",
            "Original: ['It', 'was', 'a', 'black', 'female', 'that', 'use', 'to', 'work', 'in', 'the', 'office.']\n",
            "Tokenized: ['[CLS]', 'it', 'was', 'a', 'black', 'female', 'that', 'use', 'to', 'work', 'in', 'the', 'office', '.', '[SEP]']\n",
            "Original: ['She', 'stole', 'the', 'information', 'and', 'gave', 'it', 'to', 'another', 'guy', 'that', 'did', 'all', 'the', 'work.']\n",
            "Tokenized: ['[CLS]', 'she', 'stole', 'the', 'information', 'and', 'gave', 'it', 'to', 'another', 'guy', 'that', 'did', 'all', 'the', 'work', '.', '[SEP]']\n",
            "Original: ['The', 'other', 'guy', 'was', 'pulled', 'over', 'one', 'day', 'and', 'a', 'cop', 'saw', 'suspicious', 'papers', 'with', 'names', 'and', 'social', 'security', 'numbers', 'on', 'it.']\n",
            "Tokenized: ['[CLS]', 'the', 'other', 'guy', 'was', 'pulled', 'over', 'one', 'day', 'and', 'a', 'cop', 'saw', 'suspicious', 'papers', 'with', 'names', 'and', 'social', 'security', 'numbers', 'on', 'it', '.', '[SEP]']\n",
            "Original: ['Thats', 'how', 'they', 'were', 'caught.']\n",
            "Tokenized: ['[CLS]', 'that', '##s', 'how', 'they', 'were', 'caught', '.', '[SEP]']\n",
            "Original: ['They', 'both', 'went', 'to', 'jail', 'and', 'a', 'new', 'manager', 'was', 'put', 'in', 'charge', 'of', 'the', 'apartments.']\n",
            "Tokenized: ['[CLS]', 'they', 'both', 'went', 'to', 'jail', 'and', 'a', 'new', 'manager', 'was', 'put', 'in', 'charge', 'of', 'the', 'apartments', '.', '[SEP]']\n",
            "Original: ['The', 'apartment', 'across', 'from', 'mine', 'belonged', 'to', 'a', 'gang', 'of', 'hookers.']\n",
            "Tokenized: ['[CLS]', 'the', 'apartment', 'across', 'from', 'mine', 'belonged', 'to', 'a', 'gang', 'of', 'hooker', '##s', '.', '[SEP]']\n",
            "Original: ['Nobody', 'lived', 'there.']\n",
            "Tokenized: ['[CLS]', 'nobody', 'lived', 'there', '.', '[SEP]']\n",
            "Original: ['A', 'girl', 'would', 'show', 'up,', 'then', 'a', 'guy', 'in', 'a', 'nice', 'car', 'would', 'show', 'up.']\n",
            "Tokenized: ['[CLS]', 'a', 'girl', 'would', 'show', 'up', ',', 'then', 'a', 'guy', 'in', 'a', 'nice', 'car', 'would', 'show', 'up', '.', '[SEP]']\n",
            "Original: ['Short', 'time', 'later', 'the', 'guy', 'would', 'leave,', 'then', 'the', 'girl.']\n",
            "Tokenized: ['[CLS]', 'short', 'time', 'later', 'the', 'guy', 'would', 'leave', ',', 'then', 'the', 'girl', '.', '[SEP]']\n",
            "Original: ['My', 'apartment', 'was', 'usually', 'quiet.']\n",
            "Tokenized: ['[CLS]', 'my', 'apartment', 'was', 'usually', 'quiet', '.', '[SEP]']\n",
            "Original: ['I', 'lived', 'in', 'one', 'that', 'did', 'not', 'face', 'the', 'parking', 'lot.']\n",
            "Tokenized: ['[CLS]', 'i', 'lived', 'in', 'one', 'that', 'did', 'not', 'face', 'the', 'parking', 'lot', '.', '[SEP]']\n",
            "Original: ['Parking', 'spaces', 'are', 'just', 'big', 'enough', 'for', 'a', 'Mini', 'Cooper.']\n",
            "Tokenized: ['[CLS]', 'parking', 'spaces', 'are', 'just', 'big', 'enough', 'for', 'a', 'mini', 'cooper', '.', '[SEP]']\n",
            "Original: ['It', 'sucked', 'having', 'an', 'SUV.']\n",
            "Tokenized: ['[CLS]', 'it', 'sucked', 'having', 'an', 'suv', '.', '[SEP]']\n",
            "Original: ['If', 'I', 'found', 'a', 'spot,', 'I', 'couldnt', 'fit', 'in', 'it.']\n",
            "Tokenized: ['[CLS]', 'if', 'i', 'found', 'a', 'spot', ',', 'i', 'couldn', '##t', 'fit', 'in', 'it', '.', '[SEP]']\n",
            "Original: ['Gates', 'worked', '30%', 'of', 'the', 'time', 'at', 'best.']\n",
            "Tokenized: ['[CLS]', 'gates', 'worked', '30', '%', 'of', 'the', 'time', 'at', 'best', '.', '[SEP]']\n",
            "Original: ['Bugs', 'were', 'a', 'small', 'problem,', 'nothing', 'too', 'bad.']\n",
            "Tokenized: ['[CLS]', 'bugs', 'were', 'a', 'small', 'problem', ',', 'nothing', 'too', 'bad', '.', '[SEP]']\n",
            "Original: ['Great', 'Prices,', 'Great', 'service!']\n",
            "Tokenized: ['[CLS]', 'great', 'prices', ',', 'great', 'service', '!', '[SEP]']\n",
            "Original: [\"I've\", 'been', 'to', 'this', 'shop', 'twice', '(once', 'for', 'an', 'inspection', 'and', 'again', 'for', 'an', 'oil', 'change)', 'and', 'they', 'truly', 'live', 'up', 'to', 'their', 'name:', 'Discount!']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 've', 'been', 'to', 'this', 'shop', 'twice', '(', 'once', 'for', 'an', 'inspection', 'and', 'again', 'for', 'an', 'oil', 'change', ')', 'and', 'they', 'truly', 'live', 'up', 'to', 'their', 'name', ':', 'discount', '!', '[SEP]']\n",
            "Original: ['They', 'have', 'all', 'kind', 'of', 'coupons', 'available', 'for', 'car', 'washes,', 'oil', 'changes,', 'state', 'inspection,', 'etc.']\n",
            "Tokenized: ['[CLS]', 'they', 'have', 'all', 'kind', 'of', 'coup', '##ons', 'available', 'for', 'car', 'wash', '##es', ',', 'oil', 'changes', ',', 'state', 'inspection', ',', 'etc', '.', '[SEP]']\n",
            "Original: ['The', 'thing', 'is,', 'you', 'still', 'get', 'high', 'quality', 'service', 'at', 'nicely', 'discounted', 'rates!']\n",
            "Tokenized: ['[CLS]', 'the', 'thing', 'is', ',', 'you', 'still', 'get', 'high', 'quality', 'service', 'at', 'nicely', 'discount', '##ed', 'rates', '!', '[SEP]']\n",
            "Original: ['There', 'is', 'even', 'free', 'coffee', 'and', 'bottles', 'of', 'water', 'if', \"you'd\", 'like.']\n",
            "Tokenized: ['[CLS]', 'there', 'is', 'even', 'free', 'coffee', 'and', 'bottles', 'of', 'water', 'if', 'you', \"'\", 'd', 'like', '.', '[SEP]']\n",
            "Original: ['The', 'owner', 'is', 'a', 'pleasant', 'guy', 'and', 'I', 'would', 'trust', 'my', 'car', 'with', 'him', 'or', 'any', 'of', 'his', 'workers.']\n",
            "Tokenized: ['[CLS]', 'the', 'owner', 'is', 'a', 'pleasant', 'guy', 'and', 'i', 'would', 'trust', 'my', 'car', 'with', 'him', 'or', 'any', 'of', 'his', 'workers', '.', '[SEP]']\n",
            "Original: ['Top', 'notch,', 'all', 'the', 'way!']\n",
            "Tokenized: ['[CLS]', 'top', 'notch', ',', 'all', 'the', 'way', '!', '[SEP]']\n",
            "Original: ['Top', 'notch', 'eats!']\n",
            "Tokenized: ['[CLS]', 'top', 'notch', 'eats', '!', '[SEP]']\n",
            "Original: ['So', 'here', 'we', 'are', 'in', 'Manson.']\n",
            "Tokenized: ['[CLS]', 'so', 'here', 'we', 'are', 'in', 'manson', '.', '[SEP]']\n",
            "Original: ['Manson?']\n",
            "Tokenized: ['[CLS]', 'manson', '?', '[SEP]']\n",
            "Original: ['Yes,', 'Manson.']\n",
            "Tokenized: ['[CLS]', 'yes', ',', 'manson', '.', '[SEP]']\n",
            "Original: ['Right', 'near', 'Chelan.']\n",
            "Tokenized: ['[CLS]', 'right', 'near', 'che', '##lan', '.', '[SEP]']\n",
            "Original: ['Aka', 'Nowheresville.']\n",
            "Tokenized: ['[CLS]', 'aka', 'nowhere', '##sville', '.', '[SEP]']\n",
            "Original: ['And', 'this', 'litttle', 'gem', 'of', 'a', '7-table', 'restaurant', 'is', 'a', 'complete', 'and', 'utterly', 'wonderful', 'surprise.']\n",
            "Tokenized: ['[CLS]', 'and', 'this', 'lit', '##ttle', 'gem', 'of', 'a', '7', '-', 'table', 'restaurant', 'is', 'a', 'complete', 'and', 'utterly', 'wonderful', 'surprise', '.', '[SEP]']\n",
            "Original: ['A', 'short', 'but', 'wide-ranging', 'menu', 'executed', 'with', 'innovative', 'perfection', 'in', 'a', 'cozy', 'hole', 'in', 'the', 'wall', 'just', 'off', 'the', 'main', 'street.']\n",
            "Tokenized: ['[CLS]', 'a', 'short', 'but', 'wide', '-', 'ranging', 'menu', 'executed', 'with', 'innovative', 'perfection', 'in', 'a', 'cozy', 'hole', 'in', 'the', 'wall', 'just', 'off', 'the', 'main', 'street', '.', '[SEP]']\n",
            "Original: ['Fantastic', 'food', 'served', 'without', 'pretense,', 'very', 'reasonably', 'priced', 'wine', 'selections.']\n",
            "Tokenized: ['[CLS]', 'fantastic', 'food', 'served', 'without', 'pre', '##tens', '##e', ',', 'very', 'reasonably', 'priced', 'wine', 'selections', '.', '[SEP]']\n",
            "Original: ['A', 'great', 'place', 'to', 'go', 'for', 'dinner', 'after', 'a', 'day', 'of', 'wine', 'tasting.']\n",
            "Tokenized: ['[CLS]', 'a', 'great', 'place', 'to', 'go', 'for', 'dinner', 'after', 'a', 'day', 'of', 'wine', 'tasting', '.', '[SEP]']\n",
            "Original: ['The', 'food', 'tasted', 'like', 'rat', 'feces']\n",
            "Tokenized: ['[CLS]', 'the', 'food', 'tasted', 'like', 'rat', 'fe', '##ces', '[SEP]']\n",
            "Original: ['these', 'guys', 'were', 'fantastic!']\n",
            "Tokenized: ['[CLS]', 'these', 'guys', 'were', 'fantastic', '!', '[SEP]']\n",
            "Original: ['they', 'fixed', 'my', 'garage', 'doors', 'in', 'literally', 'less', 'than', 'an', 'hour.']\n",
            "Tokenized: ['[CLS]', 'they', 'fixed', 'my', 'garage', 'doors', 'in', 'literally', 'less', 'than', 'an', 'hour', '.', '[SEP]']\n",
            "Original: ['the', 'guy', 'came', 'on', 'time', 'and', \"didn't\", 'take', 'any', 'breaks,', 'he', 'went', 'straight', 'to', 'work', 'and', 'finished', 'the', 'job', 'efficiently', 'and', 'promptly!']\n",
            "Tokenized: ['[CLS]', 'the', 'guy', 'came', 'on', 'time', 'and', 'didn', \"'\", 't', 'take', 'any', 'breaks', ',', 'he', 'went', 'straight', 'to', 'work', 'and', 'finished', 'the', 'job', 'efficiently', 'and', 'promptly', '!', '[SEP]']\n",
            "Original: ['i', \"couldn't\", 'be', 'more', 'happier', 'with', 'the', 'way', 'my', 'garage', 'looks.']\n",
            "Tokenized: ['[CLS]', 'i', 'couldn', \"'\", 't', 'be', 'more', 'happier', 'with', 'the', 'way', 'my', 'garage', 'looks', '.', '[SEP]']\n",
            "Original: ['GREAT', 'JOB', 'GUYS!']\n",
            "Tokenized: ['[CLS]', 'great', 'job', 'guys', '!', '[SEP]']\n",
            "Original: ['Nice', 'and', 'quiet', 'place', 'with', 'cosy', 'living', 'room', 'just', 'outside', 'the', 'city.']\n",
            "Tokenized: ['[CLS]', 'nice', 'and', 'quiet', 'place', 'with', 'co', '##sy', 'living', 'room', 'just', 'outside', 'the', 'city', '.', '[SEP]']\n",
            "Original: ['Not', 'so', 'good']\n",
            "Tokenized: ['[CLS]', 'not', 'so', 'good', '[SEP]']\n",
            "Original: ['Not', 'worth', 'the', 'money.']\n",
            "Tokenized: ['[CLS]', 'not', 'worth', 'the', 'money', '.', '[SEP]']\n",
            "Original: ['Bland', 'and', 'over', 'cooked.']\n",
            "Tokenized: ['[CLS]', 'bland', 'and', 'over', 'cooked', '.', '[SEP]']\n",
            "Original: ['I', 'felt', 'as', 'if', 'I', 'was', 'in', 'an', 'over', 'priced', 'Olive', 'Garden.']\n",
            "Tokenized: ['[CLS]', 'i', 'felt', 'as', 'if', 'i', 'was', 'in', 'an', 'over', 'priced', 'olive', 'garden', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'hoping', 'to', 'have', 'found', 'a', 'regular', 'place', 'to', 'eat.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'hoping', 'to', 'have', 'found', 'a', 'regular', 'place', 'to', 'eat', '.', '[SEP]']\n",
            "Original: ['But', 'not', 'so.']\n",
            "Tokenized: ['[CLS]', 'but', 'not', 'so', '.', '[SEP]']\n",
            "Original: ['Great,', 'and', 'probably', 'the', 'only', 'West', 'Indian', 'spot', 'worth', 'hitting', 'up', 'in', 'Nashville.']\n",
            "Tokenized: ['[CLS]', 'great', ',', 'and', 'probably', 'the', 'only', 'west', 'indian', 'spot', 'worth', 'hitting', 'up', 'in', 'nashville', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'born', 'and', 'raised', 'in', 'Toronto,', 'which', 'has', 'a', 'huge', 'West', 'Indian', '(Trinidadian,', 'Jamaican,', 'etc)', 'population.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'born', 'and', 'raised', 'in', 'toronto', ',', 'which', 'has', 'a', 'huge', 'west', 'indian', '(', 'trinidad', '##ian', ',', 'jamaican', ',', 'etc', ')', 'population', '.', '[SEP]']\n",
            "Original: ['So', 'huge', 'in', 'fact,', 'that', 'Toronto', 'slang', 'is', 'influenced', 'by', 'and', 'has', 'Jamaican', 'references,', 'and', 'Jamaican', 'beef', 'patties', 'are', 'staples', 'in', 'my', 'high', 'school', 'cafeteria.']\n",
            "Tokenized: ['[CLS]', 'so', 'huge', 'in', 'fact', ',', 'that', 'toronto', 'slang', 'is', 'influenced', 'by', 'and', 'has', 'jamaican', 'references', ',', 'and', 'jamaican', 'beef', 'patti', '##es', 'are', 'staples', 'in', 'my', 'high', 'school', 'cafeteria', '.', '[SEP]']\n",
            "Original: ['Anyway,', 'I', 'was', 'practically', 'raised', 'on', 'this', 'stuff,', 'and', 'being', 'a', 'connoisseur', 'of', 'West', 'Indian', 'cuisine,', 'Jamaica', 'Way', 'is', 'a', 'bit', 'toned', 'down', 'to', 'suit', 'the', 'American', 'palette.']\n",
            "Tokenized: ['[CLS]', 'anyway', ',', 'i', 'was', 'practically', 'raised', 'on', 'this', 'stuff', ',', 'and', 'being', 'a', 'con', '##no', '##isse', '##ur', 'of', 'west', 'indian', 'cuisine', ',', 'jamaica', 'way', 'is', 'a', 'bit', 'toned', 'down', 'to', 'suit', 'the', 'american', 'palette', '.', '[SEP]']\n",
            "Original: ['All', 'you', 'have', 'to', 'do', 'to', 'make', 'it', 'authentic', 'Jamaican', 'food,', 'is', 'add', 'a', 'whole', 'lot', 'of', 'pepper.']\n",
            "Tokenized: ['[CLS]', 'all', 'you', 'have', 'to', 'do', 'to', 'make', 'it', 'authentic', 'jamaican', 'food', ',', 'is', 'add', 'a', 'whole', 'lot', 'of', 'pepper', '.', '[SEP]']\n",
            "Original: ['Alot.']\n",
            "Tokenized: ['[CLS]', 'al', '##ot', '.', '[SEP]']\n",
            "Original: ['Friendly', 'staff,', 'but', 'definitely', 'some', 'problems']\n",
            "Tokenized: ['[CLS]', 'friendly', 'staff', ',', 'but', 'definitely', 'some', 'problems', '[SEP]']\n",
            "Original: ['May,', '2009.']\n",
            "Tokenized: ['[CLS]', 'may', ',', '2009', '.', '[SEP]']\n",
            "Original: ['We', 'were', 'booked', 'at', 'the', 'Sheraton', 'with', 'a', 'number', 'of', 'other', 'out-of-town', 'wedding', 'guests.']\n",
            "Tokenized: ['[CLS]', 'we', 'were', 'booked', 'at', 'the', 'she', '##rat', '##on', 'with', 'a', 'number', 'of', 'other', 'out', '-', 'of', '-', 'town', 'wedding', 'guests', '.', '[SEP]']\n",
            "Original: ['Got', 'put', 'into', 'the', 'wrong', 'room', 'the', 'first', 'night,', 'and', 'were', 'quite', 'surprised', 'to', 'have', 'someone', 'with', 'the', 'same', 'room', 'key', 'trying', 'to', 'get', 'in', 'the', 'door', 'at', '1:00', 'am!']\n",
            "Tokenized: ['[CLS]', 'got', 'put', 'into', 'the', 'wrong', 'room', 'the', 'first', 'night', ',', 'and', 'were', 'quite', 'surprised', 'to', 'have', 'someone', 'with', 'the', 'same', 'room', 'key', 'trying', 'to', 'get', 'in', 'the', 'door', 'at', '1', ':', '00', 'am', '!', '[SEP]']\n",
            "Original: ['Next', 'day', 'got', 'moved', 'into', 'another', 'room,', 'on', 'the', 'same', 'floor', 'with', 'other', 'wedding', 'guests.']\n",
            "Tokenized: ['[CLS]', 'next', 'day', 'got', 'moved', 'into', 'another', 'room', ',', 'on', 'the', 'same', 'floor', 'with', 'other', 'wedding', 'guests', '.', '[SEP]']\n",
            "Original: ['There', 'were', '3', 'adults', 'in', 'our', 'room', 'but', 'towels', 'for', 'only', '2,', 'no', 'linens', 'for', 'sofa', 'bed.']\n",
            "Tokenized: ['[CLS]', 'there', 'were', '3', 'adults', 'in', 'our', 'room', 'but', 'towels', 'for', 'only', '2', ',', 'no', 'linen', '##s', 'for', 'sofa', 'bed', '.', '[SEP]']\n",
            "Original: ['In', 'the', 'second', 'room', 'it', 'took', '3', 'tries', 'to', 'get', 'all', 'the', 'towels', 'and', 'linens', 'we', 'requested.']\n",
            "Tokenized: ['[CLS]', 'in', 'the', 'second', 'room', 'it', 'took', '3', 'tries', 'to', 'get', 'all', 'the', 'towels', 'and', 'linen', '##s', 'we', 'requested', '.', '[SEP]']\n",
            "Original: ['A', 'package', 'and', 'some', 'wedding', 'cards', 'were', 'left', 'in', 'our', 'first', 'room.']\n",
            "Tokenized: ['[CLS]', 'a', 'package', 'and', 'some', 'wedding', 'cards', 'were', 'left', 'in', 'our', 'first', 'room', '.', '[SEP]']\n",
            "Original: ['They', 'were', 'sent', 'to', 'our', 'second', 'room', 'and', 'had', 'been', 'opened--the', 'ribbon-wrapped', 'present,', 'and', 'all', '3', 'envelopes.']\n",
            "Tokenized: ['[CLS]', 'they', 'were', 'sent', 'to', 'our', 'second', 'room', 'and', 'had', 'been', 'opened', '-', '-', 'the', 'ribbon', '-', 'wrapped', 'present', ',', 'and', 'all', '3', 'envelope', '##s', '.', '[SEP]']\n",
            "Original: ['Security', 'in', 'the', 'hotel', 'seemed', 'to', 'be', 'excellent,', 'but', 'we', 'were', 'never', 'given', 'an', 'explanation', 'as', 'to', 'why', 'someone', 'would', 'open', 'these', 'items.']\n",
            "Tokenized: ['[CLS]', 'security', 'in', 'the', 'hotel', 'seemed', 'to', 'be', 'excellent', ',', 'but', 'we', 'were', 'never', 'given', 'an', 'explanation', 'as', 'to', 'why', 'someone', 'would', 'open', 'these', 'items', '.', '[SEP]']\n",
            "Original: ['A', 'mid-afternoon', '\"fire', 'drill\"', 'was', 'disruptive,', 'putting', 'everyone', 'out', 'of', 'the', 'hotel.']\n",
            "Tokenized: ['[CLS]', 'a', 'mid', '-', 'afternoon', '\"', 'fire', 'drill', '\"', 'was', 'disrupt', '##ive', ',', 'putting', 'everyone', 'out', 'of', 'the', 'hotel', '.', '[SEP]']\n",
            "Original: ['When', 'we', 'called', 'the', 'front', 'desk', 'about', 'an', 'extremely', 'boisterous', 'crowd', 'in', 'the', 'hall', 'outside', 'our', 'door', 'quite', 'late', 'at', 'night,', 'it', 'seemed', 'to', 'take', 'the', 'hotel', 'staff', 'quite', 'a', 'while', 'to', 'quiet', 'them', 'down.']\n",
            "Tokenized: ['[CLS]', 'when', 'we', 'called', 'the', 'front', 'desk', 'about', 'an', 'extremely', 'bois', '##ter', '##ous', 'crowd', 'in', 'the', 'hall', 'outside', 'our', 'door', 'quite', 'late', 'at', 'night', ',', 'it', 'seemed', 'to', 'take', 'the', 'hotel', 'staff', 'quite', 'a', 'while', 'to', 'quiet', 'them', 'down', '.', '[SEP]']\n",
            "Original: ['The', 'staff', 'was', 'friendly,', 'especially', 'the', 'front', 'desk', 'female', 'supervisor,', 'and', 'seemed', 'to', 'want', 'to', 'help,', 'but', 'too', 'many', 'unusual', 'things', 'happened', 'to', 'make', 'us', 'want', 'to', 'stay', 'there', 'again.']\n",
            "Tokenized: ['[CLS]', 'the', 'staff', 'was', 'friendly', ',', 'especially', 'the', 'front', 'desk', 'female', 'supervisor', ',', 'and', 'seemed', 'to', 'want', 'to', 'help', ',', 'but', 'too', 'many', 'unusual', 'things', 'happened', 'to', 'make', 'us', 'want', 'to', 'stay', 'there', 'again', '.', '[SEP]']\n",
            "Original: ['Excellent', 'piano', 'lessons']\n",
            "Tokenized: ['[CLS]', 'excellent', 'piano', 'lessons', '[SEP]']\n",
            "Original: [\"I'm\", 'very', 'happy', 'with', 'the', 'piano', 'lessons', 'Mrs.', 'Lynda', 'Mcmanus', 'taught', 'me.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'm', 'very', 'happy', 'with', 'the', 'piano', 'lessons', 'mrs', '.', 'l', '##yn', '##da', 'mc', '##man', '##us', 'taught', 'me', '.', '[SEP]']\n",
            "Original: ['Now', \"I'm\", 'able', 'to', 'play', 'the', 'piano', 'pretty', 'well.']\n",
            "Tokenized: ['[CLS]', 'now', 'i', \"'\", 'm', 'able', 'to', 'play', 'the', 'piano', 'pretty', 'well', '.', '[SEP]']\n",
            "Original: ['Got', 'to', 'love', 'this', 'place.']\n",
            "Tokenized: ['[CLS]', 'got', 'to', 'love', 'this', 'place', '.', '[SEP]']\n",
            "Original: ['Everyone', 'is', 'relaxed', 'and', 'having', 'fun!!!']\n",
            "Tokenized: ['[CLS]', 'everyone', 'is', 'relaxed', 'and', 'having', 'fun', '!', '!', '!', '[SEP]']\n",
            "Original: ['Bad', 'Service']\n",
            "Tokenized: ['[CLS]', 'bad', 'service', '[SEP]']\n",
            "Original: ['Definately', \"won't\", 'be', 'returning.']\n",
            "Tokenized: ['[CLS]', 'def', '##inate', '##ly', 'won', \"'\", 't', 'be', 'returning', '.', '[SEP]']\n",
            "Original: ['Travelled', '40mins', 'after', 'calling', 'to', 'see', 'if', 'a', 'product', 'was', 'in', 'stock.']\n",
            "Tokenized: ['[CLS]', 'travelled', '40', '##mins', 'after', 'calling', 'to', 'see', 'if', 'a', 'product', 'was', 'in', 'stock', '.', '[SEP]']\n",
            "Original: ['Told', 'that', 'they', 'had', 'plenty.']\n",
            "Tokenized: ['[CLS]', 'told', 'that', 'they', 'had', 'plenty', '.', '[SEP]']\n",
            "Original: ['Get', 'there', 'and', 'there', 'was', 'nothing.']\n",
            "Tokenized: ['[CLS]', 'get', 'there', 'and', 'there', 'was', 'nothing', '.', '[SEP]']\n",
            "Original: ['Not', 'impressed!!!']\n",
            "Tokenized: ['[CLS]', 'not', 'impressed', '!', '!', '!', '[SEP]']\n",
            "Original: ['Do', 'not', 'use', 'this', 'company!']\n",
            "Tokenized: ['[CLS]', 'do', 'not', 'use', 'this', 'company', '!', '[SEP]']\n",
            "Original: ['I', 'dropped', 'off', 'a', 'sheet', 'metal', 'piece', 'that', 'I', 'needed', 'copied', 'due', 'to', 'th', 'it', 'was', 'needing', 'to', 'be', 'replaced.']\n",
            "Tokenized: ['[CLS]', 'i', 'dropped', 'off', 'a', 'sheet', 'metal', 'piece', 'that', 'i', 'needed', 'copied', 'due', 'to', 'th', 'it', 'was', 'needing', 'to', 'be', 'replaced', '.', '[SEP]']\n",
            "Original: ['I', 'asked', 'if', 'they', 'could', 'copy', 'the', 'piece', 'I', 'dropped', 'off.']\n",
            "Tokenized: ['[CLS]', 'i', 'asked', 'if', 'they', 'could', 'copy', 'the', 'piece', 'i', 'dropped', 'off', '.', '[SEP]']\n",
            "Original: ['They', 'said', 'it', 'would', 'be', 'made', 'exactly', 'like', 'the', 'one', 'I', 'needed', 'to', 'replace.']\n",
            "Tokenized: ['[CLS]', 'they', 'said', 'it', 'would', 'be', 'made', 'exactly', 'like', 'the', 'one', 'i', 'needed', 'to', 'replace', '.', '[SEP]']\n",
            "Original: ['I', 'picked', 'it', 'up', 'when', 'it', 'was', 'finished', 'and', 'was', 'charge', '30.00.']\n",
            "Tokenized: ['[CLS]', 'i', 'picked', 'it', 'up', 'when', 'it', 'was', 'finished', 'and', 'was', 'charge', '30', '.', '00', '.', '[SEP]']\n",
            "Original: ['When', 'I', 'got', 'to', 'the', 'job', 'and', 'tried', 'to', 'insert', 'the', 'new', 'piece', 'of', 'metal', 'IT', 'WOULD', 'NOT', 'FIT!!']\n",
            "Tokenized: ['[CLS]', 'when', 'i', 'got', 'to', 'the', 'job', 'and', 'tried', 'to', 'insert', 'the', 'new', 'piece', 'of', 'metal', 'it', 'would', 'not', 'fit', '!', '!', '[SEP]']\n",
            "Original: ['I', 'took', 'the', 'original', 'piece', 'of', 'metal', 'and', 'rigged', 'it', 'to', 'make', 'due', 'since', 'I', 'had', 'to', 'complete', 'the', 'job.']\n",
            "Tokenized: ['[CLS]', 'i', 'took', 'the', 'original', 'piece', 'of', 'metal', 'and', 'rigged', 'it', 'to', 'make', 'due', 'since', 'i', 'had', 'to', 'complete', 'the', 'job', '.', '[SEP]']\n",
            "Original: ['I', 'took', 'the', 'receipt', 'and', 'the', 'metal', 'that', 'did', 'not', 'fit', 'and', 'asked', 'Pomper', 'for', 'my', 'money', 'back.']\n",
            "Tokenized: ['[CLS]', 'i', 'took', 'the', 'receipt', 'and', 'the', 'metal', 'that', 'did', 'not', 'fit', 'and', 'asked', 'po', '##mp', '##er', 'for', 'my', 'money', 'back', '.', '[SEP]']\n",
            "Original: ['The', 'girl', 'at', 'the', 'desk', 'was', 'sooo', 'rude', 'I', 'could', 'not', 'believe', 'it!']\n",
            "Tokenized: ['[CLS]', 'the', 'girl', 'at', 'the', 'desk', 'was', 'soo', '##o', 'rude', 'i', 'could', 'not', 'believe', 'it', '!', '[SEP]']\n",
            "Original: ['She', 'told', 'me', 'she', 'could', 'not', 'use', 'the', 'piece', 'I', 'was', 'returning', 'and', 'the', 'company', 'would', 'only', 'put', 'it', 'in', 'the', 'trash', 'so', 'I', 'could', 'not', 'return', 'it.']\n",
            "Tokenized: ['[CLS]', 'she', 'told', 'me', 'she', 'could', 'not', 'use', 'the', 'piece', 'i', 'was', 'returning', 'and', 'the', 'company', 'would', 'only', 'put', 'it', 'in', 'the', 'trash', 'so', 'i', 'could', 'not', 'return', 'it', '.', '[SEP]']\n",
            "Original: ['I', 'explained', 'I', 'did', 'not', 'get', 'what', 'I', 'paid', 'for.']\n",
            "Tokenized: ['[CLS]', 'i', 'explained', 'i', 'did', 'not', 'get', 'what', 'i', 'paid', 'for', '.', '[SEP]']\n",
            "Original: ['She', 'asked', 'me', 'to', 'bring', 'the', 'original', 'piece', 'back', 'and', 'I', 'told', 'her', 'I', 'had', 'to', 'use', 'it', 'on', 'the', 'job.']\n",
            "Tokenized: ['[CLS]', 'she', 'asked', 'me', 'to', 'bring', 'the', 'original', 'piece', 'back', 'and', 'i', 'told', 'her', 'i', 'had', 'to', 'use', 'it', 'on', 'the', 'job', '.', '[SEP]']\n",
            "Original: ['She', 'told', 'me', 'that', 'was', 'to', 'bad', 'she', 'would', 'do', 'nothing', 'to', 'help', 'me', 'since', 'she', 'could', 'not', 'use', 'or', 'resell', 'the', 'piece.']\n",
            "Tokenized: ['[CLS]', 'she', 'told', 'me', 'that', 'was', 'to', 'bad', 'she', 'would', 'do', 'nothing', 'to', 'help', 'me', 'since', 'she', 'could', 'not', 'use', 'or', 'res', '##ell', 'the', 'piece', '.', '[SEP]']\n",
            "Original: ['I', 'said', 'I', 'was', 'going', 'to', 'trash', 'it', 'also', 'and', 'could', 'I', 'at', 'least', 'have', 'a', 'credit.']\n",
            "Tokenized: ['[CLS]', 'i', 'said', 'i', 'was', 'going', 'to', 'trash', 'it', 'also', 'and', 'could', 'i', 'at', 'least', 'have', 'a', 'credit', '.', '[SEP]']\n",
            "Original: ['With', 'a', 'smirk', 'on', 'her', 'face', 'she', 'told', 'me', 'NO', 'MONEY', 'IS', 'BEING', 'RETURNED', 'and', 'THAT', 'IS', 'THE', 'WAY', 'IT', 'WAS.']\n",
            "Tokenized: ['[CLS]', 'with', 'a', 'smirk', 'on', 'her', 'face', 'she', 'told', 'me', 'no', 'money', 'is', 'being', 'returned', 'and', 'that', 'is', 'the', 'way', 'it', 'was', '.', '[SEP]']\n",
            "Original: ['DO', 'NOT', 'USE', 'THIS', 'COMPANY.']\n",
            "Tokenized: ['[CLS]', 'do', 'not', 'use', 'this', 'company', '.', '[SEP]']\n",
            "Original: ['There', 'are', 'to', 'many', 'people', 'that', 'need', 'our', 'business', 'to', 'have', 'to', 'put', 'up', 'with', 'this', 'unfair', 'treatment!!!!!']\n",
            "Tokenized: ['[CLS]', 'there', 'are', 'to', 'many', 'people', 'that', 'need', 'our', 'business', 'to', 'have', 'to', 'put', 'up', 'with', 'this', 'unfair', 'treatment', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['First', 'Time', 'Ballerina']\n",
            "Tokenized: ['[CLS]', 'first', 'time', 'ball', '##erina', '[SEP]']\n",
            "Original: ['My', 'daughter', 'is', 'starting', 'ballet', 'this', 'year', 'for', 'the', 'first', 'time.']\n",
            "Tokenized: ['[CLS]', 'my', 'daughter', 'is', 'starting', 'ballet', 'this', 'year', 'for', 'the', 'first', 'time', '.', '[SEP]']\n",
            "Original: [\"I'ma\", 'soccer', 'mom', 'so', 'I', \"wasn't\", 'sure', 'what', 'I', 'was', 'looking', 'for', 'when', 'it', 'comes', 'to', 'dancewear.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'ma', 'soccer', 'mom', 'so', 'i', 'wasn', \"'\", 't', 'sure', 'what', 'i', 'was', 'looking', 'for', 'when', 'it', 'comes', 'to', 'dance', '##wear', '.', '[SEP]']\n",
            "Original: ['The', 'staff', 'was', 'very', 'helpful', 'and', 'gave', 'me', 'exactly', 'what', 'I', 'needed', 'for', 'my', 'first', 'time', 'ballerina.']\n",
            "Tokenized: ['[CLS]', 'the', 'staff', 'was', 'very', 'helpful', 'and', 'gave', 'me', 'exactly', 'what', 'i', 'needed', 'for', 'my', 'first', 'time', 'ball', '##erina', '.', '[SEP]']\n",
            "Original: ['The', 'service', 'at', 'Instep', 'was', 'great!!']\n",
            "Tokenized: ['[CLS]', 'the', 'service', 'at', 'ins', '##te', '##p', 'was', 'great', '!', '!', '[SEP]']\n",
            "Original: ['I', 'would', 'recommend', 'them', 'to', 'anyone!']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'recommend', 'them', 'to', 'anyone', '!', '[SEP]']\n",
            "Original: ['Did', 'a', 'great', 'job', 'of', 'removing', 'my', 'tree', 'in', 'Conyers.']\n",
            "Tokenized: ['[CLS]', 'did', 'a', 'great', 'job', 'of', 'removing', 'my', 'tree', 'in', 'con', '##yer', '##s', '.', '[SEP]']\n",
            "Original: ['Thanks', 'Southland.']\n",
            "Tokenized: ['[CLS]', 'thanks', 'southland', '.', '[SEP]']\n",
            "Original: ['A', 'TERRIBLE', 'EXPERIENCE!']\n",
            "Tokenized: ['[CLS]', 'a', 'terrible', 'experience', '!', '[SEP]']\n",
            "Original: ['I', 'remain', 'unhappy.']\n",
            "Tokenized: ['[CLS]', 'i', 'remain', 'unhappy', '.', '[SEP]']\n",
            "Original: ['I', 'still', 'have', 'noticeable', 'scarring.']\n",
            "Tokenized: ['[CLS]', 'i', 'still', 'have', 'noticeable', 'scar', '##ring', '.', '[SEP]']\n",
            "Original: ['I', 'still', 'have', 'surgically', 'induced', 'hair', 'loss.']\n",
            "Tokenized: ['[CLS]', 'i', 'still', 'have', 'surgical', '##ly', 'induced', 'hair', 'loss', '.', '[SEP]']\n",
            "Original: ['My', 'results', 'were', 'just', 'AWFUL.']\n",
            "Tokenized: ['[CLS]', 'my', 'results', 'were', 'just', 'awful', '.', '[SEP]']\n",
            "Original: ['My', 'post-op', 'treatment', 'was', 'TERRIBLE.']\n",
            "Tokenized: ['[CLS]', 'my', 'post', '-', 'op', 'treatment', 'was', 'terrible', '.', '[SEP]']\n",
            "Original: ['I', \"wouldn't\", 'recommend', 'this', 'place', 'last', 'year,', 'and', 'I', 'certainly', \"wouldn't\", 'recommend', 'them', 'this', 'year.']\n",
            "Tokenized: ['[CLS]', 'i', 'wouldn', \"'\", 't', 'recommend', 'this', 'place', 'last', 'year', ',', 'and', 'i', 'certainly', 'wouldn', \"'\", 't', 'recommend', 'them', 'this', 'year', '.', '[SEP]']\n",
            "Original: ['Fantastic', 'Place', 'to', 'buy', 'your', 'next', 'vehicle']\n",
            "Tokenized: ['[CLS]', 'fantastic', 'place', 'to', 'buy', 'your', 'next', 'vehicle', '[SEP]']\n",
            "Original: ['We', 'have', 'never', 'had', 'a', 'bad', 'experience', 'buying', 'from', 'Edmark.']\n",
            "Tokenized: ['[CLS]', 'we', 'have', 'never', 'had', 'a', 'bad', 'experience', 'buying', 'from', 'ed', '##mark', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'car', 'number', '3', \"we've\", 'purchased', 'through', 'them.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'car', 'number', '3', 'we', \"'\", 've', 'purchased', 'through', 'them', '.', '[SEP]']\n",
            "Original: ['We', 'trust', 'and', 'appreciate', 'Scott', 'Larson', 'and', 'know', 'that', 'he', 'will', 'always', 'take', 'good', 'care', 'of', 'us', 'and', 'listen', 'to', 'our', 'needs!']\n",
            "Tokenized: ['[CLS]', 'we', 'trust', 'and', 'appreciate', 'scott', 'larson', 'and', 'know', 'that', 'he', 'will', 'always', 'take', 'good', 'care', 'of', 'us', 'and', 'listen', 'to', 'our', 'needs', '!', '[SEP]']\n",
            "Original: ['Thank', 'you', 'again', 'for', 'great', 'customer', 'service!']\n",
            "Tokenized: ['[CLS]', 'thank', 'you', 'again', 'for', 'great', 'customer', 'service', '!', '[SEP]']\n",
            "Original: ['Excellent', 'customer', 'service', 'and', 'quality', 'work.']\n",
            "Tokenized: ['[CLS]', 'excellent', 'customer', 'service', 'and', 'quality', 'work', '.', '[SEP]']\n",
            "Original: ['They', 'went', 'the', 'extra', 'mile', 'to', 'repair', 'my', 'cowboy', 'boots--', 'they', 'had', 'to', 'have', 'a', 'special', 'kind', 'of', 'paper', 'that', 'looked', 'like', 'wood', 'grain', 'to', 'fix', 'the', 'heels.']\n",
            "Tokenized: ['[CLS]', 'they', 'went', 'the', 'extra', 'mile', 'to', 'repair', 'my', 'cowboy', 'boots', '-', '-', 'they', 'had', 'to', 'have', 'a', 'special', 'kind', 'of', 'paper', 'that', 'looked', 'like', 'wood', 'grain', 'to', 'fix', 'the', 'heels', '.', '[SEP]']\n",
            "Original: ['That', 'was', '4', 'years', 'ago.']\n",
            "Tokenized: ['[CLS]', 'that', 'was', '4', 'years', 'ago', '.', '[SEP]']\n",
            "Original: ['I', \"haven't\", 'been', 'able', 'to', 'find', 'a', 'shoe', 'repair', 'place', 'in', 'Seattle', 'since', 'that', 'has', 'been', 'able', 'to', 'do', 'it.']\n",
            "Tokenized: ['[CLS]', 'i', 'haven', \"'\", 't', 'been', 'able', 'to', 'find', 'a', 'shoe', 'repair', 'place', 'in', 'seattle', 'since', 'that', 'has', 'been', 'able', 'to', 'do', 'it', '.', '[SEP]']\n",
            "Original: [\"(I've\", 'been', 'through', 'at', 'least', '5', 'places', 'already.)']\n",
            "Tokenized: ['[CLS]', '(', 'i', \"'\", 've', 'been', 'through', 'at', 'least', '5', 'places', 'already', '.', ')', '[SEP]']\n",
            "Original: ['If', 'I', 'had', 'time', 'to', 'drive', 'to', 'Tacoma', 'before', 'they', 'closed', 'during', 'the', 'work', 'week,', 'I', 'would', 'just', 'so', 'I', 'could', 'get', 'those', 'boots', 'fixed', 'properly', 'again.']\n",
            "Tokenized: ['[CLS]', 'if', 'i', 'had', 'time', 'to', 'drive', 'to', 'tacoma', 'before', 'they', 'closed', 'during', 'the', 'work', 'week', ',', 'i', 'would', 'just', 'so', 'i', 'could', 'get', 'those', 'boots', 'fixed', 'properly', 'again', '.', '[SEP]']\n",
            "Original: ['happy', 'customer']\n",
            "Tokenized: ['[CLS]', 'happy', 'customer', '[SEP]']\n",
            "Original: ['Mr.', 'Squeege', 'is', 'THE', 'BEST.']\n",
            "Tokenized: ['[CLS]', 'mr', '.', 'sq', '##ue', '##ege', 'is', 'the', 'best', '.', '[SEP]']\n",
            "Original: ['Prompt,', 'Clean', 'Windows.']\n",
            "Tokenized: ['[CLS]', 'prompt', ',', 'clean', 'windows', '.', '[SEP]']\n",
            "Original: ['Affordable', 'pricing.']\n",
            "Tokenized: ['[CLS]', 'affordable', 'pricing', '.', '[SEP]']\n",
            "Original: ['Friendly', 'responses.']\n",
            "Tokenized: ['[CLS]', 'friendly', 'responses', '.', '[SEP]']\n",
            "Original: ['How', 'else', 'can', 'excellent', 'be', 'described', 'for', 'a', 'business', 'of', 'this', 'sort?']\n",
            "Tokenized: ['[CLS]', 'how', 'else', 'can', 'excellent', 'be', 'described', 'for', 'a', 'business', 'of', 'this', 'sort', '?', '[SEP]']\n",
            "Original: ['They', 'have', 'been', 'my', 'only', '\"go-to\"', 'resource', 'since', 'we', 'first', 'did', 'business', 'together.']\n",
            "Tokenized: ['[CLS]', 'they', 'have', 'been', 'my', 'only', '\"', 'go', '-', 'to', '\"', 'resource', 'since', 'we', 'first', 'did', 'business', 'together', '.', '[SEP]']\n",
            "Original: ['You', 'will', 'find', 'the', 'same', 'to', 'be', 'true', 'for', 'you.']\n",
            "Tokenized: ['[CLS]', 'you', 'will', 'find', 'the', 'same', 'to', 'be', 'true', 'for', 'you', '.', '[SEP]']\n",
            "Original: ['Awesome!!!!']\n",
            "Tokenized: ['[CLS]', 'awesome', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['Yes', 'my', 'G1', 'screen', 'is', 'back', 'working.']\n",
            "Tokenized: ['[CLS]', 'yes', 'my', 'g', '##1', 'screen', 'is', 'back', 'working', '.', '[SEP]']\n",
            "Original: ['They', 'was', 'about', 'to', 'Charge', 'me', '$129...']\n",
            "Tokenized: ['[CLS]', 'they', 'was', 'about', 'to', 'charge', 'me', '$', '129', '.', '.', '.', '[SEP]']\n",
            "Original: ['But', 'i', 'paid', '$100.']\n",
            "Tokenized: ['[CLS]', 'but', 'i', 'paid', '$', '100', '.', '[SEP]']\n",
            "Original: ['they', 'save', 'me', 'from', 'having', 'to', 'deal', 'with', 'Tmobile...']\n",
            "Tokenized: ['[CLS]', 'they', 'save', 'me', 'from', 'having', 'to', 'deal', 'with', 't', '##mobile', '.', '.', '.', '[SEP]']\n",
            "Original: ['Tmobile', 'want', 'to', 'Send', 'of', 'my', 'phone', 'and', 'i', \"didn't\", 'want', 'to', 'go', 'thru', 'that...']\n",
            "Tokenized: ['[CLS]', 't', '##mobile', 'want', 'to', 'send', 'of', 'my', 'phone', 'and', 'i', 'didn', \"'\", 't', 'want', 'to', 'go', 'thru', 'that', '.', '.', '.', '[SEP]']\n",
            "Original: ['3', 'Days', 'For', 'get', 'that...']\n",
            "Tokenized: ['[CLS]', '3', 'days', 'for', 'get', 'that', '.', '.', '.', '[SEP]']\n",
            "Original: ['Great', 'service.']\n",
            "Tokenized: ['[CLS]', 'great', 'service', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'recommend', 'them', 'to', 'anyone..']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'recommend', 'them', 'to', 'anyone', '.', '.', '[SEP]']\n",
            "Original: ['I', 'purchased', 'a', '2-year', 'old', 'certified', 'pre-owned', 'BMW', 'from', 'this', 'dealership.']\n",
            "Tokenized: ['[CLS]', 'i', 'purchased', 'a', '2', '-', 'year', 'old', 'certified', 'pre', '-', 'owned', 'bmw', 'from', 'this', 'dealers', '##hip', '.', '[SEP]']\n",
            "Original: ['The', 'night', 'I', 'drove', 'back', 'home,', 'I', 'found', 'that', 'the', 'rear', 'window', 'has', 'some', 'leakage.']\n",
            "Tokenized: ['[CLS]', 'the', 'night', 'i', 'drove', 'back', 'home', ',', 'i', 'found', 'that', 'the', 'rear', 'window', 'has', 'some', 'leak', '##age', '.', '[SEP]']\n",
            "Original: ['(You', 'can', 'hear', 'the', 'wind', 'while', 'driving', 'on', 'highway.']\n",
            "Tokenized: ['[CLS]', '(', 'you', 'can', 'hear', 'the', 'wind', 'while', 'driving', 'on', 'highway', '.', '[SEP]']\n",
            "Original: ['Very', 'likely', 'it', 'needs', 'a', 'new', 'window', 'seal).']\n",
            "Tokenized: ['[CLS]', 'very', 'likely', 'it', 'needs', 'a', 'new', 'window', 'seal', ')', '.', '[SEP]']\n",
            "Original: ['I', 'admit', 'that', 'I', 'should', 'have', 'paid', 'attention', 'to', 'this', 'kind', 'of', 'little', 'things', 'while', 'test', 'drive.']\n",
            "Tokenized: ['[CLS]', 'i', 'admit', 'that', 'i', 'should', 'have', 'paid', 'attention', 'to', 'this', 'kind', 'of', 'little', 'things', 'while', 'test', 'drive', '.', '[SEP]']\n",
            "Original: ['(But', 'this', 'is', 'a', 'certified', 'car', 'from', 'a', 'dealer.)']\n",
            "Tokenized: ['[CLS]', '(', 'but', 'this', 'is', 'a', 'certified', 'car', 'from', 'a', 'dealer', '.', ')', '[SEP]']\n",
            "Original: ['So', 'I', 'brought', 'the', 'car', 'back', 'the', 'second', 'day.']\n",
            "Tokenized: ['[CLS]', 'so', 'i', 'brought', 'the', 'car', 'back', 'the', 'second', 'day', '.', '[SEP]']\n",
            "Original: ['They', 'told', 'me', 'that', 'this', 'is', 'not', 'under', 'warranty', 'and', 'want', 'to', 'charge', 'me', '$175', 'just', 'to', 'diagnose', 'the', 'problem!']\n",
            "Tokenized: ['[CLS]', 'they', 'told', 'me', 'that', 'this', 'is', 'not', 'under', 'warrant', '##y', 'and', 'want', 'to', 'charge', 'me', '$', '175', 'just', 'to', 'dia', '##gno', '##se', 'the', 'problem', '!', '[SEP]']\n",
            "Original: ['Who', 'knows', 'how', 'much', 'they', 'want', 'me', 'to', 'pay', 'to', 'fix', 'this', 'thing.']\n",
            "Tokenized: ['[CLS]', 'who', 'knows', 'how', 'much', 'they', 'want', 'me', 'to', 'pay', 'to', 'fix', 'this', 'thing', '.', '[SEP]']\n",
            "Original: ['I', 'walked', 'away.']\n",
            "Tokenized: ['[CLS]', 'i', 'walked', 'away', '.', '[SEP]']\n",
            "Original: ['So', 'my', 'advice', 'is', 'that', 'NEVER', 'TRUST', 'THIS', 'DEALER.']\n",
            "Tokenized: ['[CLS]', 'so', 'my', 'advice', 'is', 'that', 'never', 'trust', 'this', 'dealer', '.', '[SEP]']\n",
            "Original: ['STAY', 'AWAY', 'AS', 'FAR', 'AS', 'POSSIBLE.']\n",
            "Tokenized: ['[CLS]', 'stay', 'away', 'as', 'far', 'as', 'possible', '.', '[SEP]']\n",
            "Original: ['One', 'of', 'the', 'worst', 'places']\n",
            "Tokenized: ['[CLS]', 'one', 'of', 'the', 'worst', 'places', '[SEP]']\n",
            "Original: ['This', 'place', 'and', 'its', 'sister', 'store', 'Peking', 'Garden', 'are', 'the', 'worst', 'places', 'to', 'order', 'from.']\n",
            "Tokenized: ['[CLS]', 'this', 'place', 'and', 'its', 'sister', 'store', 'peking', 'garden', 'are', 'the', 'worst', 'places', 'to', 'order', 'from', '.', '[SEP]']\n",
            "Original: ['the', 'food', 'was', 'horrible', 'not', 'cooked', 'like', 'it', 'should', 'be,', 'they', 'got', 'the', 'order', 'wrong', 'on', 'a', 'number', 'of', 'occasions,', 'and', 'once', 'forgot', 'about', 'my', 'order.']\n",
            "Tokenized: ['[CLS]', 'the', 'food', 'was', 'horrible', 'not', 'cooked', 'like', 'it', 'should', 'be', ',', 'they', 'got', 'the', 'order', 'wrong', 'on', 'a', 'number', 'of', 'occasions', ',', 'and', 'once', 'forgot', 'about', 'my', 'order', '.', '[SEP]']\n",
            "Original: ['i', 'had', 'to', 'call', 'back', 'up', 'there', 'two', 'hours', 'later', 'and', 'the', 'lady', '(who', 'claimed', 'to', 'be', 'a', 'manager)', 'said', 'my', 'food', 'was', 'on', 'the', 'way,', 'and', 'she', 'didnt', 'offer', 'to', 'compensate', 'me', 'in', 'any', 'kind', 'of', 'way.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'to', 'call', 'back', 'up', 'there', 'two', 'hours', 'later', 'and', 'the', 'lady', '(', 'who', 'claimed', 'to', 'be', 'a', 'manager', ')', 'said', 'my', 'food', 'was', 'on', 'the', 'way', ',', 'and', 'she', 'didn', '##t', 'offer', 'to', 'compensate', 'me', 'in', 'any', 'kind', 'of', 'way', '.', '[SEP]']\n",
            "Original: ['i', 'waited', 'another', '30', 'mins', 'before', 'receiving', 'my', 'food', 'and', 'it', 'was', 'cold.']\n",
            "Tokenized: ['[CLS]', 'i', 'waited', 'another', '30', 'min', '##s', 'before', 'receiving', 'my', 'food', 'and', 'it', 'was', 'cold', '.', '[SEP]']\n",
            "Original: ['in', 'my', 'opinon', 'this', 'place', 'should', 'be', 'shut', 'down', 'by', 'the', 'health', 'inspector,', 'and', 'anyone', 'who', 'is', 'satisfied', 'with', 'there', 'service', 'and', 'food', 'has', 'never', 'eaten', 'at', 'a', 'real', 'asian', 'restaurant.']\n",
            "Tokenized: ['[CLS]', 'in', 'my', 'op', '##ino', '##n', 'this', 'place', 'should', 'be', 'shut', 'down', 'by', 'the', 'health', 'inspector', ',', 'and', 'anyone', 'who', 'is', 'satisfied', 'with', 'there', 'service', 'and', 'food', 'has', 'never', 'eaten', 'at', 'a', 'real', 'asian', 'restaurant', '.', '[SEP]']\n",
            "Original: ['Dumbest', \"F'ers\", 'ever']\n",
            "Tokenized: ['[CLS]', 'dumb', '##est', 'f', \"'\", 'er', '##s', 'ever', '[SEP]']\n",
            "Original: ['I', 'called', 'dominos', 'tonight,', 'it', 'rang', 'forever,', 'I', 'get', 'put', 'on', 'hold', 'twice', 'without', 'saying', 'a', 'word', 'and', 'FINALLY', 'someone', 'says,', 'MAY', 'I', 'HELP', 'YOU?']\n",
            "Tokenized: ['[CLS]', 'i', 'called', 'domino', '##s', 'tonight', ',', 'it', 'rang', 'forever', ',', 'i', 'get', 'put', 'on', 'hold', 'twice', 'without', 'saying', 'a', 'word', 'and', 'finally', 'someone', 'says', ',', 'may', 'i', 'help', 'you', '?', '[SEP]']\n",
            "Original: ['So', 'I', 'say:', \"I'm\", 'at', 'the', 'Radison', 'Warwick', 'hotel', 'in', 'Rittenhouse', 'Square', '(built', 'in', '1926)', 'do', 'you', 'deliver', 'to', 'the', 'Warwick?']\n",
            "Tokenized: ['[CLS]', 'so', 'i', 'say', ':', 'i', \"'\", 'm', 'at', 'the', 'ra', '##dis', '##on', 'warwick', 'hotel', 'in', 'ri', '##tten', '##house', 'square', '(', 'built', 'in', '1926', ')', 'do', 'you', 'deliver', 'to', 'the', 'warwick', '?', '[SEP]']\n",
            "Original: ['They', 'say', 'no,', 'Warwick', 'in', 'New', 'Jersey,', 'Call', 'New', 'Jersey.']\n",
            "Tokenized: ['[CLS]', 'they', 'say', 'no', ',', 'warwick', 'in', 'new', 'jersey', ',', 'call', 'new', 'jersey', '.', '[SEP]']\n",
            "Original: ['I', 'laugh', 'and', 'say,', 'no,', 'that', 'Warwick', 'is', 'in', 'New', 'York,', 'but', \"I'm\", 'at', 'the', 'Radison-Warwick.']\n",
            "Tokenized: ['[CLS]', 'i', 'laugh', 'and', 'say', ',', 'no', ',', 'that', 'warwick', 'is', 'in', 'new', 'york', ',', 'but', 'i', \"'\", 'm', 'at', 'the', 'ra', '##dis', '##on', '-', 'warwick', '.', '[SEP]']\n",
            "Original: ['And', 'he', 'says:', \"You're\", 'at', 'Warwick', 'in', 'Pennsylvania?']\n",
            "Tokenized: ['[CLS]', 'and', 'he', 'says', ':', 'you', \"'\", 're', 'at', 'warwick', 'in', 'pennsylvania', '?', '[SEP]']\n",
            "Original: ['and', 'I', 'said,', 'YES,', 'CENTER', 'CITY', 'PHILLY,', 'and', 'he', 'says,', 'NO,', 'Warwick', 'is', 'a', 'township,', 'If', \"you're\", 'at', 'a', 'Radison', 'in', 'Warwick', 'thats', 'too', 'far,', 'try', 'dominos', 'in', 'Pottstown.']\n",
            "Tokenized: ['[CLS]', 'and', 'i', 'said', ',', 'yes', ',', 'center', 'city', 'phil', '##ly', ',', 'and', 'he', 'says', ',', 'no', ',', 'warwick', 'is', 'a', 'township', ',', 'if', 'you', \"'\", 're', 'at', 'a', 'ra', '##dis', '##on', 'in', 'warwick', 'that', '##s', 'too', 'far', ',', 'try', 'domino', '##s', 'in', 'pot', '##ts', '##town', '.', '[SEP]']\n",
            "Original: ['I', 'say,', 'NO,', 'I', 'am', 'at', 'the', 'RADISON', 'WARWICK', 'HOTEL', 'in', 'Rittenhouse', 'Square.']\n",
            "Tokenized: ['[CLS]', 'i', 'say', ',', 'no', ',', 'i', 'am', 'at', 'the', 'ra', '##dis', '##on', 'warwick', 'hotel', 'in', 'ri', '##tten', '##house', 'square', '.', '[SEP]']\n",
            "Original: ['He', 'says:', 'I', 'not', 'know', 'that', 'town,', 'I', 'have', 'to', 'get', 'to', 'work,', \"I'm\", 'in', 'PHILLY.']\n",
            "Tokenized: ['[CLS]', 'he', 'says', ':', 'i', 'not', 'know', 'that', 'town', ',', 'i', 'have', 'to', 'get', 'to', 'work', ',', 'i', \"'\", 'm', 'in', 'phil', '##ly', '.', '[SEP]']\n",
            "Original: ['Call', 'dominos', 'in', 'your', 'town.']\n",
            "Tokenized: ['[CLS]', 'call', 'domino', '##s', 'in', 'your', 'town', '.', '[SEP]']\n",
            "Original: ['I', 'SAY', 'LISTEN:', \"I'm\", 'at', '17th', 'and', 'LOCUST,', 'do', 'you', 'deliver', 'there?']\n",
            "Tokenized: ['[CLS]', 'i', 'say', 'listen', ':', 'i', \"'\", 'm', 'at', '17th', 'and', 'locus', '##t', ',', 'do', 'you', 'deliver', 'there', '?', '[SEP]']\n",
            "Original: ['He', 'says,', 'I', 'have', 'to', 'have', 'an', 'exact', 'ADDRESS.']\n",
            "Tokenized: ['[CLS]', 'he', 'says', ',', 'i', 'have', 'to', 'have', 'an', 'exact', 'address', '.', '[SEP]']\n",
            "Original: ['OK,', '1701', 'LOCUST', 'STREET', 'i', 'say.']\n",
            "Tokenized: ['[CLS]', 'ok', ',', '1701', 'locus', '##t', 'street', 'i', 'say', '.', '[SEP]']\n",
            "Original: ['he', 'says:', 'Why', 'you', 'tell', 'me', 'your', 'in', 'WARWICK', 'TOWNSHIP?']\n",
            "Tokenized: ['[CLS]', 'he', 'says', ':', 'why', 'you', 'tell', 'me', 'your', 'in', 'warwick', 'township', '?', '[SEP]']\n",
            "Original: ['He', 'gives', 'the', 'phone', 'to', 'a', 'girl,', 'she', 'says,', 'I', 'have', 'to', 'have', 'your', 'address,', 'I', 'say,', 'do', 'you', 'deliver', 'to', '17th', 'and', 'locust,', 'she', 'says,', 'your', 'exact', 'address,', 'I', 'say', '1-7-0-1', 'Locust,', 'ARe', 'you', 'sure?', 'she', 'asks?']\n",
            "Tokenized: ['[CLS]', 'he', 'gives', 'the', 'phone', 'to', 'a', 'girl', ',', 'she', 'says', ',', 'i', 'have', 'to', 'have', 'your', 'address', ',', 'i', 'say', ',', 'do', 'you', 'deliver', 'to', '17th', 'and', 'locus', '##t', ',', 'she', 'says', ',', 'your', 'exact', 'address', ',', 'i', 'say', '1', '-', '7', '-', '0', '-', '1', 'locus', '##t', ',', 'are', 'you', 'sure', '?', 'she', 'asks', '?', '[SEP]']\n",
            "Original: ['YES', 'I', 'am', 'sure,', 'well,', 'she', 'says,', 'is', 'that', 'ON', '17th', 'STREET.']\n",
            "Tokenized: ['[CLS]', 'yes', 'i', 'am', 'sure', ',', 'well', ',', 'she', 'says', ',', 'is', 'that', 'on', '17th', 'street', '.', '[SEP]']\n",
            "Original: ['Yes,', 'I', 'say.']\n",
            "Tokenized: ['[CLS]', 'yes', ',', 'i', 'say', '.', '[SEP]']\n",
            "Original: ['17th,', 'like', 'over', 'by', '16th', 'and', '15th', 'YES,', 'I', 'say,', 'one', 'mile', 'west', 'of', 'you.']\n",
            "Tokenized: ['[CLS]', '17th', ',', 'like', 'over', 'by', '16th', 'and', '15th', 'yes', ',', 'i', 'say', ',', 'one', 'mile', 'west', 'of', 'you', '.', '[SEP]']\n",
            "Original: [\"You're\", 'at', '7th.']\n",
            "Tokenized: ['[CLS]', 'you', \"'\", 're', 'at', '7th', '.', '[SEP]']\n",
            "Original: ['I', 'am', 'just', 'south', 'of', 'Walnut.']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'just', 'south', 'of', 'walnut', '.', '[SEP]']\n",
            "Original: ['She', 'says,', 'Is', 'that', '17th', 'like', 'over', 'past', 'broad.']\n",
            "Tokenized: ['[CLS]', 'she', 'says', ',', 'is', 'that', '17th', 'like', 'over', 'past', 'broad', '.', '[SEP]']\n",
            "Original: ['YES,', 'I', 'am', 'west', 'of', 'broad.']\n",
            "Tokenized: ['[CLS]', 'yes', ',', 'i', 'am', 'west', 'of', 'broad', '.', '[SEP]']\n",
            "Original: ['Broad,', 'I', 'say,', 'is', '14th', 'street', 'and', 'I', 'am', '3', 'blocks', 'west', 'of', 'broad', 'and', 'one', 'south', 'of', 'walnut.']\n",
            "Tokenized: ['[CLS]', 'broad', ',', 'i', 'say', ',', 'is', '14th', 'street', 'and', 'i', 'am', '3', 'blocks', 'west', 'of', 'broad', 'and', 'one', 'south', 'of', 'walnut', '.', '[SEP]']\n",
            "Original: ['Hmmm,', 'she', 'says,', 'Then', 'why', 'are', 'you', 'calling', 'here,', 'we', \"don't\", 'go', 'past', 'broad?']\n",
            "Tokenized: ['[CLS]', 'hmm', '##m', ',', 'she', 'says', ',', 'then', 'why', 'are', 'you', 'calling', 'here', ',', 'we', 'don', \"'\", 't', 'go', 'past', 'broad', '?', '[SEP]']\n",
            "Original: ['Anyway,', 'after', 'much', 'yelling', 'and', 'cussing', 'I', 'hung', 'up,', 'grabbed', 'a', 'cab,', 'and', 'went', 'to', \"Geno's.\"]\n",
            "Tokenized: ['[CLS]', 'anyway', ',', 'after', 'much', 'yelling', 'and', 'cu', '##ssing', 'i', 'hung', 'up', ',', 'grabbed', 'a', 'cab', ',', 'and', 'went', 'to', 'gen', '##o', \"'\", 's', '.', '[SEP]']\n",
            "Original: ['Overpriced']\n",
            "Tokenized: ['[CLS]', 'over', '##pr', '##ice', '##d', '[SEP]']\n",
            "Original: ['This', 'place', 'is', 'identical', 'to', 'the', 'Youngstown', 'Sports', 'Grille,', 'so', 'I', 'imagine', 'they', 'are', 'owned/operated', 'by', 'the', 'same', 'people.']\n",
            "Tokenized: ['[CLS]', 'this', 'place', 'is', 'identical', 'to', 'the', 'young', '##stown', 'sports', 'grille', ',', 'so', 'i', 'imagine', 'they', 'are', 'owned', '/', 'operated', 'by', 'the', 'same', 'people', '.', '[SEP]']\n",
            "Original: ['The', 'food', 'is', 'mediocre', 'at', 'best,', 'and', 'largely', 'overpriced', 'given', 'the', 'portion', 'size', 'and', 'quality.']\n",
            "Tokenized: ['[CLS]', 'the', 'food', 'is', 'med', '##io', '##cre', 'at', 'best', ',', 'and', 'largely', 'over', '##pr', '##ice', '##d', 'given', 'the', 'portion', 'size', 'and', 'quality', '.', '[SEP]']\n",
            "Original: [\"Don't\", 'even', 'get', 'me', 'started', 'on', 'how', 'expensive', 'it', 'is', 'to', 'drink', 'there.']\n",
            "Tokenized: ['[CLS]', 'don', \"'\", 't', 'even', 'get', 'me', 'started', 'on', 'how', 'expensive', 'it', 'is', 'to', 'drink', 'there', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'ate', 'here', '3', 'times', 'since', 'they', 'first', 'opened,', 'and', 'the', 'service', 'has', 'been', 'poor', 'each', 'time,', 'the', 'staff', 'always', 'comes', 'across', 'as', 'somewhat', 'rude', 'and', 'slow.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'ate', 'here', '3', 'times', 'since', 'they', 'first', 'opened', ',', 'and', 'the', 'service', 'has', 'been', 'poor', 'each', 'time', ',', 'the', 'staff', 'always', 'comes', 'across', 'as', 'somewhat', 'rude', 'and', 'slow', '.', '[SEP]']\n",
            "Original: ['Caldwell', 'insurance', 'has', 'been', 'doing', 'our', 'insurance', 'for', 'a', 'couple', 'years', 'now', 'and', 'they', 'have', 'been', 'extremely', 'thorough.']\n",
            "Tokenized: ['[CLS]', 'caldwell', 'insurance', 'has', 'been', 'doing', 'our', 'insurance', 'for', 'a', 'couple', 'years', 'now', 'and', 'they', 'have', 'been', 'extremely', 'thorough', '.', '[SEP]']\n",
            "Original: [\"We've\", 'only', 'had', 'one', 'urgent', 'issue', 'to', 'deal', 'with', 'and', 'they', 'were', 'very', 'prompt', 'in', 'their', 'response.']\n",
            "Tokenized: ['[CLS]', 'we', \"'\", 've', 'only', 'had', 'one', 'urgent', 'issue', 'to', 'deal', 'with', 'and', 'they', 'were', 'very', 'prompt', 'in', 'their', 'response', '.', '[SEP]']\n",
            "Original: ['Highly', 'recommend!']\n",
            "Tokenized: ['[CLS]', 'highly', 'recommend', '!', '[SEP]']\n",
            "Original: ['local', 'crew!!!']\n",
            "Tokenized: ['[CLS]', 'local', 'crew', '!', '!', '!', '[SEP]']\n",
            "Original: ['home', 'team', '-', 'thanks', '4', 'playin!!!']\n",
            "Tokenized: ['[CLS]', 'home', 'team', '-', 'thanks', '4', 'play', '##in', '!', '!', '!', '[SEP]']\n",
            "Original: ['Easy', 'registration,', 'helpful', 'staff', 'and', 'fun', 'teachers!']\n",
            "Tokenized: ['[CLS]', 'easy', 'registration', ',', 'helpful', 'staff', 'and', 'fun', 'teachers', '!', '[SEP]']\n",
            "Original: ['My', 'favorite', 'place', 'to', 'eat.']\n",
            "Tokenized: ['[CLS]', 'my', 'favorite', 'place', 'to', 'eat', '.', '[SEP]']\n",
            "Original: ['The', 'Atmosphere', 'is', 'the', 'best..', 'Italian', 'music,', 'candles,', 'helpful', 'and', 'friendly', 'staff...', 'And', 'the', 'food', 'is', 'beautiful', 'too', '!']\n",
            "Tokenized: ['[CLS]', 'the', 'atmosphere', 'is', 'the', 'best', '.', '.', 'italian', 'music', ',', 'candles', ',', 'helpful', 'and', 'friendly', 'staff', '.', '.', '.', 'and', 'the', 'food', 'is', 'beautiful', 'too', '!', '[SEP]']\n",
            "Original: ['Great', 'Manicure']\n",
            "Tokenized: ['[CLS]', 'great', 'mani', '##cure', '[SEP]']\n",
            "Original: ['This', 'place', 'offers', 'a', 'great', 'manicure', 'and', 'pedicure.']\n",
            "Tokenized: ['[CLS]', 'this', 'place', 'offers', 'a', 'great', 'mani', '##cure', 'and', 'pe', '##dic', '##ure', '.', '[SEP]']\n",
            "Original: ['My', 'nails', 'looked', 'great', 'for', 'the', 'better', 'part', 'of', '2', 'weeks!']\n",
            "Tokenized: ['[CLS]', 'my', 'nails', 'looked', 'great', 'for', 'the', 'better', 'part', 'of', '2', 'weeks', '!', '[SEP]']\n",
            "Original: ['Also', 'very', 'friendly', 'and', 'the', 'stylists', 'are', 'not', 'in', 'the', '\"been', 'there/done', 'that\"', 'mood!']\n",
            "Tokenized: ['[CLS]', 'also', 'very', 'friendly', 'and', 'the', 'st', '##yl', '##ists', 'are', 'not', 'in', 'the', '\"', 'been', 'there', '/', 'done', 'that', '\"', 'mood', '!', '[SEP]']\n",
            "Original: ['Hidden', 'Gem', 'in', 'Alpharetta']\n",
            "Tokenized: ['[CLS]', 'hidden', 'gem', 'in', 'alpha', '##ret', '##ta', '[SEP]']\n",
            "Original: ['This', 'French', 'born,', 'French', 'trained', 'chef', 'and', 'his', 'creative', 'partners', 'offer', 'a', 'taste', 'fresh,', 'locally', 'sourced,', 'fabulously', 'prepared', 'food', 'in', 'the', 'most', 'unlikely', 'of', 'locations.']\n",
            "Tokenized: ['[CLS]', 'this', 'french', 'born', ',', 'french', 'trained', 'chef', 'and', 'his', 'creative', 'partners', 'offer', 'a', 'taste', 'fresh', ',', 'locally', 'sourced', ',', 'fabulous', '##ly', 'prepared', 'food', 'in', 'the', 'most', 'unlikely', 'of', 'locations', '.', '[SEP]']\n",
            "Original: ['Try', 'their', 'weekend', '\"tastings\"', 'which', 'you', 'can', 'learn', 'about', 'by', 'getting', 'on', 'their', 'weekly', 'email', 'list.']\n",
            "Tokenized: ['[CLS]', 'try', 'their', 'weekend', '\"', 'tasting', '##s', '\"', 'which', 'you', 'can', 'learn', 'about', 'by', 'getting', 'on', 'their', 'weekly', 'email', 'list', '.', '[SEP]']\n",
            "Original: [\"It's\", 'the', 'best', 'meal', 'for', 'the', 'money', 'you', 'will', 'find', 'in', 'all', 'of', 'metro', 'Atlanta.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'the', 'best', 'meal', 'for', 'the', 'money', 'you', 'will', 'find', 'in', 'all', 'of', 'metro', 'atlanta', '.', '[SEP]']\n",
            "Original: ['Great', 'work!']\n",
            "Tokenized: ['[CLS]', 'great', 'work', '!', '[SEP]']\n",
            "Original: ['The', 'people', 'at', 'Gulf', 'Coast', 'Siding', 'were', 'very', 'easy', 'and', 'clear', 'to', 'work', 'with.']\n",
            "Tokenized: ['[CLS]', 'the', 'people', 'at', 'gulf', 'coast', 'siding', 'were', 'very', 'easy', 'and', 'clear', 'to', 'work', 'with', '.', '[SEP]']\n",
            "Original: ['They', 'walked', 'me', 'through', 'all', 'the', 'steps', 'involved', 'in', 'the', 'the', 'installation', 'project', 'so', 'that', 'there', 'were', 'no', 'surprises.']\n",
            "Tokenized: ['[CLS]', 'they', 'walked', 'me', 'through', 'all', 'the', 'steps', 'involved', 'in', 'the', 'the', 'installation', 'project', 'so', 'that', 'there', 'were', 'no', 'surprises', '.', '[SEP]']\n",
            "Original: ['I', 'found', 'them', 'extremely', 'professional', 'and', 'would', 'highly', 'recommend', 'them.']\n",
            "Tokenized: ['[CLS]', 'i', 'found', 'them', 'extremely', 'professional', 'and', 'would', 'highly', 'recommend', 'them', '.', '[SEP]']\n",
            "Original: ['Thanks']\n",
            "Tokenized: ['[CLS]', 'thanks', '[SEP]']\n",
            "Original: ['the', 'service', 'is', 'quick.']\n",
            "Tokenized: ['[CLS]', 'the', 'service', 'is', 'quick', '.', '[SEP]']\n",
            "Original: ['and', 'the', 'people', 'are', 'sweet', ':)']\n",
            "Tokenized: ['[CLS]', 'and', 'the', 'people', 'are', 'sweet', ':', ')', '[SEP]']\n",
            "Original: ['The', 'best', 'company', 'in', 'Phuket', 'for', 'creating', 'website', 'and', 'e-commerce', 'website.']\n",
            "Tokenized: ['[CLS]', 'the', 'best', 'company', 'in', 'ph', '##uke', '##t', 'for', 'creating', 'website', 'and', 'e', '-', 'commerce', 'website', '.', '[SEP]']\n",
            "Original: ['From', 'first', 'meeting', 'with', 'them', 'to', 'launch', 'of', 'my', 'website,', 'everything', 'went', 'smooth', 'and', 'on', 'schedule.']\n",
            "Tokenized: ['[CLS]', 'from', 'first', 'meeting', 'with', 'them', 'to', 'launch', 'of', 'my', 'website', ',', 'everything', 'went', 'smooth', 'and', 'on', 'schedule', '.', '[SEP]']\n",
            "Original: ['Highly', 'recommended', 'for', 'who', 'wants', 'to', 'have', 'website.']\n",
            "Tokenized: ['[CLS]', 'highly', 'recommended', 'for', 'who', 'wants', 'to', 'have', 'website', '.', '[SEP]']\n",
            "Original: ['Great', 'with', 'SEO', 'as', 'well.']\n",
            "Tokenized: ['[CLS]', 'great', 'with', 'seo', 'as', 'well', '.', '[SEP]']\n",
            "Original: ['My', 'favorite', 'florist!']\n",
            "Tokenized: ['[CLS]', 'my', 'favorite', 'fl', '##oris', '##t', '!', '[SEP]']\n",
            "Original: ['I', 'came', 'to', 'La', 'Crosse', 'to', 'go', 'to', 'college,', 'and', 'my', 'mom', 'would', 'send', 'me', 'birthday', 'flowers', 'though', 'here.']\n",
            "Tokenized: ['[CLS]', 'i', 'came', 'to', 'la', 'cross', '##e', 'to', 'go', 'to', 'college', ',', 'and', 'my', 'mom', 'would', 'send', 'me', 'birthday', 'flowers', 'though', 'here', '.', '[SEP]']\n",
            "Original: ['They', 'were', 'beautiful', 'and', 'lasted', 'forever!']\n",
            "Tokenized: ['[CLS]', 'they', 'were', 'beautiful', 'and', 'lasted', 'forever', '!', '[SEP]']\n",
            "Original: ['Now', 'that', 'I', 'live', 'here,', 'this', 'is', 'my', 'favorite', 'place', 'to', 'grab', 'flowers', 'for', 'friends', 'and', 'coworkers!']\n",
            "Tokenized: ['[CLS]', 'now', 'that', 'i', 'live', 'here', ',', 'this', 'is', 'my', 'favorite', 'place', 'to', 'grab', 'flowers', 'for', 'friends', 'and', 'cow', '##or', '##kers', '!', '[SEP]']\n",
            "Original: ['Hands', 'down,', 'best', 'place', 'in', 'the', 'area!']\n",
            "Tokenized: ['[CLS]', 'hands', 'down', ',', 'best', 'place', 'in', 'the', 'area', '!', '[SEP]']\n",
            "Original: ['Great', 'service']\n",
            "Tokenized: ['[CLS]', 'great', 'service', '[SEP]']\n",
            "Original: ['A', 'very', 'well', 'established', 'service', 'with', 'a', 'satisfying', 'outcome.']\n",
            "Tokenized: ['[CLS]', 'a', 'very', 'well', 'established', 'service', 'with', 'a', 'satisfying', 'outcome', '.', '[SEP]']\n",
            "Original: ['A', 'well', 'communicated', 'and', 'will', 'be', 'hireing', 'again', 'for', 'another', 'projects......']\n",
            "Tokenized: ['[CLS]', 'a', 'well', 'communicated', 'and', 'will', 'be', 'hire', '##ing', 'again', 'for', 'another', 'projects', '.', '.', '.', '.', '.', '.', '[SEP]']\n",
            "Original: ['Thanks']\n",
            "Tokenized: ['[CLS]', 'thanks', '[SEP]']\n",
            "Original: ['You', 'were', 'extremely', 'polite', 'and', 'professional.']\n",
            "Tokenized: ['[CLS]', 'you', 'were', 'extremely', 'polite', 'and', 'professional', '.', '[SEP]']\n",
            "Original: ['Very', 'Impressed.']\n",
            "Tokenized: ['[CLS]', 'very', 'impressed', '.', '[SEP]']\n",
            "Original: ['Great', 'electrician.']\n",
            "Tokenized: ['[CLS]', 'great', 'electric', '##ian', '.', '[SEP]']\n",
            "Original: ['I', 'called', 'on', 'a', 'Friday', 'at', '12:30', 'complaining', 'of', 'a', 'severe', 'toothache.']\n",
            "Tokenized: ['[CLS]', 'i', 'called', 'on', 'a', 'friday', 'at', '12', ':', '30', 'complaining', 'of', 'a', 'severe', 'tooth', '##ache', '.', '[SEP]']\n",
            "Original: ['Dr.', 'Obina', 'told', 'me', 'that', 'his', 'office', 'closed', 'at', 'noon', 'and', 'that', 'I', 'should', 'call', 'him', 'on', 'Monday.']\n",
            "Tokenized: ['[CLS]', 'dr', '.', 'ob', '##ina', 'told', 'me', 'that', 'his', 'office', 'closed', 'at', 'noon', 'and', 'that', 'i', 'should', 'call', 'him', 'on', 'monday', '.', '[SEP]']\n",
            "Original: ['I', 'had', 'been', 'a', 'patient', 'of', 'Dr.', 'Olbina', 'for', '9', 'years', 'and', 'had', 'spent', 'thousands', 'of', 'dollars', 'on', 'crowns', 'etc.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'been', 'a', 'patient', 'of', 'dr', '.', 'ol', '##bina', 'for', '9', 'years', 'and', 'had', 'spent', 'thousands', 'of', 'dollars', 'on', 'crowns', 'etc', '.', '[SEP]']\n",
            "Original: ['There', 'are', 'plenty', 'of', 'good', 'dentists', 'in', 'Fernandina.']\n",
            "Tokenized: ['[CLS]', 'there', 'are', 'plenty', 'of', 'good', 'dentist', '##s', 'in', 'fern', '##and', '##ina', '.', '[SEP]']\n",
            "Original: [\"Don't\", 'go', 'to', 'Amelia', 'Gentle', 'Dentistry.']\n",
            "Tokenized: ['[CLS]', 'don', \"'\", 't', 'go', 'to', 'amelia', 'gentle', 'dentistry', '.', '[SEP]']\n",
            "Original: ['A', 'Definite', 'No']\n",
            "Tokenized: ['[CLS]', 'a', 'definite', 'no', '[SEP]']\n",
            "Original: ['Incompetent', 'servers,', 'kitchen', 'and', 'management.']\n",
            "Tokenized: ['[CLS]', 'inc', '##omp', '##ete', '##nt', 'servers', ',', 'kitchen', 'and', 'management', '.', '[SEP]']\n",
            "Original: ['Expect', 'either', 'undercooked', 'or', 'mushy', 'food', 'and', 'lackluster', 'service.']\n",
            "Tokenized: ['[CLS]', 'expect', 'either', 'under', '##co', '##oked', 'or', 'mu', '##sh', '##y', 'food', 'and', 'lack', '##lus', '##ter', 'service', '.', '[SEP]']\n",
            "Original: ['The', 'investors', 'put', 'big', 'bucks', 'into', 'the', 'building', 'but', 'are', 'clueless', 'about', 'what', 'makes', 'a', 'good', 'dining', 'or', 'bar', 'experience.']\n",
            "Tokenized: ['[CLS]', 'the', 'investors', 'put', 'big', 'bucks', 'into', 'the', 'building', 'but', 'are', 'clue', '##less', 'about', 'what', 'makes', 'a', 'good', 'dining', 'or', 'bar', 'experience', '.', '[SEP]']\n",
            "Original: ['Even', 'the', 'least', 'discriminating', 'diner', 'would', 'know', 'not', 'to', 'eat', 'at', \"Sprecher's.\"]\n",
            "Tokenized: ['[CLS]', 'even', 'the', 'least', 'disc', '##rim', '##inating', 'diner', 'would', 'know', 'not', 'to', 'eat', 'at', 'sp', '##re', '##cher', \"'\", 's', '.', '[SEP]']\n",
            "Original: ['Friendly', 'Efficient', 'and', 'overall', 'great', 'place', 'for', 'people', 'in', 'chronic', 'intractable', 'pain']\n",
            "Tokenized: ['[CLS]', 'friendly', 'efficient', 'and', 'overall', 'great', 'place', 'for', 'people', 'in', 'chronic', 'intra', '##ctable', 'pain', '[SEP]']\n",
            "Original: ['Great', 'place', 'for', 'people', 'in', 'chronic', 'pain.']\n",
            "Tokenized: ['[CLS]', 'great', 'place', 'for', 'people', 'in', 'chronic', 'pain', '.', '[SEP]']\n",
            "Original: ['Staff', 'is', 'very', 'friendly', 'they', 'treat', 'you', 'like', 'a', 'human', 'being', 'and', 'not', 'just', 'another', 'patient.']\n",
            "Tokenized: ['[CLS]', 'staff', 'is', 'very', 'friendly', 'they', 'treat', 'you', 'like', 'a', 'human', 'being', 'and', 'not', 'just', 'another', 'patient', '.', '[SEP]']\n",
            "Original: ['Very', 'efficient', 'at', 'treating', 'chronic', 'pain!']\n",
            "Tokenized: ['[CLS]', 'very', 'efficient', 'at', 'treating', 'chronic', 'pain', '!', '[SEP]']\n",
            "Original: ['Slowest,', 'Unfriendly', 'Sstaff', 'on', 'Weekends']\n",
            "Tokenized: ['[CLS]', 'slow', '##est', ',', 'un', '##fr', '##ien', '##dly', 'ss', '##taff', 'on', 'weekends', '[SEP]']\n",
            "Original: ['There', 'are', 'three', 'Starbucks', 'locations', 'that', 'I', 'frequent.']\n",
            "Tokenized: ['[CLS]', 'there', 'are', 'three', 'starbucks', 'locations', 'that', 'i', 'frequent', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'a', 'bit', 'of', 'experience', 'watching', 'the', 'usual', 'assembly', 'line.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'a', 'bit', 'of', 'experience', 'watching', 'the', 'usual', 'assembly', 'line', '.', '[SEP]']\n",
            "Original: ['I', 'also', 'understand', 'that', 'weekend', 'staffs', 'are', 'different', 'than', 'daytime', 'staffs', 'and', 'not', 'necessarily', 'Starbucks', 'A-team', 'or', 'even', 'full-time.']\n",
            "Tokenized: ['[CLS]', 'i', 'also', 'understand', 'that', 'weekend', 'staff', '##s', 'are', 'different', 'than', 'daytime', 'staff', '##s', 'and', 'not', 'necessarily', 'starbucks', 'a', '-', 'team', 'or', 'even', 'full', '-', 'time', '.', '[SEP]']\n",
            "Original: ['But', 'this', 'location', 'has', 'the', 'worst', 'weekend', 'staff', \"I've\", 'seen', 'EVER.']\n",
            "Tokenized: ['[CLS]', 'but', 'this', 'location', 'has', 'the', 'worst', 'weekend', 'staff', 'i', \"'\", 've', 'seen', 'ever', '.', '[SEP]']\n",
            "Original: ['Good', 'honest', 'wrok']\n",
            "Tokenized: ['[CLS]', 'good', 'honest', 'wr', '##ok', '[SEP]']\n",
            "Original: ['Harlan', 'provides', 'great', 'service.']\n",
            "Tokenized: ['[CLS]', 'harlan', 'provides', 'great', 'service', '.', '[SEP]']\n",
            "Original: ['He', 'is', 'very', 'knowledgeable', 'and', 'took', 'the', 'time', 'to', 'explain', 'the', 'repairs', 'to', 'me.']\n",
            "Tokenized: ['[CLS]', 'he', 'is', 'very', 'knowledge', '##able', 'and', 'took', 'the', 'time', 'to', 'explain', 'the', 'repairs', 'to', 'me', '.', '[SEP]']\n",
            "Original: ['The', 'work', 'on', 'my', 'car', 'was', 'done', 'quickly', 'and', 'I', 'felt', 'I', 'could', 'trust', 'his', 'work.']\n",
            "Tokenized: ['[CLS]', 'the', 'work', 'on', 'my', 'car', 'was', 'done', 'quickly', 'and', 'i', 'felt', 'i', 'could', 'trust', 'his', 'work', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'nothing', 'but', 'fantastic', 'things', 'to', 'say.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'nothing', 'but', 'fantastic', 'things', 'to', 'say', '.', '[SEP]']\n",
            "Original: ['I', 'highly', 'recommend', 'his', 'shop.']\n",
            "Tokenized: ['[CLS]', 'i', 'highly', 'recommend', 'his', 'shop', '.', '[SEP]']\n",
            "Original: ['High', 'guality', 'pup', 'food', 'at', 'a', 'good', 'price.']\n",
            "Tokenized: ['[CLS]', 'high', 'gu', '##ality', 'pup', 'food', 'at', 'a', 'good', 'price', '.', '[SEP]']\n",
            "Original: ['Great', 'Place', 'To', 'Use', 'The', 'Fix', 'appliances', 'Plumbing', 'Air', 'Conditioning', '&', 'Electric', 'Problems.']\n",
            "Tokenized: ['[CLS]', 'great', 'place', 'to', 'use', 'the', 'fix', 'appliances', 'plumbing', 'air', 'conditioning', '&', 'electric', 'problems', '.', '[SEP]']\n",
            "Original: ['We', 'have', 'used', 'them', 'for', 'plumbing', '&', 'A/C', 'and', 'they', 'are', 'affordable', 'and', 'get', 'the', 'work', 'done', 'right.']\n",
            "Tokenized: ['[CLS]', 'we', 'have', 'used', 'them', 'for', 'plumbing', '&', 'a', '/', 'c', 'and', 'they', 'are', 'affordable', 'and', 'get', 'the', 'work', 'done', 'right', '.', '[SEP]']\n",
            "Original: ['Great', 'place', '5', 'stars', 'for', 'sure.']\n",
            "Tokenized: ['[CLS]', 'great', 'place', '5', 'stars', 'for', 'sure', '.', '[SEP]']\n",
            "Original: ['Thanks', 'From', 'Bill']\n",
            "Tokenized: ['[CLS]', 'thanks', 'from', 'bill', '[SEP]']\n",
            "Original: ['Its', 'been', 'a', 'few', 'years', 'since', 'I', 'have', 'been', 'to', 'Ipanema.']\n",
            "Tokenized: ['[CLS]', 'its', 'been', 'a', 'few', 'years', 'since', 'i', 'have', 'been', 'to', 'ipa', '##nem', '##a', '.', '[SEP]']\n",
            "Original: ['But', 'my', 'wife', 'and', 'I', 'first', 'went', 'there', 'thinking', 'it', 'would', 'be', 'Brazilian', 'food', '(think', 'lots', 'of', 'meat),', 'but', 'it', 'turned', 'out', 'to', 'be', 'a', 'vegan', 'restaurant!']\n",
            "Tokenized: ['[CLS]', 'but', 'my', 'wife', 'and', 'i', 'first', 'went', 'there', 'thinking', 'it', 'would', 'be', 'brazilian', 'food', '(', 'think', 'lots', 'of', 'meat', ')', ',', 'but', 'it', 'turned', 'out', 'to', 'be', 'a', 'vega', '##n', 'restaurant', '!', '[SEP]']\n",
            "Original: ['And,', 'I', 'can', 'say,', 'this', 'was', 'one', 'of', 'my', 'favorite', 'places', 'to', 'eat', 'in', 'all', 'of', 'Richmond.']\n",
            "Tokenized: ['[CLS]', 'and', ',', 'i', 'can', 'say', ',', 'this', 'was', 'one', 'of', 'my', 'favorite', 'places', 'to', 'eat', 'in', 'all', 'of', 'richmond', '.', '[SEP]']\n",
            "Original: ['Amazing!']\n",
            "Tokenized: ['[CLS]', 'amazing', '!', '[SEP]']\n",
            "Original: [\"Don't\", 'let', 'the', 'nondescript', 'building', 'entrance', 'fool', 'you,', 'these', 'are', 'some', 'creative', 'and', 'talented', 'chefs...two', 'thumbs', 'way,', 'way', 'up!']\n",
            "Tokenized: ['[CLS]', 'don', \"'\", 't', 'let', 'the', 'non', '##des', '##cript', 'building', 'entrance', 'fool', 'you', ',', 'these', 'are', 'some', 'creative', 'and', 'talented', 'chefs', '.', '.', '.', 'two', 'thumbs', 'way', ',', 'way', 'up', '!', '[SEP]']\n",
            "Original: ['I', 'stopped', 'in', 'today', '@', 'Yards', 'Brewery.']\n",
            "Tokenized: ['[CLS]', 'i', 'stopped', 'in', 'today', '@', 'yards', 'brewery', '.', '[SEP]']\n",
            "Original: ['I', 'must', 'say,', 'I', 'was', 'impressed', 'with', 'the', 'size', 'of', 'the', 'bar', 'area', 'and', 'lounge,', '&', 'I', 'liked', 'that', 'you', 'could', 'see', 'the', 'brewery', 'right', 'thru', 'the', 'glass!']\n",
            "Tokenized: ['[CLS]', 'i', 'must', 'say', ',', 'i', 'was', 'impressed', 'with', 'the', 'size', 'of', 'the', 'bar', 'area', 'and', 'lounge', ',', '&', 'i', 'liked', 'that', 'you', 'could', 'see', 'the', 'brewery', 'right', 'thru', 'the', 'glass', '!', '[SEP]']\n",
            "Original: ['I', 'had', 'a', 'sampler', 'of', 'IPA,', 'Brawler,', 'Love', 'Stout', '&', 'ESA.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'a', 'sample', '##r', 'of', 'ipa', ',', 'brawl', '##er', ',', 'love', 'stout', '&', 'esa', '.', '[SEP]']\n",
            "Original: ['All', 'were', 'awesome,', '&', 'I', 'had', 'a', 'Dogwood', 'Grilled', 'Cheese', 'which', 'was', 'enjoyable', 'with', 'the', 'fine', 'beers.']\n",
            "Tokenized: ['[CLS]', 'all', 'were', 'awesome', ',', '&', 'i', 'had', 'a', 'dog', '##wood', 'grille', '##d', 'cheese', 'which', 'was', 'enjoyable', 'with', 'the', 'fine', 'beers', '.', '[SEP]']\n",
            "Original: ['After', 'my', 'sampler', '&', 'sandwich,', 'I', 'asked', 'for', 'a', 'pint', 'of', 'there', 'Nitrogen', 'Love', 'Stout,', 'I', 'must', 'say', 'I', 'was', 'impressed,', 'with', 'the', 'great', 'taste', '&', 'I', 'am', 'a', 'Guiness', 'Lover', 'so', 'coming', 'from', 'me,', 'I', 'think', 'this', 'is', 'better,', 'its', 'less', 'dry', '&', 'smoother!']\n",
            "Tokenized: ['[CLS]', 'after', 'my', 'sample', '##r', '&', 'sandwich', ',', 'i', 'asked', 'for', 'a', 'pin', '##t', 'of', 'there', 'nitrogen', 'love', 'stout', ',', 'i', 'must', 'say', 'i', 'was', 'impressed', ',', 'with', 'the', 'great', 'taste', '&', 'i', 'am', 'a', 'gui', '##ness', 'lover', 'so', 'coming', 'from', 'me', ',', 'i', 'think', 'this', 'is', 'better', ',', 'its', 'less', 'dry', '&', 'smooth', '##er', '!', '[SEP]']\n",
            "Original: ['If', 'you', 'are', 'in', 'Philly', 'you', 'have', 'to', 'come', 'check', 'this', 'place', 'out!']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'are', 'in', 'phil', '##ly', 'you', 'have', 'to', 'come', 'check', 'this', 'place', 'out', '!', '[SEP]']\n",
            "Original: ['The', 'only', 'negative', 'I', 'have', 'abou', 'this', 'place', 'is', 'the', 'parking!']\n",
            "Tokenized: ['[CLS]', 'the', 'only', 'negative', 'i', 'have', 'ab', '##ou', 'this', 'place', 'is', 'the', 'parking', '!', '[SEP]']\n",
            "Original: ['I', 'left', 'with', 'a', 'case', 'of', 'BRAWLER!!!!!']\n",
            "Tokenized: ['[CLS]', 'i', 'left', 'with', 'a', 'case', 'of', 'brawl', '##er', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['best', 'quote', 'ever', 'My', 'gate', 'was', 'stuck', 'halfway', 'open', 'so', 'I', 'calledA', 'CLASS', 'Garage', 'Doors', 'Dr', 'Services.']\n",
            "Tokenized: ['[CLS]', 'best', 'quote', 'ever', 'my', 'gate', 'was', 'stuck', 'halfway', 'open', 'so', 'i', 'called', '##a', 'class', 'garage', 'doors', 'dr', 'services', '.', '[SEP]']\n",
            "Original: ['They', 'came', 'to', 'my', 'house', 'in', 'no', 'time', 'and', 'started', 'working', 'on', 'the', 'gate.']\n",
            "Tokenized: ['[CLS]', 'they', 'came', 'to', 'my', 'house', 'in', 'no', 'time', 'and', 'started', 'working', 'on', 'the', 'gate', '.', '[SEP]']\n",
            "Original: ['They', 'were', 'very', 'friendly', 'and', 'were', 'able', 'to', 'explain', 'me', 'exactly', 'what', 'was', 'wrong', 'with', 'it.']\n",
            "Tokenized: ['[CLS]', 'they', 'were', 'very', 'friendly', 'and', 'were', 'able', 'to', 'explain', 'me', 'exactly', 'what', 'was', 'wrong', 'with', 'it', '.', '[SEP]']\n",
            "Original: ['Once', 'they', 'fixed', 'it', 'they', 'answetred', 'all', 'of', 'my', 'questions', 'with', 'no', 'hesitations', 'and', 'then', 'gave', 'me', 'the', 'best', 'quote', 'ever.']\n",
            "Tokenized: ['[CLS]', 'once', 'they', 'fixed', 'it', 'they', 'an', '##sw', '##et', '##red', 'all', 'of', 'my', 'questions', 'with', 'no', 'hesitation', '##s', 'and', 'then', 'gave', 'me', 'the', 'best', 'quote', 'ever', '.', '[SEP]']\n",
            "Original: ['I', 'know', 'that', 'if', 'my', 'garage', 'door', 'needs', 'to', 'be', 'repaired,', 'I', 'will', 'be', 'calling', 'A', 'CLASS', 'Garage', 'Doors']\n",
            "Tokenized: ['[CLS]', 'i', 'know', 'that', 'if', 'my', 'garage', 'door', 'needs', 'to', 'be', 'repaired', ',', 'i', 'will', 'be', 'calling', 'a', 'class', 'garage', 'doors', '[SEP]']\n",
            "Original: ['HORRIBLE!!!!!']\n",
            "Tokenized: ['[CLS]', 'horrible', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['This', 'has', 'to', 'be', 'some', 'of', 'the', 'worst', 'pizza', 'I', 'have', 'ever', 'had', 'the', 'misfortune', 'of', 'ordering.']\n",
            "Tokenized: ['[CLS]', 'this', 'has', 'to', 'be', 'some', 'of', 'the', 'worst', 'pizza', 'i', 'have', 'ever', 'had', 'the', 'mis', '##fort', '##une', 'of', 'ordering', '.', '[SEP]']\n",
            "Original: ['The', 'crust', 'was', 'lopsided,', 'thicker', 'on', 'one', 'side', 'than', 'the', 'other.']\n",
            "Tokenized: ['[CLS]', 'the', 'crust', 'was', 'lo', '##ps', '##ided', ',', 'thicker', 'on', 'one', 'side', 'than', 'the', 'other', '.', '[SEP]']\n",
            "Original: ['It', 'actually', 'had', 'a', 'hole', 'in', 'one', 'of', 'the', 'slices.']\n",
            "Tokenized: ['[CLS]', 'it', 'actually', 'had', 'a', 'hole', 'in', 'one', 'of', 'the', 'slices', '.', '[SEP]']\n",
            "Original: ['There', 'was', 'minimal', 'cheese', 'and', 'sauce', 'and', 'it', 'completely', 'lacked', 'flavor.']\n",
            "Tokenized: ['[CLS]', 'there', 'was', 'minimal', 'cheese', 'and', 'sauce', 'and', 'it', 'completely', 'lacked', 'flavor', '.', '[SEP]']\n",
            "Original: ['I', 'know', 'New', 'York', 'pizza', 'and', 'this', 'is', 'not', 'it!!']\n",
            "Tokenized: ['[CLS]', 'i', 'know', 'new', 'york', 'pizza', 'and', 'this', 'is', 'not', 'it', '!', '!', '[SEP]']\n",
            "Original: ['This', 'was', 'nothing', 'like', 'New', 'York', 'style', 'pizza!!!.']\n",
            "Tokenized: ['[CLS]', 'this', 'was', 'nothing', 'like', 'new', 'york', 'style', 'pizza', '!', '!', '!', '.', '[SEP]']\n",
            "Original: ['I', 'love', 'pizza', 'and', 'this', 'was', 'a', 'complete', 'and', 'utter', 'disappointment!!']\n",
            "Tokenized: ['[CLS]', 'i', 'love', 'pizza', 'and', 'this', 'was', 'a', 'complete', 'and', 'utter', 'disappointment', '!', '!', '[SEP]']\n",
            "Original: ['I', 'would', 'not', 'suggest', 'this', 'pizza', 'to', 'anyone!!!']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'not', 'suggest', 'this', 'pizza', 'to', 'anyone', '!', '!', '!', '[SEP]']\n",
            "Original: ['One', 'suspects', 'that', 'earlier', 'reviewer', 'works', 'for', 'another', 'laundry.']\n",
            "Tokenized: ['[CLS]', 'one', 'suspects', 'that', 'earlier', 'reviewer', 'works', 'for', 'another', 'laundry', '.', '[SEP]']\n",
            "Original: ['Outside', 'of', 'parking', 'being', 'at', 'a', 'premium,', 'especially', 'on', 'discount', 'days,', 'The', 'Laundry', 'Tub', 'is', 'not', 'filthy', 'and', \"it's\", 'no', 'smaller', 'than', 'any', 'of', 'a', 'dozen', 'laundromats', \"I've\", 'been', 'in.']\n",
            "Tokenized: ['[CLS]', 'outside', 'of', 'parking', 'being', 'at', 'a', 'premium', ',', 'especially', 'on', 'discount', 'days', ',', 'the', 'laundry', 'tub', 'is', 'not', 'filthy', 'and', 'it', \"'\", 's', 'no', 'smaller', 'than', 'any', 'of', 'a', 'dozen', 'lau', '##nd', '##rom', '##ats', 'i', \"'\", 've', 'been', 'in', '.', '[SEP]']\n",
            "Original: ['Yes,', 'there', 'are', 'bigger,', 'but', 'bigger', \"isn't\", 'necessarily', 'better.']\n",
            "Tokenized: ['[CLS]', 'yes', ',', 'there', 'are', 'bigger', ',', 'but', 'bigger', 'isn', \"'\", 't', 'necessarily', 'better', '.', '[SEP]']\n",
            "Original: ['I', 'availed', 'myself', 'of', 'the', 'wash', \"'n\", 'fold', 'service,', 'taking', 'just', 'about', 'a', \"year's\", 'worth', 'of', 'dirty', 'clothes', 'in', 'and', 'getting', 'back', 'neatly', 'folded,', 'clean', 'clothes', 'in', 'clear', 'plastic', 'bags', \"(I'd\", 'originally', 'brought', 'them', 'in,', 'in', 'six', 'large', 'yellow', 'garbage', 'bags).']\n",
            "Tokenized: ['[CLS]', 'i', 'avail', '##ed', 'myself', 'of', 'the', 'wash', \"'\", 'n', 'fold', 'service', ',', 'taking', 'just', 'about', 'a', 'year', \"'\", 's', 'worth', 'of', 'dirty', 'clothes', 'in', 'and', 'getting', 'back', 'neatly', 'folded', ',', 'clean', 'clothes', 'in', 'clear', 'plastic', 'bags', '(', 'i', \"'\", 'd', 'originally', 'brought', 'them', 'in', ',', 'in', 'six', 'large', 'yellow', 'garbage', 'bags', ')', '.', '[SEP]']\n",
            "Original: ['The', 'cost', 'was', 'certainly', 'reasonable', 'and', 'I', 'will', 'continue', 'my', 'patronage', 'of', 'The', 'Laundry', 'Tub', 'in', 'the', 'future.']\n",
            "Tokenized: ['[CLS]', 'the', 'cost', 'was', 'certainly', 'reasonable', 'and', 'i', 'will', 'continue', 'my', 'patronage', 'of', 'the', 'laundry', 'tub', 'in', 'the', 'future', '.', '[SEP]']\n",
            "Original: [\"Didn't\", 'hurt', 'any', 'that', 'they', 'knew', 'my', 'name', 'by', 'my', 'second', 'visit', 'and', 'greeted', 'me', 'warmly', 'then', 'and', 'on', 'my', 'third', 'visit.']\n",
            "Tokenized: ['[CLS]', 'didn', \"'\", 't', 'hurt', 'any', 'that', 'they', 'knew', 'my', 'name', 'by', 'my', 'second', 'visit', 'and', 'greeted', 'me', 'warmly', 'then', 'and', 'on', 'my', 'third', 'visit', '.', '[SEP]']\n",
            "Original: ['Whereas', 'my', 'answer', 'to', 'the', 'question', '\"Where', 'do', 'you', 'get', 'your', 'laundry', 'done?\"', 'used', 'to', 'be,', '\"At', 'the', 'checkout', 'line', 'at', 'WalMart,\"', 'I', 'can', 'honestly', 'say', 'the', 'answer', 'now', 'is,', '\"At', 'The', 'Laundry', 'Tub.\"']\n",
            "Tokenized: ['[CLS]', 'whereas', 'my', 'answer', 'to', 'the', 'question', '\"', 'where', 'do', 'you', 'get', 'your', 'laundry', 'done', '?', '\"', 'used', 'to', 'be', ',', '\"', 'at', 'the', 'check', '##out', 'line', 'at', 'wal', '##mart', ',', '\"', 'i', 'can', 'honestly', 'say', 'the', 'answer', 'now', 'is', ',', '\"', 'at', 'the', 'laundry', 'tub', '.', '\"', '[SEP]']\n",
            "Original: ['Great', 'food', 'and', 'nice', 'people', 'very', 'pleasant', 'experience.']\n",
            "Tokenized: ['[CLS]', 'great', 'food', 'and', 'nice', 'people', 'very', 'pleasant', 'experience', '.', '[SEP]']\n",
            "Original: ['Excellent', 'Tattoo', 'Shop']\n",
            "Tokenized: ['[CLS]', 'excellent', 'tattoo', 'shop', '[SEP]']\n",
            "Original: ['I', 'recently', 'got', 'a', 'tattoo', 'done', 'at', 'Aztec', 'and', 'I', 'could', 'not', 'be', 'happier.']\n",
            "Tokenized: ['[CLS]', 'i', 'recently', 'got', 'a', 'tattoo', 'done', 'at', 'aztec', 'and', 'i', 'could', 'not', 'be', 'happier', '.', '[SEP]']\n",
            "Original: ['It', 'came', 'out', 'better', 'than', 'I', 'even', 'imagined.']\n",
            "Tokenized: ['[CLS]', 'it', 'came', 'out', 'better', 'than', 'i', 'even', 'imagined', '.', '[SEP]']\n",
            "Original: ['The', 'shop', 'was', 'great,', 'the', 'service', 'was', 'excellent', 'and', 'the', 'employees', 'were', 'fun', 'guys.']\n",
            "Tokenized: ['[CLS]', 'the', 'shop', 'was', 'great', ',', 'the', 'service', 'was', 'excellent', 'and', 'the', 'employees', 'were', 'fun', 'guys', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'highly', 'recommend', 'this', 'shop', 'to', 'anyone', 'looking', 'to', 'get', 'a', 'quality', 'tattoo', 'done.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'highly', 'recommend', 'this', 'shop', 'to', 'anyone', 'looking', 'to', 'get', 'a', 'quality', 'tattoo', 'done', '.', '[SEP]']\n",
            "Original: ['Pam', 'the', 'Pom']\n",
            "Tokenized: ['[CLS]', 'pam', 'the', 'po', '##m', '[SEP]']\n",
            "Original: ['Fantastic', 'couple', 'of', 'days.']\n",
            "Tokenized: ['[CLS]', 'fantastic', 'couple', 'of', 'days', '.', '[SEP]']\n",
            "Original: ['Breathtaking', 'views', 'and', 'fabulous', 'accommodation.']\n",
            "Tokenized: ['[CLS]', 'breath', '##taking', 'views', 'and', 'fabulous', 'accommodation', '.', '[SEP]']\n",
            "Original: ['Nothing', 'too', 'much', 'trouble', 'for', 'Ian,', 'thanks', 'for', 'a', 'great', 'stay.']\n",
            "Tokenized: ['[CLS]', 'nothing', 'too', 'much', 'trouble', 'for', 'ian', ',', 'thanks', 'for', 'a', 'great', 'stay', '.', '[SEP]']\n",
            "Original: ['The', 'food', 'is', 'excellent,', 'but', 'very', 'overpriced.']\n",
            "Tokenized: ['[CLS]', 'the', 'food', 'is', 'excellent', ',', 'but', 'very', 'over', '##pr', '##ice', '##d', '.', '[SEP]']\n",
            "Original: ['How', 'do', 'you', 'run', 'a', 'cafe,', 'with', 'no', 'refills', 'on', 'coffee-?']\n",
            "Tokenized: ['[CLS]', 'how', 'do', 'you', 'run', 'a', 'cafe', ',', 'with', 'no', 'ref', '##ill', '##s', 'on', 'coffee', '-', '?', '[SEP]']\n",
            "Original: ['Favorite', 'Restaurant']\n",
            "Tokenized: ['[CLS]', 'favorite', 'restaurant', '[SEP]']\n",
            "Original: ['Best', 'yellow', 'curry', 'that', 'I', 'have', 'ever', 'tasted.']\n",
            "Tokenized: ['[CLS]', 'best', 'yellow', 'curry', 'that', 'i', 'have', 'ever', 'tasted', '.', '[SEP]']\n",
            "Original: ['Staff', 'is', 'super', 'friendly', 'and', 'very', 'attentive.']\n",
            "Tokenized: ['[CLS]', 'staff', 'is', 'super', 'friendly', 'and', 'very', 'at', '##ten', '##tive', '.', '[SEP]']\n",
            "Original: ['Price', 'is', 'also', 'very', 'reasonable.']\n",
            "Tokenized: ['[CLS]', 'price', 'is', 'also', 'very', 'reasonable', '.', '[SEP]']\n",
            "Original: ['Lots', 'of', 'rules,', 'phantom', 'innkeeper,', 'last', 'minute', 'price', 'was', 'worth', 'it.']\n",
            "Tokenized: ['[CLS]', 'lots', 'of', 'rules', ',', 'phantom', 'inn', '##keeper', ',', 'last', 'minute', 'price', 'was', 'worth', 'it', '.', '[SEP]']\n",
            "Original: ['I', 'called', 'the', '\"207\"', 'number', 'and', 'listened', 'to', 'the', 'same', 'recording', 'loop', '3', 'times', 'before', 'I', 'gave', 'up.']\n",
            "Tokenized: ['[CLS]', 'i', 'called', 'the', '\"', '207', '\"', 'number', 'and', 'listened', 'to', 'the', 'same', 'recording', 'loop', '3', 'times', 'before', 'i', 'gave', 'up', '.', '[SEP]']\n",
            "Original: ['I', 'then', 'called', 'the', '800', 'number', '(which', 'was', 'answered)', 'and', 'inquired', 'about', 'last', 'minute', 'rates.']\n",
            "Tokenized: ['[CLS]', 'i', 'then', 'called', 'the', '800', 'number', '(', 'which', 'was', 'answered', ')', 'and', 'inquired', 'about', 'last', 'minute', 'rates', '.', '[SEP]']\n",
            "Original: ['They', 'had', 'a', 'room', 'with', 'a', '$99', 'rate,', 'which', 'I', 'booked.']\n",
            "Tokenized: ['[CLS]', 'they', 'had', 'a', 'room', 'with', 'a', '$', '99', 'rate', ',', 'which', 'i', 'booked', '.', '[SEP]']\n",
            "Original: ['The', 'room', 'was', 'supposed', 'to', 'be', 'on', 'the', '2nd', 'floor,', 'but', 'they', 'put', 'us', 'on', 'the', '3rd.']\n",
            "Tokenized: ['[CLS]', 'the', 'room', 'was', 'supposed', 'to', 'be', 'on', 'the', '2nd', 'floor', ',', 'but', 'they', 'put', 'us', 'on', 'the', '3rd', '.', '[SEP]']\n",
            "Original: ['The', 'email', 'confirmation', '(which', 'I', 'read', 'in', 'the', 'car)', 'warned', 'about', 'large', 'suitcases,', 'declaring', 'that', 'we', 'are', 'innkeepers,', 'not', 'longshoreman.']\n",
            "Tokenized: ['[CLS]', 'the', 'email', 'confirmation', '(', 'which', 'i', 'read', 'in', 'the', 'car', ')', 'warned', 'about', 'large', 'suitcase', '##s', ',', 'declaring', 'that', 'we', 'are', 'inn', '##keepers', ',', 'not', 'long', '##shore', '##man', '.', '[SEP]']\n",
            "Original: ['In', 'other', 'words,', 'they', 'do', 'not', 'help', 'with', 'suitcases,', 'but', 'they', 'promise', 'totes', 'to', 'help.']\n",
            "Tokenized: ['[CLS]', 'in', 'other', 'words', ',', 'they', 'do', 'not', 'help', 'with', 'suitcase', '##s', ',', 'but', 'they', 'promise', 'to', '##tes', 'to', 'help', '.', '[SEP]']\n",
            "Original: ['However', 'upon', 'our', 'arrival', 'no', 'one', 'there', '(the', 'inn', 'was', 'open).']\n",
            "Tokenized: ['[CLS]', 'however', 'upon', 'our', 'arrival', 'no', 'one', 'there', '(', 'the', 'inn', 'was', 'open', ')', '.', '[SEP]']\n",
            "Original: ['So', 'no', 'totes.']\n",
            "Tokenized: ['[CLS]', 'so', 'no', 'to', '##tes', '.', '[SEP]']\n",
            "Original: ['Finally', 'a', 'chambermaid', 'stuck', 'her', 'head', 'around', 'the', 'corner', 'from', 'the', 'top', 'of', 'the', 'stairs', 'and', 'told', 'us', 'sternly', 'that', 'we', 'could', 'not', 'be', 'accommodated', 'until', '3M,', 'no', 'exceptions.']\n",
            "Tokenized: ['[CLS]', 'finally', 'a', 'chamber', '##maid', 'stuck', 'her', 'head', 'around', 'the', 'corner', 'from', 'the', 'top', 'of', 'the', 'stairs', 'and', 'told', 'us', 'stern', '##ly', 'that', 'we', 'could', 'not', 'be', 'accommodated', 'until', '3', '##m', ',', 'no', 'exceptions', '.', '[SEP]']\n",
            "Original: ['Then', 'she', 'was', 'gone.']\n",
            "Tokenized: ['[CLS]', 'then', 'she', 'was', 'gone', '.', '[SEP]']\n",
            "Original: ['We', 'returned', 'after', '3PM,', 'found', 'no', 'one', 'there,', 'and', 'a', 'note', 'from', 'the', 'innkeeper', 'with', 'directions', 'to', 'our', 'room.']\n",
            "Tokenized: ['[CLS]', 'we', 'returned', 'after', '3', '##pm', ',', 'found', 'no', 'one', 'there', ',', 'and', 'a', 'note', 'from', 'the', 'inn', '##keeper', 'with', 'directions', 'to', 'our', 'room', '.', '[SEP]']\n",
            "Original: ['Rules', 'in', 'the', 'room:', '#1)', 'if', 'you', 'drink', 'the', 'soda', 'from', 'the', 'fridge', 'in', 'your', 'room', 'you', 'must', 'prove', 'it', 'by', 'leaving', 'the', 'can', 'in', 'the', 'trash.']\n",
            "Tokenized: ['[CLS]', 'rules', 'in', 'the', 'room', ':', '#', '1', ')', 'if', 'you', 'drink', 'the', 'soda', 'from', 'the', 'fridge', 'in', 'your', 'room', 'you', 'must', 'prove', 'it', 'by', 'leaving', 'the', 'can', 'in', 'the', 'trash', '.', '[SEP]']\n",
            "Original: ['If', 'they', 'think', \"you've\", 'taken', 'a', 'soda', 'from', 'your', 'room', 'home', 'with', 'you,', 'they', 'will', 'charge', 'you', '$1.50', 'per', 'can.']\n",
            "Tokenized: ['[CLS]', 'if', 'they', 'think', 'you', \"'\", 've', 'taken', 'a', 'soda', 'from', 'your', 'room', 'home', 'with', 'you', ',', 'they', 'will', 'charge', 'you', '$', '1', '.', '50', 'per', 'can', '.', '[SEP]']\n",
            "Original: ['They', 'count', 'the', 'cans', 'in', 'the', 'trash', 'to', 'make', 'sure.']\n",
            "Tokenized: ['[CLS]', 'they', 'count', 'the', 'cans', 'in', 'the', 'trash', 'to', 'make', 'sure', '.', '[SEP]']\n",
            "Original: ['#2)', 'If', 'you', 'take', 'the', 'shampoo', 'products', 'home,', 'they', 'will', 'charge', 'you', '$8', 'per', 'item.']\n",
            "Tokenized: ['[CLS]', '#', '2', ')', 'if', 'you', 'take', 'the', 'sham', '##poo', 'products', 'home', ',', 'they', 'will', 'charge', 'you', '$', '8', 'per', 'item', '.', '[SEP]']\n",
            "Original: ['#)', 'If', 'you', 'want', 'a', 'late', 'checkout,', '(after', '11', 'AM)', 'they', 'charge', 'you', '$15', 'for', 'the', 'first', 'hour,', '$25', 'for', 'the', 'second', 'hour,', 'and', 'after', '2PM', \"it's\", 'a', 'full', 'day', 'charge.']\n",
            "Tokenized: ['[CLS]', '#', ')', 'if', 'you', 'want', 'a', 'late', 'check', '##out', ',', '(', 'after', '11', 'am', ')', 'they', 'charge', 'you', '$', '15', 'for', 'the', 'first', 'hour', ',', '$', '25', 'for', 'the', 'second', 'hour', ',', 'and', 'after', '2', '##pm', 'it', \"'\", 's', 'a', 'full', 'day', 'charge', '.', '[SEP]']\n",
            "Original: ['#4)', 'Breakfast', 'is', '8AM', 'to', '10AM.']\n",
            "Tokenized: ['[CLS]', '#', '4', ')', 'breakfast', 'is', '8', '##am', 'to', '10', '##am', '.', '[SEP]']\n",
            "Original: ['No', 'earlier', 'and', 'no', 'later.']\n",
            "Tokenized: ['[CLS]', 'no', 'earlier', 'and', 'no', 'later', '.', '[SEP]']\n",
            "Original: ['If', 'you', 'go', 'later,', \"it's\", 'all', 'cleaned', 'up.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'go', 'later', ',', 'it', \"'\", 's', 'all', 'cleaned', 'up', '.', '[SEP]']\n",
            "Original: ['(By', 'whom,', 'I', \"don't\", 'know.']\n",
            "Tokenized: ['[CLS]', '(', 'by', 'whom', ',', 'i', 'don', \"'\", 't', 'know', '.', '[SEP]']\n",
            "Original: ['I', 'never', 'saw', 'anyone', 'there.']\n",
            "Tokenized: ['[CLS]', 'i', 'never', 'saw', 'anyone', 'there', '.', '[SEP]']\n",
            "Original: ['All', 'these', 'rules', 'are', 'posted', 'in', 'the', 'rooms.)']\n",
            "Tokenized: ['[CLS]', 'all', 'these', 'rules', 'are', 'posted', 'in', 'the', 'rooms', '.', ')', '[SEP]']\n",
            "Original: ['Snacks:', 'uninspired', 'bread,', 'tea', 'backs,', 'and', 'individual', 'coffee', 'things', 'for', 'a', 'machine', 'that', \"didn't\", 'exist.']\n",
            "Tokenized: ['[CLS]', 'snacks', ':', 'un', '##ins', '##pired', 'bread', ',', 'tea', 'backs', ',', 'and', 'individual', 'coffee', 'things', 'for', 'a', 'machine', 'that', 'didn', \"'\", 't', 'exist', '.', '[SEP]']\n",
            "Original: ['I', 'put', 'the', 'coffee', 'thing', 'in', 'hot', 'water', 'and', 'settled', 'for', 'a', 'cup', 'of', 'weak', 'coffee.']\n",
            "Tokenized: ['[CLS]', 'i', 'put', 'the', 'coffee', 'thing', 'in', 'hot', 'water', 'and', 'settled', 'for', 'a', 'cup', 'of', 'weak', 'coffee', '.', '[SEP]']\n",
            "Original: ['No', 'wine', 'glasses.']\n",
            "Tokenized: ['[CLS]', 'no', 'wine', 'glasses', '.', '[SEP]']\n",
            "Original: ['Room', 'was', 'clean,', 'but', 'had', 'a', 'weird,', 'dated,', 'sink/stove', 'combo', 'that', \"didn't\", 'work.']\n",
            "Tokenized: ['[CLS]', 'room', 'was', 'clean', ',', 'but', 'had', 'a', 'weird', ',', 'dated', ',', 'sink', '/', 'stove', 'combo', 'that', 'didn', \"'\", 't', 'work', '.', '[SEP]']\n",
            "Original: ['Bath', 'was', 'clean', 'except', 'shower', 'stall', 'which', 'had', 'mildew', 'problems.']\n",
            "Tokenized: ['[CLS]', 'bath', 'was', 'clean', 'except', 'shower', 'stall', 'which', 'had', 'mild', '##ew', 'problems', '.', '[SEP]']\n",
            "Original: ['No', 'tub.']\n",
            "Tokenized: ['[CLS]', 'no', 'tub', '.', '[SEP]']\n",
            "Original: ['The', 'Inn', 'touts', 'a', 'shower', 'with', 'dual', 'shower', 'heads,', 'but', 'only', 'one', 'worked.']\n",
            "Tokenized: ['[CLS]', 'the', 'inn', 'to', '##uts', 'a', 'shower', 'with', 'dual', 'shower', 'heads', ',', 'but', 'only', 'one', 'worked', '.', '[SEP]']\n",
            "Original: [\"Don't\", 'get', 'the', 'rooms', 'off', 'the', 'two', 'kitchens.']\n",
            "Tokenized: ['[CLS]', 'don', \"'\", 't', 'get', 'the', 'rooms', 'off', 'the', 'two', 'kitchens', '.', '[SEP]']\n",
            "Original: ['They', 'are', 'RIGHT', 'OFF', 'the', 'kitchen', 'so', 'you', 'hear', 'everything.']\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'right', 'off', 'the', 'kitchen', 'so', 'you', 'hear', 'everything', '.', '[SEP]']\n",
            "Original: ['Free', 'parking.']\n",
            "Tokenized: ['[CLS]', 'free', 'parking', '.', '[SEP]']\n",
            "Original: [\"I'd\", 'go', 'back', 'if', 'I', 'could', 'get', 'the', 'last', 'minute', 'rate', 'again', 'of', '$99,', 'but', 'I', \"wouldn't\", 'pay', 'their', 'rack', 'rate.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'd', 'go', 'back', 'if', 'i', 'could', 'get', 'the', 'last', 'minute', 'rate', 'again', 'of', '$', '99', ',', 'but', 'i', 'wouldn', \"'\", 't', 'pay', 'their', 'rack', 'rate', '.', '[SEP]']\n",
            "Original: ['Worst', 'Apartments', 'EVER']\n",
            "Tokenized: ['[CLS]', 'worst', 'apartments', 'ever', '[SEP]']\n",
            "Original: ['We', 'lived', 'here', 'for', '2', 'years,', 'the', 'first', 'year', 'or', 'so', 'was', 'okay.']\n",
            "Tokenized: ['[CLS]', 'we', 'lived', 'here', 'for', '2', 'years', ',', 'the', 'first', 'year', 'or', 'so', 'was', 'okay', '.', '[SEP]']\n",
            "Original: ['Then,', 'the', 'more', 'power', 'they', 'gave', 'to', 'Linda,', 'the', 'worse', 'the', 'place', 'got.']\n",
            "Tokenized: ['[CLS]', 'then', ',', 'the', 'more', 'power', 'they', 'gave', 'to', 'linda', ',', 'the', 'worse', 'the', 'place', 'got', '.', '[SEP]']\n",
            "Original: ['We', 'were', 'always', 'having', 'our', 'water', 'shut', 'off,', 'there', 'were', 'always', 'people', 'having', 'parties', 'at', 'the', 'pool,', 'even', 'after', 'it', 'was', 'supposed', 'to', 'be', 'closed.']\n",
            "Tokenized: ['[CLS]', 'we', 'were', 'always', 'having', 'our', 'water', 'shut', 'off', ',', 'there', 'were', 'always', 'people', 'having', 'parties', 'at', 'the', 'pool', ',', 'even', 'after', 'it', 'was', 'supposed', 'to', 'be', 'closed', '.', '[SEP]']\n",
            "Original: ['The', 'pool', 'was', 'supposed', 'to', 'close', 'at', '10', 'and', 'they', 'would', 'have', 'people', 'down', 'there', 'until', '11:45', 'yelling,', 'playing', 'music', 'and', 'do', 'who-knows-what', 'in', 'the', 'dark', 'corners', 'of', 'the', 'pool.']\n",
            "Tokenized: ['[CLS]', 'the', 'pool', 'was', 'supposed', 'to', 'close', 'at', '10', 'and', 'they', 'would', 'have', 'people', 'down', 'there', 'until', '11', ':', '45', 'yelling', ',', 'playing', 'music', 'and', 'do', 'who', '-', 'knows', '-', 'what', 'in', 'the', 'dark', 'corners', 'of', 'the', 'pool', '.', '[SEP]']\n",
            "Original: ['Then,', 'when', 'we', 'moved', 'out,', 'we', 'cleaned', 'the', 'apartment', 'top', 'to', 'bottom,', 'they', 'came', 'back', 'and', 'tried', 'to', 'charge', 'us', 'for', 'two', 'cleaning', 'fees', '(and', 'we', 'never', 'got', 'our', 'deposit', 'back)', 'and', 'past', 'utilities', '(that', 'were', 'already', 'paid,', 'we', 'have', 'check', 'numbers', 'and', 'records', 'of', 'this).']\n",
            "Tokenized: ['[CLS]', 'then', ',', 'when', 'we', 'moved', 'out', ',', 'we', 'cleaned', 'the', 'apartment', 'top', 'to', 'bottom', ',', 'they', 'came', 'back', 'and', 'tried', 'to', 'charge', 'us', 'for', 'two', 'cleaning', 'fees', '(', 'and', 'we', 'never', 'got', 'our', 'deposit', 'back', ')', 'and', 'past', 'utilities', '(', 'that', 'were', 'already', 'paid', ',', 'we', 'have', 'check', 'numbers', 'and', 'records', 'of', 'this', ')', '.', '[SEP]']\n",
            "Original: ['Linda', 'is', 'the', 'rudest', 'person', 'you', 'will', 'ever', 'talk', 'to', 'and', 'she', 'sticks', 'up', 'for', 'all', 'of', 'the', 'trashy,', 'rude', 'people', 'that', 'live', 'there,', 'not', 'the', 'nice', 'ones', 'that', 'actually', 'give', 'a', 'crap', 'about', 'respecting', 'others.']\n",
            "Tokenized: ['[CLS]', 'linda', 'is', 'the', 'rude', '##st', 'person', 'you', 'will', 'ever', 'talk', 'to', 'and', 'she', 'sticks', 'up', 'for', 'all', 'of', 'the', 'trash', '##y', ',', 'rude', 'people', 'that', 'live', 'there', ',', 'not', 'the', 'nice', 'ones', 'that', 'actually', 'give', 'a', 'crap', 'about', 'respecting', 'others', '.', '[SEP]']\n",
            "Original: ['Do', 'not', 'live', 'here,', 'you', 'will', 'regret', 'it!']\n",
            "Tokenized: ['[CLS]', 'do', 'not', 'live', 'here', ',', 'you', 'will', 'regret', 'it', '!', '[SEP]']\n",
            "Original: ['A', 'very', 'satisfied', 'new', 'customer!']\n",
            "Tokenized: ['[CLS]', 'a', 'very', 'satisfied', 'new', 'customer', '!', '[SEP]']\n",
            "Original: ['As', 'a', 'very', 'satisfied', 'new', 'customer,', 'I', 'wholeheartedly', 'recommend', 'United', 'Air', 'Duct', 'Cleaning.']\n",
            "Tokenized: ['[CLS]', 'as', 'a', 'very', 'satisfied', 'new', 'customer', ',', 'i', 'whole', '##hearted', '##ly', 'recommend', 'united', 'air', 'duct', 'cleaning', '.', '[SEP]']\n",
            "Original: ['They', 'are', 'professional,', 'knowledgeable,', 'and', 'take', 'meticulous', 'care', 'and', 'pride', 'in', 'accomplishing', 'their', 'work.']\n",
            "Tokenized: ['[CLS]', 'they', 'are', 'professional', ',', 'knowledge', '##able', ',', 'and', 'take', 'met', '##ic', '##ulous', 'care', 'and', 'pride', 'in', 'accomplish', '##ing', 'their', 'work', '.', '[SEP]']\n",
            "Original: ['Not', 'only', 'were', 'my', 'wife', 'and', 'I', 'very', 'pleased,', 'but', 'I', 'also', 'had', 'the', 'air', 'duct', 'quality', 'tested', 'professionally', 'by', 'the', 'home', 'inspector', 'that', 'I', 'regularly', 'use,', 'before', 'and', 'after', 'United', 'Air', 'Duct', 'performed', 'their', 'work.']\n",
            "Tokenized: ['[CLS]', 'not', 'only', 'were', 'my', 'wife', 'and', 'i', 'very', 'pleased', ',', 'but', 'i', 'also', 'had', 'the', 'air', 'duct', 'quality', 'tested', 'professionally', 'by', 'the', 'home', 'inspector', 'that', 'i', 'regularly', 'use', ',', 'before', 'and', 'after', 'united', 'air', 'duct', 'performed', 'their', 'work', '.', '[SEP]']\n",
            "Original: ['Based', 'on', 'the', 'test', 'results,', 'the', 'home', 'inspector', 'stated', 'that', 'the', 'quality', 'of', 'their', 'job', 'was', '“excellent”.']\n",
            "Tokenized: ['[CLS]', 'based', 'on', 'the', 'test', 'results', ',', 'the', 'home', 'inspector', 'stated', 'that', 'the', 'quality', 'of', 'their', 'job', 'was', '“', 'excellent', '”', '.', '[SEP]']\n",
            "Original: ['Dr.', 'Shady']\n",
            "Tokenized: ['[CLS]', 'dr', '.', 'shady', '[SEP]']\n",
            "Original: ['Kelly', 'hit', 'the', 'nail', 'on', 'the', 'head.']\n",
            "Tokenized: ['[CLS]', 'kelly', 'hit', 'the', 'nail', 'on', 'the', 'head', '.', '[SEP]']\n",
            "Original: ['\"Dr.', 'Shady\"', 'is', 'a', 'jerk.']\n",
            "Tokenized: ['[CLS]', '\"', 'dr', '.', 'shady', '\"', 'is', 'a', 'jerk', '.', '[SEP]']\n",
            "Original: ['After', 'the', 'way', 'she', 'spoke', 'to', 'me', 'on', 'my', 'last', 'visit,', 'I', 'will', 'not', 'be', 'returning!!']\n",
            "Tokenized: ['[CLS]', 'after', 'the', 'way', 'she', 'spoke', 'to', 'me', 'on', 'my', 'last', 'visit', ',', 'i', 'will', 'not', 'be', 'returning', '!', '!', '[SEP]']\n",
            "Original: ['Good', 'luck', 'keeping', 'business', 'with', 'that', 'stuck', 'up', 'attitude', 'Dr.', 'Shady.']\n",
            "Tokenized: ['[CLS]', 'good', 'luck', 'keeping', 'business', 'with', 'that', 'stuck', 'up', 'attitude', 'dr', '.', 'shady', '.', '[SEP]']\n",
            "Original: ['You', 'have', 'just', 'lost', 'mine.']\n",
            "Tokenized: ['[CLS]', 'you', 'have', 'just', 'lost', 'mine', '.', '[SEP]']\n",
            "Original: ['The', 'next', 'time', 'you', 'feel', 'like', 'being', 'condescending', 'to', 'someone,', 'it', 'is', 'not', 'going', 'to', 'be', 'me!!!!!!']\n",
            "Tokenized: ['[CLS]', 'the', 'next', 'time', 'you', 'feel', 'like', 'being', 'conde', '##sc', '##ending', 'to', 'someone', ',', 'it', 'is', 'not', 'going', 'to', 'be', 'me', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['Dr.', 'Shady', 'is', 'inexperienced', 'and', 'prideful.']\n",
            "Tokenized: ['[CLS]', 'dr', '.', 'shady', 'is', 'inexperienced', 'and', 'pride', '##ful', '.', '[SEP]']\n",
            "Original: ['She', 'probably', 'does', 'not', 'even', 'have', '10%', 'of', 'the', 'knowledge', 'that', 'some', 'of', 'the', 'other', 'EXPERIENCED', 'vets', 'do', 'in', 'this', 'area.']\n",
            "Tokenized: ['[CLS]', 'she', 'probably', 'does', 'not', 'even', 'have', '10', '%', 'of', 'the', 'knowledge', 'that', 'some', 'of', 'the', 'other', 'experienced', 'vet', '##s', 'do', 'in', 'this', 'area', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'be', 'carefully', 'researching', 'vets', 'before', 'I', 'take', 'my', 'dog', 'someplace', 'else.']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'be', 'carefully', 'researching', 'vet', '##s', 'before', 'i', 'take', 'my', 'dog', 'someplace', 'else', '.', '[SEP]']\n",
            "Original: ['Beautifully', 'written', 'reviews', 'Doctor,', 'but', 'completely', 'UNTRUE.']\n",
            "Tokenized: ['[CLS]', 'beautifully', 'written', 'reviews', 'doctor', ',', 'but', 'completely', 'un', '##tr', '##ue', '.', '[SEP]']\n",
            "Original: ['A+']\n",
            "Tokenized: ['[CLS]', 'a', '+', '[SEP]']\n",
            "Original: ['I', 'would', 'rate', 'Fran', 'pcs', 'an', 'A+', 'because', 'the', 'price', 'was', 'lower', 'than', 'everyone', 'else,', 'i', 'got', 'my', 'computer', 'back', 'the', 'next', 'day,', 'and', 'the', 'professionalism', 'he', 'showed', 'was', 'great.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'rate', 'fran', 'pcs', 'an', 'a', '+', 'because', 'the', 'price', 'was', 'lower', 'than', 'everyone', 'else', ',', 'i', 'got', 'my', 'computer', 'back', 'the', 'next', 'day', ',', 'and', 'the', 'professional', '##ism', 'he', 'showed', 'was', 'great', '.', '[SEP]']\n",
            "Original: ['He', 'took', 'the', 'time', 'to', 'explain', 'things', 'to', 'me', 'about', 'my', 'computer,', 'i', 'would', 'recommend', 'you', 'go', 'to', 'him.']\n",
            "Tokenized: ['[CLS]', 'he', 'took', 'the', 'time', 'to', 'explain', 'things', 'to', 'me', 'about', 'my', 'computer', ',', 'i', 'would', 'recommend', 'you', 'go', 'to', 'him', '.', '[SEP]']\n",
            "Original: ['David']\n",
            "Tokenized: ['[CLS]', 'david', '[SEP]']\n",
            "Original: ['No', 'meat', 'on', 'Burger', 'and', 'too', 'much', 'pepper.']\n",
            "Tokenized: ['[CLS]', 'no', 'meat', 'on', 'burger', 'and', 'too', 'much', 'pepper', '.', '[SEP]']\n",
            "Original: ['no', 'place', 'to', 'seat']\n",
            "Tokenized: ['[CLS]', 'no', 'place', 'to', 'seat', '[SEP]']\n",
            "Original: ['They', 'must', 'have', 'read', 'these', 'reviews', 'and', 'improved!']\n",
            "Tokenized: ['[CLS]', 'they', 'must', 'have', 'read', 'these', 'reviews', 'and', 'improved', '!', '[SEP]']\n",
            "Original: ['My', 'husband', 'and', 'I', 'happened', 'in', 'on', 'a', 'whim.']\n",
            "Tokenized: ['[CLS]', 'my', 'husband', 'and', 'i', 'happened', 'in', 'on', 'a', 'w', '##him', '.', '[SEP]']\n",
            "Original: ['We', 'sat', 'in', 'the', 'front', 'dining', 'area,', 'it', 'was', 'very', 'cozy', 'and', 'pleasant.']\n",
            "Tokenized: ['[CLS]', 'we', 'sat', 'in', 'the', 'front', 'dining', 'area', ',', 'it', 'was', 'very', 'cozy', 'and', 'pleasant', '.', '[SEP]']\n",
            "Original: ['Our', 'server', 'was', 'quite', 'attentive', 'and', 'the', 'food', 'was', 'fantastic.']\n",
            "Tokenized: ['[CLS]', 'our', 'server', 'was', 'quite', 'at', '##ten', '##tive', 'and', 'the', 'food', 'was', 'fantastic', '.', '[SEP]']\n",
            "Original: ['My', 'husband', 'has', 'been', 'a', 'professional', 'chef,', 'so', 'he', 'is', 'a', 'good', 'judge', 'of', 'quality', 'food.']\n",
            "Tokenized: ['[CLS]', 'my', 'husband', 'has', 'been', 'a', 'professional', 'chef', ',', 'so', 'he', 'is', 'a', 'good', 'judge', 'of', 'quality', 'food', '.', '[SEP]']\n",
            "Original: ['This', 'was', 'a', 'flavorful,', 'enjoyable', 'meal', 'for', 'both', 'of', 'us.']\n",
            "Tokenized: ['[CLS]', 'this', 'was', 'a', 'flavor', '##ful', ',', 'enjoyable', 'meal', 'for', 'both', 'of', 'us', '.', '[SEP]']\n",
            "Original: ['Eulogic']\n",
            "Tokenized: ['[CLS]', 'eu', '##logic', '[SEP]']\n",
            "Original: ['Good', 'place', 'to', 'be', 'on', 'a', 'Sunday', 'Night.']\n",
            "Tokenized: ['[CLS]', 'good', 'place', 'to', 'be', 'on', 'a', 'sunday', 'night', '.', '[SEP]']\n",
            "Original: ['The', 'beers', 'were', 'good,', 'nice', 'choice', 'of', 'beers', 'as', 'well,', 'and', 'as', 'usual', 'the', 'mussels', 'were', 'great,', 'the', 'place', 'upstairs', 'is', 'a', 'nice', 'addition', 'to', 'the', 'bar', 'downstairs.']\n",
            "Tokenized: ['[CLS]', 'the', 'beers', 'were', 'good', ',', 'nice', 'choice', 'of', 'beers', 'as', 'well', ',', 'and', 'as', 'usual', 'the', 'mu', '##ssel', '##s', 'were', 'great', ',', 'the', 'place', 'upstairs', 'is', 'a', 'nice', 'addition', 'to', 'the', 'bar', 'downstairs', '.', '[SEP]']\n",
            "Original: ['Filled', 'up', 'on', 'too', 'much', 'beer', 'and', 'hence', 'cannot', 'comment', 'on', 'the', 'food.']\n",
            "Tokenized: ['[CLS]', 'filled', 'up', 'on', 'too', 'much', 'beer', 'and', 'hence', 'cannot', 'comment', 'on', 'the', 'food', '.', '[SEP]']\n",
            "Original: ['But', 'the', 'menu', 'had', 'standard', 'stuff', 'that', 'one', 'would', 'get', 'at', 'a', 'Belgian', 'Tavern.']\n",
            "Tokenized: ['[CLS]', 'but', 'the', 'menu', 'had', 'standard', 'stuff', 'that', 'one', 'would', 'get', 'at', 'a', 'belgian', 'tavern', '.', '[SEP]']\n",
            "Original: ['If', 'you', 'are', 'a', 'handcraft', 'beer', 'person,', 'this', 'is', 'a', 'fantastic', 'place', 'to', 'be.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'are', 'a', 'hand', '##craft', 'beer', 'person', ',', 'this', 'is', 'a', 'fantastic', 'place', 'to', 'be', '.', '[SEP]']\n",
            "Original: ['Well', 'kept', 'facility', 'with', 'friendly', 'staff.']\n",
            "Tokenized: ['[CLS]', 'well', 'kept', 'facility', 'with', 'friendly', 'staff', '.', '[SEP]']\n",
            "Original: [\"Ray's\", 'pizza', ':', 'my', 'favorite']\n",
            "Tokenized: ['[CLS]', 'ray', \"'\", 's', 'pizza', ':', 'my', 'favorite', '[SEP]']\n",
            "Original: [\"Ray's\", 'Pizza', 'is', 'just', 'too', 'good.']\n",
            "Tokenized: ['[CLS]', 'ray', \"'\", 's', 'pizza', 'is', 'just', 'too', 'good', '.', '[SEP]']\n",
            "Original: ['I', 'wish', 'I', 'could', 'have', 'a', 'slice', 'for', 'every', 'single', 'meal.']\n",
            "Tokenized: ['[CLS]', 'i', 'wish', 'i', 'could', 'have', 'a', 'slice', 'for', 'every', 'single', 'meal', '.', '[SEP]']\n",
            "Original: ['Luckily', 'I', 'live', 'very', 'close,', 'so', 'I', 'can', 'abuse', 'it', 'during', 'week-ends...']\n",
            "Tokenized: ['[CLS]', 'luckily', 'i', 'live', 'very', 'close', ',', 'so', 'i', 'can', 'abuse', 'it', 'during', 'week', '-', 'ends', '.', '.', '.', '[SEP]']\n",
            "Original: ['The', 'Best', 'Service', 'Ever!!']\n",
            "Tokenized: ['[CLS]', 'the', 'best', 'service', 'ever', '!', '!', '[SEP]']\n",
            "Original: ['I', 'have', 'never', 'had', 'better', 'service.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'never', 'had', 'better', 'service', '.', '[SEP]']\n",
            "Original: ['My', 'car', 'broke', 'down', 'and', 'roadside', 'towed', 'my', 'vehicle', 'to', 'Sussman', 'Kia.']\n",
            "Tokenized: ['[CLS]', 'my', 'car', 'broke', 'down', 'and', 'roadside', 'towed', 'my', 'vehicle', 'to', 'su', '##ss', '##man', 'kia', '.', '[SEP]']\n",
            "Original: ['They', 'squeezed', 'me', 'in', 'and', 'had', 'me', 'back', 'up', 'and', 'running', 'in', 'no', 'time.']\n",
            "Tokenized: ['[CLS]', 'they', 'squeezed', 'me', 'in', 'and', 'had', 'me', 'back', 'up', 'and', 'running', 'in', 'no', 'time', '.', '[SEP]']\n",
            "Original: ['Everyone', 'was', 'pleasant', 'and', 'very', 'helpful.']\n",
            "Tokenized: ['[CLS]', 'everyone', 'was', 'pleasant', 'and', 'very', 'helpful', '.', '[SEP]']\n",
            "Original: ['The', 'service', 'department', 'even', 'gave', 'me', 'a', 'ride', 'home', 'and', 'picked', 'me', 'up', 'when', 'my', 'car', 'was', 'finished.']\n",
            "Tokenized: ['[CLS]', 'the', 'service', 'department', 'even', 'gave', 'me', 'a', 'ride', 'home', 'and', 'picked', 'me', 'up', 'when', 'my', 'car', 'was', 'finished', '.', '[SEP]']\n",
            "Original: ['The', 'advisor', 'kept', 'me', 'up', 'to', 'date', 'and', 'informed', 'on', 'the', 'progress', 'of', 'my', 'vehicle.']\n",
            "Tokenized: ['[CLS]', 'the', 'advisor', 'kept', 'me', 'up', 'to', 'date', 'and', 'informed', 'on', 'the', 'progress', 'of', 'my', 'vehicle', '.', '[SEP]']\n",
            "Original: ['I', 'give', 'this', 'dealer', 'an', 'A+!']\n",
            "Tokenized: ['[CLS]', 'i', 'give', 'this', 'dealer', 'an', 'a', '+', '!', '[SEP]']\n",
            "Original: ['I', 'will', 'definitely', 'be', 'bringing', 'my', 'car', 'back', 'for', 'service.']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'definitely', 'be', 'bringing', 'my', 'car', 'back', 'for', 'service', '.', '[SEP]']\n",
            "Original: ['When', 'I', 'arrived', 'at', 'Brickell', 'Honda', 'on', '6/4/11,', 'I', 'was', 'greeted', 'and', 'attended', 'to', 'by', 'the', 'Sales', 'Manager,', 'Gustavo', 'Guerra,', 'in', 'a', 'very', 'friendly', 'and', 'professional', 'manner.']\n",
            "Tokenized: ['[CLS]', 'when', 'i', 'arrived', 'at', 'brick', '##ell', 'honda', 'on', '6', '/', '4', '/', '11', ',', 'i', 'was', 'greeted', 'and', 'attended', 'to', 'by', 'the', 'sales', 'manager', ',', 'gustavo', 'guerra', ',', 'in', 'a', 'very', 'friendly', 'and', 'professional', 'manner', '.', '[SEP]']\n",
            "Original: ['I', 'explained', 'to', 'him', 'what', 'I', 'wanted', 'and', 'that', 'I', 'previously', 'went', 'to', 'Braman', 'Honda.']\n",
            "Tokenized: ['[CLS]', 'i', 'explained', 'to', 'him', 'what', 'i', 'wanted', 'and', 'that', 'i', 'previously', 'went', 'to', 'bram', '##an', 'honda', '.', '[SEP]']\n",
            "Original: ['Bramen', 'Honda', 'was', 'a', 'bit', 'of', 'a', 'hassle.']\n",
            "Tokenized: ['[CLS]', 'bram', '##en', 'honda', 'was', 'a', 'bit', 'of', 'a', 'has', '##sle', '.', '[SEP]']\n",
            "Original: ['He', 'told', 'me', '\"no', 'problem,', 'we', 'will', 'match', 'the', 'offer', 'or', 'do', 'better.\"']\n",
            "Tokenized: ['[CLS]', 'he', 'told', 'me', '\"', 'no', 'problem', ',', 'we', 'will', 'match', 'the', 'offer', 'or', 'do', 'better', '.', '\"', '[SEP]']\n",
            "Original: ['Mr.', 'Guerra', 'gave', 'me', 'a', 'better', 'deal', 'without', 'any', 'hassles', 'nor', 'any', 'type', 'of', 'problems.']\n",
            "Tokenized: ['[CLS]', 'mr', '.', 'guerra', 'gave', 'me', 'a', 'better', 'deal', 'without', 'any', 'has', '##sle', '##s', 'nor', 'any', 'type', 'of', 'problems', '.', '[SEP]']\n",
            "Original: ['Brickell', 'Honda', 'has', 'been', 'the', 'best', 'buying', 'experience', 'in', 'the', 'world.']\n",
            "Tokenized: ['[CLS]', 'brick', '##ell', 'honda', 'has', 'been', 'the', 'best', 'buying', 'experience', 'in', 'the', 'world', '.', '[SEP]']\n",
            "Original: ['I', 'urge', 'all', 'St.', 'Thomas', 'the', 'Apostle', 'parishioners', 'and', 'all', 'of', 'South', 'Florida', 'residents', 'to', 'come', 'see', 'Gus!!!']\n",
            "Tokenized: ['[CLS]', 'i', 'urge', 'all', 'st', '.', 'thomas', 'the', 'apostle', 'parish', '##ion', '##ers', 'and', 'all', 'of', 'south', 'florida', 'residents', 'to', 'come', 'see', 'gus', '!', '!', '!', '[SEP]']\n",
            "Original: ['Excellent', 'customer', 'service!!!']\n",
            "Tokenized: ['[CLS]', 'excellent', 'customer', 'service', '!', '!', '!', '[SEP]']\n",
            "Original: ['The', 'management', 'and', 'staff', 'are', 'superb.']\n",
            "Tokenized: ['[CLS]', 'the', 'management', 'and', 'staff', 'are', 'superb', '.', '[SEP]']\n",
            "Original: ['I', 'worked', 'with', 'Sam', 'Mones', 'who', 'took', 'great', 'care', 'of', 'me.']\n",
            "Tokenized: ['[CLS]', 'i', 'worked', 'with', 'sam', 'mon', '##es', 'who', 'took', 'great', 'care', 'of', 'me', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'by', 'far', 'the', 'best', 'run', 'dealership', 'in', 'Miami.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'by', 'far', 'the', 'best', 'run', 'dealers', '##hip', 'in', 'miami', '.', '[SEP]']\n",
            "Original: ['beware', 'they', 'will', 'rip', 'u', 'off']\n",
            "Tokenized: ['[CLS]', 'be', '##ware', 'they', 'will', 'rip', 'u', 'off', '[SEP]']\n",
            "Original: ['Never', 'again']\n",
            "Tokenized: ['[CLS]', 'never', 'again', '[SEP]']\n",
            "Original: [\"Don't\", 'go', 'here', 'unless', 'you', 'want', 'to', 'sit,order,eat', 'and', 'be', 'asked', 'to', 'leave', 'all', 'in', 'a', 'matter', 'of', '20', 'minutes.']\n",
            "Tokenized: ['[CLS]', 'don', \"'\", 't', 'go', 'here', 'unless', 'you', 'want', 'to', 'sit', ',', 'order', ',', 'eat', 'and', 'be', 'asked', 'to', 'leave', 'all', 'in', 'a', 'matter', 'of', '20', 'minutes', '.', '[SEP]']\n",
            "Original: ['You', \"won't\", 'even', 'have', 'time', 'to', 'read', 'the', 'entire', 'menu', 'before', 'being', 'asked', 'to', 'order', 'and', 'if', 'you', 'ask', 'for', 'more', 'time', 'your', 'server', 'will', 'wait', 'at', 'the', 'table.']\n",
            "Tokenized: ['[CLS]', 'you', 'won', \"'\", 't', 'even', 'have', 'time', 'to', 'read', 'the', 'entire', 'menu', 'before', 'being', 'asked', 'to', 'order', 'and', 'if', 'you', 'ask', 'for', 'more', 'time', 'your', 'server', 'will', 'wait', 'at', 'the', 'table', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'the', 'only', 'place', 'I', 'have', 'ever', 'eaten', 'and', 'been', 'told', 'to', 'leave', 'because', 'other', 'people', 'were', 'waiting.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'the', 'only', 'place', 'i', 'have', 'ever', 'eaten', 'and', 'been', 'told', 'to', 'leave', 'because', 'other', 'people', 'were', 'waiting', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'told', 'to', 'take', 'my', 'coffee', 'to', 'go', 'if', 'I', 'wanted', 'to', 'finish', 'it.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'told', 'to', 'take', 'my', 'coffee', 'to', 'go', 'if', 'i', 'wanted', 'to', 'finish', 'it', '.', '[SEP]']\n",
            "Original: ['Oh,', 'and', 'their', 'liquor', 'license', 'was', 'expired', 'so', 'no', 'Bloody', 'Mary', 'or', 'Mimosas.']\n",
            "Tokenized: ['[CLS]', 'oh', ',', 'and', 'their', 'liquor', 'license', 'was', 'expired', 'so', 'no', 'bloody', 'mary', 'or', 'mi', '##mos', '##as', '.', '[SEP]']\n",
            "Original: ['Plus', 'the', 'drinks', 'are', 'self', 'service,', 'have', 'fun', 'trying', 'to', 'negotiate', 'the', 'small', 'cafeteria', 'space', 'to', 'get', 'your', 'coffee,', 'juice', 'or', 'water.']\n",
            "Tokenized: ['[CLS]', 'plus', 'the', 'drinks', 'are', 'self', 'service', ',', 'have', 'fun', 'trying', 'to', 'negotiate', 'the', 'small', 'cafeteria', 'space', 'to', 'get', 'your', 'coffee', ',', 'juice', 'or', 'water', '.', '[SEP]']\n",
            "Original: ['Go', 'next', 'door', 'to', 'the', 'Ball', 'Square', 'Cafe', 'instead.']\n",
            "Tokenized: ['[CLS]', 'go', 'next', 'door', 'to', 'the', 'ball', 'square', 'cafe', 'instead', '.', '[SEP]']\n",
            "Original: ['Excellent', 'service!']\n",
            "Tokenized: ['[CLS]', 'excellent', 'service', '!', '[SEP]']\n",
            "Original: ['I', 'am', 'so', 'glad', 'that', 'we', 'now', 'have', 'a', 'good', 'nail', 'shop', 'on', 'San', 'Mateo', 'Avenue!']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'so', 'glad', 'that', 'we', 'now', 'have', 'a', 'good', 'nail', 'shop', 'on', 'san', 'mateo', 'avenue', '!', '[SEP]']\n",
            "Original: ['No', 'more', 'having', 'to', 'drive', 'to', 'San', 'Francisco', 'for', 'a', 'great', 'mani', 'pedi.']\n",
            "Tokenized: ['[CLS]', 'no', 'more', 'having', 'to', 'drive', 'to', 'san', 'francisco', 'for', 'a', 'great', 'mani', 'pe', '##di', '.', '[SEP]']\n",
            "Original: ['Both', 'Tina', 'and', 'Vicky', 'are', 'excellent.']\n",
            "Tokenized: ['[CLS]', 'both', 'tina', 'and', 'vicky', 'are', 'excellent', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'definitely', 'refer', 'my', 'friends', 'and', 'family:)']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'definitely', 'refer', 'my', 'friends', 'and', 'family', ':', ')', '[SEP]']\n",
            "Original: ['Prime', 'rib', 'was', 'very', 'tough.']\n",
            "Tokenized: ['[CLS]', 'prime', 'rib', 'was', 'very', 'tough', '.', '[SEP]']\n",
            "Original: ['Staff', 'were', 'pleasant.']\n",
            "Tokenized: ['[CLS]', 'staff', 'were', 'pleasant', '.', '[SEP]']\n",
            "Original: [\"Won't\", 'return.']\n",
            "Tokenized: ['[CLS]', 'won', \"'\", 't', 'return', '.', '[SEP]']\n",
            "Original: ['Is', 'not', 'a', 'service', 'office']\n",
            "Tokenized: ['[CLS]', 'is', 'not', 'a', 'service', 'office', '[SEP]']\n",
            "Original: ['This', 'is', 'a', 'delivery', 'office', 'only', 'and', 'does', 'not', 'take', 'walk', 'ins', 'but', 'they', 'do', 'have', 'a', 'blue', 'box', 'out', 'front.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'a', 'delivery', 'office', 'only', 'and', 'does', 'not', 'take', 'walk', 'ins', 'but', 'they', 'do', 'have', 'a', 'blue', 'box', 'out', 'front', '.', '[SEP]']\n",
            "Original: ['Glad', 'I', 'called', 'before', 'I', 'arrived', 'with', 'my', 'box', 'to', 'ship.']\n",
            "Tokenized: ['[CLS]', 'glad', 'i', 'called', 'before', 'i', 'arrived', 'with', 'my', 'box', 'to', 'ship', '.', '[SEP]']\n",
            "Original: ['Thought', 'adding', 'a', 'comment', 'would', 'save', 'someone', 'the', 'hassle', 'with', 'a', 'useless', 'trip', 'there.']\n",
            "Tokenized: ['[CLS]', 'thought', 'adding', 'a', 'comment', 'would', 'save', 'someone', 'the', 'has', '##sle', 'with', 'a', 'useless', 'trip', 'there', '.', '[SEP]']\n",
            "Original: ['Fantastic', 'for', 'kids']\n",
            "Tokenized: ['[CLS]', 'fantastic', 'for', 'kids', '[SEP]']\n",
            "Original: ['If', 'you', 'have', 'children', 'or', 'are', 'just', 'a', 'real', 'animal', 'lover', 'yourself', \"you'll\", 'love', 'this', 'zoo.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'have', 'children', 'or', 'are', 'just', 'a', 'real', 'animal', 'lover', 'yourself', 'you', \"'\", 'll', 'love', 'this', 'zoo', '.', '[SEP]']\n",
            "Original: [\"It's\", 'only', '$10', 'and', 'in', 'essence', 'just', 'one', 'big', 'petting', 'zoo.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'only', '$', '10', 'and', 'in', 'essence', 'just', 'one', 'big', 'pet', '##ting', 'zoo', '.', '[SEP]']\n",
            "Original: ['They', 'sell', 'feed', 'and', 'milk', 'bottles', 'at', 'the', 'front', 'and', 'I', 'recommend', 'you', 'buy', 'lots.']\n",
            "Tokenized: ['[CLS]', 'they', 'sell', 'feed', 'and', 'milk', 'bottles', 'at', 'the', 'front', 'and', 'i', 'recommend', 'you', 'buy', 'lots', '.', '[SEP]']\n",
            "Original: ['We', 'took', 'our', '7', 'month', 'old', 'and', 'she', 'laughed', 'and', 'giggled', 'when', '(very', 'harshly', 'I', 'might', 'add)', 'grabbing', 'and', \"'kissed'\", 'the', 'goats', 'and', 'lambs.']\n",
            "Tokenized: ['[CLS]', 'we', 'took', 'our', '7', 'month', 'old', 'and', 'she', 'laughed', 'and', 'giggled', 'when', '(', 'very', 'harshly', 'i', 'might', 'add', ')', 'grabbing', 'and', \"'\", 'kissed', \"'\", 'the', 'goats', 'and', 'lamb', '##s', '.', '[SEP]']\n",
            "Original: ['The', 'animals', 'were', 'all', 'very', 'sweet', 'and', 'patient', 'with', 'her.']\n",
            "Tokenized: ['[CLS]', 'the', 'animals', 'were', 'all', 'very', 'sweet', 'and', 'patient', 'with', 'her', '.', '[SEP]']\n",
            "Original: ['Among', 'the', 'animals', 'that', 'were', 'available', 'to', 'touch', 'were', \"pony's,\", 'camels', 'and', 'EVEN', 'AN', 'OSTRICH!!!']\n",
            "Tokenized: ['[CLS]', 'among', 'the', 'animals', 'that', 'were', 'available', 'to', 'touch', 'were', 'pony', \"'\", 's', ',', 'camel', '##s', 'and', 'even', 'an', 'os', '##tric', '##h', '!', '!', '!', '[SEP]']\n",
            "Original: ['Wonderful,', 'inexpensive', 'and', 'lots', 'of', 'fun!']\n",
            "Tokenized: ['[CLS]', 'wonderful', ',', 'inexpensive', 'and', 'lots', 'of', 'fun', '!', '[SEP]']\n",
            "Original: ['GREAT', 'Store', 'GREAT', 'Service!']\n",
            "Tokenized: ['[CLS]', 'great', 'store', 'great', 'service', '!', '[SEP]']\n",
            "Original: ['“This', 'store', 'is', 'great!!']\n",
            "Tokenized: ['[CLS]', '“', 'this', 'store', 'is', 'great', '!', '!', '[SEP]']\n",
            "Original: ['I', 'love', 'walking', 'in', 'and', 'not', 'being', 'hassled.']\n",
            "Tokenized: ['[CLS]', 'i', 'love', 'walking', 'in', 'and', 'not', 'being', 'has', '##sle', '##d', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'there', 'when', 'they', 'did', 'a', 'free', 'raffle', 'in', 'August', 'and', 'I', 'won', 'a', 'hard', 'drive!']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'there', 'when', 'they', 'did', 'a', 'free', 'raf', '##fle', 'in', 'august', 'and', 'i', 'won', 'a', 'hard', 'drive', '!', '[SEP]']\n",
            "Original: ['The', 'reason', 'I', 'go', 'back', 'is', 'because', 'the', 'employees', 'are', 'sooooo', 'nice.”']\n",
            "Tokenized: ['[CLS]', 'the', 'reason', 'i', 'go', 'back', 'is', 'because', 'the', 'employees', 'are', 'soo', '##oo', '##o', 'nice', '.', '”', '[SEP]']\n",
            "Original: ['AMAZING']\n",
            "Tokenized: ['[CLS]', 'amazing', '[SEP]']\n",
            "Original: ['Absoul', 'is', 'the', 'greatest', 'donair', 'man', 'on', 'the', 'planet.']\n",
            "Tokenized: ['[CLS]', 'abs', '##ou', '##l', 'is', 'the', 'greatest', 'dona', '##ir', 'man', 'on', 'the', 'planet', '.', '[SEP]']\n",
            "Original: ['Highly', 'recommended.']\n",
            "Tokenized: ['[CLS]', 'highly', 'recommended', '.', '[SEP]']\n",
            "Original: ['If', 'you', 'enjoy', 'amazing', 'things,', 'you', 'must', 'go', 'to', \"World's\", 'Finest', 'Donair.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', 'enjoy', 'amazing', 'things', ',', 'you', 'must', 'go', 'to', 'world', \"'\", 's', 'finest', 'dona', '##ir', '.', '[SEP]']\n",
            "Original: ['Lest', 'you', 'be', 'lame!!!']\n",
            "Tokenized: ['[CLS]', 'lest', 'you', 'be', 'lame', '!', '!', '!', '[SEP]']\n",
            "Original: ['I', 'give', 'this', 'place', '11/10.']\n",
            "Tokenized: ['[CLS]', 'i', 'give', 'this', 'place', '11', '/', '10', '.', '[SEP]']\n",
            "Original: ['3', 'thumbs', 'up.']\n",
            "Tokenized: ['[CLS]', '3', 'thumbs', 'up', '.', '[SEP]']\n",
            "Original: ['Bon', 'appetit!']\n",
            "Tokenized: ['[CLS]', 'bon', 'app', '##eti', '##t', '!', '[SEP]']\n",
            "Original: [\"I've\", 'never', 'felt', 'the', 'need', 'to', 'write', 'a', 'review', 'or', 'make', 'a', 'complaint', 'before,', 'but', 'after', 'the', 'way', 'I', 'was', 'spoken', 'to', 'by', 'a', 'member', 'of', 'staff', 'at', 'the', 'kennels', '(whose', 'name', 'I', 'believe', 'to', 'be', 'Mrs', 'Closs)', 'I', 'would', 'now', 'not', 'recommend', 'this', 'business', 'to', 'anybody.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 've', 'never', 'felt', 'the', 'need', 'to', 'write', 'a', 'review', 'or', 'make', 'a', 'complaint', 'before', ',', 'but', 'after', 'the', 'way', 'i', 'was', 'spoken', 'to', 'by', 'a', 'member', 'of', 'staff', 'at', 'the', 'ken', '##nel', '##s', '(', 'whose', 'name', 'i', 'believe', 'to', 'be', 'mrs', 'cl', '##oss', ')', 'i', 'would', 'now', 'not', 'recommend', 'this', 'business', 'to', 'anybody', '.', '[SEP]']\n",
            "Original: ['If', 'the', 'animals', 'are', 'treated', 'in', 'the', 'same', 'way', 'the', 'customers', 'are', 'treated', 'then', 'this', 'leaves', 'a', 'lot', 'to', 'be', 'desired!']\n",
            "Tokenized: ['[CLS]', 'if', 'the', 'animals', 'are', 'treated', 'in', 'the', 'same', 'way', 'the', 'customers', 'are', 'treated', 'then', 'this', 'leaves', 'a', 'lot', 'to', 'be', 'desired', '!', '[SEP]']\n",
            "Original: ['Nobody', 'should', 'be', 'spoken', 'to', 'like', 'that', 'regardless', 'of', 'how', 'bad', 'their', 'day', 'may', 'have', 'been', 'or', 'what', 'may', 'be', 'going', 'on', 'in', 'their', 'private', 'lives!']\n",
            "Tokenized: ['[CLS]', 'nobody', 'should', 'be', 'spoken', 'to', 'like', 'that', 'regardless', 'of', 'how', 'bad', 'their', 'day', 'may', 'have', 'been', 'or', 'what', 'may', 'be', 'going', 'on', 'in', 'their', 'private', 'lives', '!', '[SEP]']\n",
            "Original: ['First', 'trip', 'to', 'Canada']\n",
            "Tokenized: ['[CLS]', 'first', 'trip', 'to', 'canada', '[SEP]']\n",
            "Original: ['I', 'recently', 'traveled', 'to', 'Canada', 'on', 'business', 'and', 'had', 'a', 'most', 'excellent', 'experience.']\n",
            "Tokenized: ['[CLS]', 'i', 'recently', 'traveled', 'to', 'canada', 'on', 'business', 'and', 'had', 'a', 'most', 'excellent', 'experience', '.', '[SEP]']\n",
            "Original: ['I', 'work', 'for', 'a', 'large', 'retail', 'company', 'recently', 'expanding', 'our', 'operations', 'into', 'Canada', 'and', 'had', 'to', 'travel', 'to', 'ensure', 'all', 'of', 'our', 'computer', 'network', 'equipment', 'was', 'installed', 'properly', 'and', 'on', 'time.']\n",
            "Tokenized: ['[CLS]', 'i', 'work', 'for', 'a', 'large', 'retail', 'company', 'recently', 'expanding', 'our', 'operations', 'into', 'canada', 'and', 'had', 'to', 'travel', 'to', 'ensure', 'all', 'of', 'our', 'computer', 'network', 'equipment', 'was', 'installed', 'properly', 'and', 'on', 'time', '.', '[SEP]']\n",
            "Original: ['This', 'can', 'tend', 'to', 'be', 'a', 'stressful', 'experience', 'in', 'itself', 'let', 'alone', 'adding', 'crossing', 'boarders', 'for', 'the', 'first', 'time.']\n",
            "Tokenized: ['[CLS]', 'this', 'can', 'tend', 'to', 'be', 'a', 'stress', '##ful', 'experience', 'in', 'itself', 'let', 'alone', 'adding', 'crossing', 'board', '##ers', 'for', 'the', 'first', 'time', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'very', 'pleased', 'to', 'find', 'my', 'accommodations', 'and', 'the', 'hotel', 'staff', 'to', 'be', 'a', 'very', 'calming', 'and', 'comforting', 'part', 'of', 'my', 'trip.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'very', 'pleased', 'to', 'find', 'my', 'accommodations', 'and', 'the', 'hotel', 'staff', 'to', 'be', 'a', 'very', 'calming', 'and', 'comforting', 'part', 'of', 'my', 'trip', '.', '[SEP]']\n",
            "Original: ['I', 'found', 'the', 'hotel', 'to', 'be', 'amazingly', 'clean,', 'not', 'to', 'mention', 'very', 'well', 'adorned', 'with', 'many', 'pleasant', 'surprises.']\n",
            "Tokenized: ['[CLS]', 'i', 'found', 'the', 'hotel', 'to', 'be', 'amazingly', 'clean', ',', 'not', 'to', 'mention', 'very', 'well', 'adorned', 'with', 'many', 'pleasant', 'surprises', '.', '[SEP]']\n",
            "Original: ['From', 'my', 'first', 'encounter', 'at', 'check', 'in', 'to', 'my', 'regrettable', 'check', 'out', 'I', 'found', 'the', 'staff', 'and', 'facility', 'to', 'exceed', 'my', 'expectation.']\n",
            "Tokenized: ['[CLS]', 'from', 'my', 'first', 'encounter', 'at', 'check', 'in', 'to', 'my', 'regret', '##table', 'check', 'out', 'i', 'found', 'the', 'staff', 'and', 'facility', 'to', 'exceed', 'my', 'expectation', '.', '[SEP]']\n",
            "Original: ['I', 'completely', 'enjoyed', 'my', 'whole', 'check', 'in', 'experience', 'and', 'was', 'impressed', 'with', 'the', 'friendliness', 'and', 'professionalism', 'of', 'the', 'staff', 'as', 'well', 'as', 'the', 'accommodations', 'themselves.']\n",
            "Tokenized: ['[CLS]', 'i', 'completely', 'enjoyed', 'my', 'whole', 'check', 'in', 'experience', 'and', 'was', 'impressed', 'with', 'the', 'friend', '##liness', 'and', 'professional', '##ism', 'of', 'the', 'staff', 'as', 'well', 'as', 'the', 'accommodations', 'themselves', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'be', 'traveling', 'in', 'this', 'area', 'in', 'the', 'future', 'and', 'you', 'can', 'be', 'assured', 'that', 'this', 'experience', 'will', 'be', 'helpful', 'in', 'my', 'choice', 'of', 'hotels', 'and', 'Novotel', 'will', 'be', 'my', 'first', 'selection.']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'be', 'traveling', 'in', 'this', 'area', 'in', 'the', 'future', 'and', 'you', 'can', 'be', 'assured', 'that', 'this', 'experience', 'will', 'be', 'helpful', 'in', 'my', 'choice', 'of', 'hotels', 'and', 'novo', '##tel', 'will', 'be', 'my', 'first', 'selection', '.', '[SEP]']\n",
            "Original: ['Yes', 'the', 'parking', 'can', 'be', 'a', 'challenge', 'but', 'being', 'from', 'NJ', 'I', 'am', 'no', 'stranger', 'to', 'tight', 'corners.']\n",
            "Tokenized: ['[CLS]', 'yes', 'the', 'parking', 'can', 'be', 'a', 'challenge', 'but', 'being', 'from', 'nj', 'i', 'am', 'no', 'stranger', 'to', 'tight', 'corners', '.', '[SEP]']\n",
            "Original: ['Please', 'pass', 'my', 'appreciation', 'to', 'the', 'Staff', 'and', 'Management', 'for', 'their', 'excellent', 'hospitality', 'and', 'good', 'spirits', 'as', 'it', 'helped', 'make', 'a', 'stressful', 'trip', 'enjoyable.']\n",
            "Tokenized: ['[CLS]', 'please', 'pass', 'my', 'appreciation', 'to', 'the', 'staff', 'and', 'management', 'for', 'their', 'excellent', 'hospitality', 'and', 'good', 'spirits', 'as', 'it', 'helped', 'make', 'a', 'stress', '##ful', 'trip', 'enjoyable', '.', '[SEP]']\n",
            "Original: ['I', 'interviewed', 'several', 'contractors', 'for', 'a', 'kitchen', 'remodel.']\n",
            "Tokenized: ['[CLS]', 'i', 'interviewed', 'several', 'contractors', 'for', 'a', 'kitchen', 're', '##mo', '##del', '.', '[SEP]']\n",
            "Original: ['Liberty', 'construction', 'shows', 'up', 'and', \"it's\", 'two', 'guys...all', 'I', 'get', 'is', '100%', 'sales', 'pitch', '\"We\\'re', 'the', \"best...we're\", 'number', '50', 'so', 'we', 'must', 'be', 'doing', 'something', 'right...look', 'at', 'all', 'these', 'certificates', 'that', 'say', \"we're\", 'great\".']\n",
            "Tokenized: ['[CLS]', 'liberty', 'construction', 'shows', 'up', 'and', 'it', \"'\", 's', 'two', 'guys', '.', '.', '.', 'all', 'i', 'get', 'is', '100', '%', 'sales', 'pitch', '\"', 'we', \"'\", 're', 'the', 'best', '.', '.', '.', 'we', \"'\", 're', 'number', '50', 'so', 'we', 'must', 'be', 'doing', 'something', 'right', '.', '.', '.', 'look', 'at', 'all', 'these', 'certificates', 'that', 'say', 'we', \"'\", 're', 'great', '\"', '.', '[SEP]']\n",
            "Original: ['Not', 'once', 'did', 'I', 'feel', 'listened', 'to', 'like', 'they', 'actually', 'cared', 'about', 'what', 'I', 'wanted,', 'all', 'they', 'were', 'interested', 'in', 'was', 'me', 'signing', 'a', 'contract', 'right', 'then', 'and', 'there.']\n",
            "Tokenized: ['[CLS]', 'not', 'once', 'did', 'i', 'feel', 'listened', 'to', 'like', 'they', 'actually', 'cared', 'about', 'what', 'i', 'wanted', ',', 'all', 'they', 'were', 'interested', 'in', 'was', 'me', 'signing', 'a', 'contract', 'right', 'then', 'and', 'there', '.', '[SEP]']\n",
            "Original: ['Very', 'high', 'pressured', 'sales', 'and', 'with', 'the', 'reviews', 'of', 'many', 'others', 'bad', 'service.']\n",
            "Tokenized: ['[CLS]', 'very', 'high', 'pressured', 'sales', 'and', 'with', 'the', 'reviews', 'of', 'many', 'others', 'bad', 'service', '.', '[SEP]']\n",
            "Original: [\"I'm\", 'glad', 'I', 'trusted', 'my', 'gut', 'and', \"didn't\", 'get', 'sucked', 'into', 'doing', 'business', 'with', 'them.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'm', 'glad', 'i', 'trusted', 'my', 'gut', 'and', 'didn', \"'\", 't', 'get', 'sucked', 'into', 'doing', 'business', 'with', 'them', '.', '[SEP]']\n",
            "Original: ['Find', 'someone', 'you', 'trust', 'that', 'actually', 'hears', 'you', 'and', 'wants', 'to', 'do', 'the', 'job', 'right.']\n",
            "Tokenized: ['[CLS]', 'find', 'someone', 'you', 'trust', 'that', 'actually', 'hears', 'you', 'and', 'wants', 'to', 'do', 'the', 'job', 'right', '.', '[SEP]']\n",
            "Original: ['Craft', 'Wonderland', 'with', 'History']\n",
            "Tokenized: ['[CLS]', 'craft', 'wonderland', 'with', 'history', '[SEP]']\n",
            "Original: ['My', 'first', 'visit', 'was', 'so', 'fun', 'yesterday.']\n",
            "Tokenized: ['[CLS]', 'my', 'first', 'visit', 'was', 'so', 'fun', 'yesterday', '.', '[SEP]']\n",
            "Original: ['I', 'could', 'have', 'stayed', 'all', 'day', 'and', 'not', 'seen', 'all', 'the', 'things.']\n",
            "Tokenized: ['[CLS]', 'i', 'could', 'have', 'stayed', 'all', 'day', 'and', 'not', 'seen', 'all', 'the', 'things', '.', '[SEP]']\n",
            "Original: ['I', 'am', 'doing', 'origami', 'jewelry', 'and', 'found', 'exactly', 'the', 'right', 'things', 'for', 'earrings', 'and', 'got', 'many', 'other', 'ideas', 'there', 'too.']\n",
            "Tokenized: ['[CLS]', 'i', 'am', 'doing', 'or', '##iga', '##mi', 'jewelry', 'and', 'found', 'exactly', 'the', 'right', 'things', 'for', 'earrings', 'and', 'got', 'many', 'other', 'ideas', 'there', 'too', '.', '[SEP]']\n",
            "Original: ['I', 'bought', 'a', 'beginners', 'quilling', 'set', 'and', 'like', 'making', 'the', 'filigree', 'forms', 'you', 'can', 'make', 'and', 'add', 'to', 'other', 'crafts.']\n",
            "Tokenized: ['[CLS]', 'i', 'bought', 'a', 'begin', '##ners', 'qui', '##lling', 'set', 'and', 'like', 'making', 'the', 'fi', '##li', '##gree', 'forms', 'you', 'can', 'make', 'and', 'add', 'to', 'other', 'crafts', '.', '[SEP]']\n",
            "Original: ['The', 'owner,', 'Jean,', 'has', 'been', 'there', '31', 'years!']\n",
            "Tokenized: ['[CLS]', 'the', 'owner', ',', 'jean', ',', 'has', 'been', 'there', '31', 'years', '!', '[SEP]']\n",
            "Original: ['What', 'a', 'history.']\n",
            "Tokenized: ['[CLS]', 'what', 'a', 'history', '.', '[SEP]']\n",
            "Original: ['She', 'is', 'a', 'super', 'sweet,', 'lovable', 'and', 'well-informed', 'woman', 'with', 'a', 'great', 'sense', 'of', 'humor.']\n",
            "Tokenized: ['[CLS]', 'she', 'is', 'a', 'super', 'sweet', ',', 'lo', '##vable', 'and', 'well', '-', 'informed', 'woman', 'with', 'a', 'great', 'sense', 'of', 'humor', '.', '[SEP]']\n",
            "Original: ['I', 'really', 'enjoyed', 'meeting', 'her', 'and', 'happy', 'to', 'learn', 'she', 'comes', 'from', 'Oklahoma', 'and', 'has', 'the', 'values', 'of', 'a', 'solid', 'no', 'bs', 'country', 'girl.']\n",
            "Tokenized: ['[CLS]', 'i', 'really', 'enjoyed', 'meeting', 'her', 'and', 'happy', 'to', 'learn', 'she', 'comes', 'from', 'oklahoma', 'and', 'has', 'the', 'values', 'of', 'a', 'solid', 'no', 'bs', 'country', 'girl', '.', '[SEP]']\n",
            "Original: ['This', 'store', 'is', 'a', 'real', 'gem', 'and', 'has', 'much', 'to', 'offer', 'the', 'serious', 'crafter', 'or', 'the', 'occasional', 'crafter.']\n",
            "Tokenized: ['[CLS]', 'this', 'store', 'is', 'a', 'real', 'gem', 'and', 'has', 'much', 'to', 'offer', 'the', 'serious', 'craft', '##er', 'or', 'the', 'occasional', 'craft', '##er', '.', '[SEP]']\n",
            "Original: ['By', 'the', 'way,', 'Salmagundi', '(the', 'store', 'name)', 'means', 'something', 'like', 'smorgasbord;', 'potpourri;', 'motley;', 'variety;', 'mixed', 'bag;', 'miscellaneous', 'assortment;', 'mixture,', 'a', 'variety', 'of', 'many', 'kinds', 'of', 'things.']\n",
            "Tokenized: ['[CLS]', 'by', 'the', 'way', ',', 'sal', '##ma', '##gun', '##di', '(', 'the', 'store', 'name', ')', 'means', 'something', 'like', 'sm', '##org', '##as', '##bor', '##d', ';', 'pot', '##pour', '##ri', ';', 'mo', '##tley', ';', 'variety', ';', 'mixed', 'bag', ';', 'miscellaneous', 'assortment', ';', 'mixture', ',', 'a', 'variety', 'of', 'many', 'kinds', 'of', 'things', '.', '[SEP]']\n",
            "Original: ['Great', 'name', 'for', 'a', 'great', 'store!']\n",
            "Tokenized: ['[CLS]', 'great', 'name', 'for', 'a', 'great', 'store', '!', '[SEP]']\n",
            "Original: ['Shop', 'Local!']\n",
            "Tokenized: ['[CLS]', 'shop', 'local', '!', '[SEP]']\n",
            "Original: ['Barbara', 'Quimba', '1/30/10']\n",
            "Tokenized: ['[CLS]', 'barbara', 'qui', '##mba', '1', '/', '30', '/', '10', '[SEP]']\n",
            "Original: ['VERYYYY!!!!', 'VERYYY!!', 'Good', 'auto', 'repair', 'men.']\n",
            "Tokenized: ['[CLS]', 'very', '##y', '##y', '##y', '!', '!', '!', '!', 'very', '##y', '##y', '!', '!', 'good', 'auto', 'repair', 'men', '.', '[SEP]']\n",
            "Original: ['Do', 'the', 'job', 'honest', 'and', 'quickly', 'as', 'possible.']\n",
            "Tokenized: ['[CLS]', 'do', 'the', 'job', 'honest', 'and', 'quickly', 'as', 'possible', '.', '[SEP]']\n",
            "Original: ['Would', '100%', 'recomend', 'to', 'others', 'for', 'a', 'great', 'service.']\n",
            "Tokenized: ['[CLS]', 'would', '100', '%', 'rec', '##ome', '##nd', 'to', 'others', 'for', 'a', 'great', 'service', '.', '[SEP]']\n",
            "Original: ['Thank', 'You', 'Barrys', 'Auto', 'Tech!']\n",
            "Tokenized: ['[CLS]', 'thank', 'you', 'barry', '##s', 'auto', 'tech', '!', '[SEP]']\n",
            "Original: ['Fantastic', 'Nova', 'Scotia', 'Cottage']\n",
            "Tokenized: ['[CLS]', 'fantastic', 'nova', 'scotia', 'cottage', '[SEP]']\n",
            "Original: ['We', 'had', 'a', 'fantastic', 'time.']\n",
            "Tokenized: ['[CLS]', 'we', 'had', 'a', 'fantastic', 'time', '.', '[SEP]']\n",
            "Original: ['Such', 'a', 'relaxing', 'atmosphere', 'and', 'inspiring', 'architecture.']\n",
            "Tokenized: ['[CLS]', 'such', 'a', 'relaxing', 'atmosphere', 'and', 'inspiring', 'architecture', '.', '[SEP]']\n",
            "Original: ['Sand', 'Hill', 'park', 'was', 'a', 'great', 'beach...']\n",
            "Tokenized: ['[CLS]', 'sand', 'hill', 'park', 'was', 'a', 'great', 'beach', '.', '.', '.', '[SEP]']\n",
            "Original: ['Nice', 'warm', 'water.']\n",
            "Tokenized: ['[CLS]', 'nice', 'warm', 'water', '.', '[SEP]']\n",
            "Original: ['Thank-You', 'for', 'sharing', 'your', 'cottage!']\n",
            "Tokenized: ['[CLS]', 'thank', '-', 'you', 'for', 'sharing', 'your', 'cottage', '!', '[SEP]']\n",
            "Original: ['Are', 'you', 'kidding', 'me?']\n",
            "Tokenized: ['[CLS]', 'are', 'you', 'kidding', 'me', '?', '[SEP]']\n",
            "Original: ['I', \"don't\", 'get', 'it.']\n",
            "Tokenized: ['[CLS]', 'i', 'don', \"'\", 't', 'get', 'it', '.', '[SEP]']\n",
            "Original: ['Spongy', 'and', 'sweet', 'bread', '(microwaved?),', 'heartless', 'salsa,', 'tiny', 'dogs...']\n",
            "Tokenized: ['[CLS]', 'sp', '##ong', '##y', 'and', 'sweet', 'bread', '(', 'microwave', '##d', '?', ')', ',', 'heart', '##less', 'salsa', ',', 'tiny', 'dogs', '.', '.', '.', '[SEP]']\n",
            "Original: ['You', 'order', 'at', 'the', 'counter', 'and', 'there', 'is', 'a', 'space', 'for', 'tip', 'on', 'your', 'credit', 'card', 'receipt.']\n",
            "Tokenized: ['[CLS]', 'you', 'order', 'at', 'the', 'counter', 'and', 'there', 'is', 'a', 'space', 'for', 'tip', 'on', 'your', 'credit', 'card', 'receipt', '.', '[SEP]']\n",
            "Original: ['The', 'dude', 'who', 'grills', 'the', 'retarded', 'dogs', 'is', 'rude.']\n",
            "Tokenized: ['[CLS]', 'the', 'dude', 'who', 'grill', '##s', 'the', 're', '##tar', '##ded', 'dogs', 'is', 'rude', '.', '[SEP]']\n",
            "Original: ['If', 'this', 'is', 'the', 'best', 'that', 'Tucson', 'has', 'to', 'offer,', 'I', 'am', 'outta', 'here...']\n",
            "Tokenized: ['[CLS]', 'if', 'this', 'is', 'the', 'best', 'that', 'tucson', 'has', 'to', 'offer', ',', 'i', 'am', 'outta', 'here', '.', '.', '.', '[SEP]']\n",
            "Original: ['FANTASTIC', 'STORE!!!']\n",
            "Tokenized: ['[CLS]', 'fantastic', 'store', '!', '!', '!', '[SEP]']\n",
            "Original: ['I', 'came', 'upon', 'this', 'store', 'as', 'the', 'building', 'caught', 'my', 'eye.']\n",
            "Tokenized: ['[CLS]', 'i', 'came', 'upon', 'this', 'store', 'as', 'the', 'building', 'caught', 'my', 'eye', '.', '[SEP]']\n",
            "Original: [\"It's\", 'located', 'in', 'the', 'huge', 'HONKA', 'Log', 'Homes', 'building,', 'by', 'Walmart', 'off', 'of', 'Evergreen', 'Parkway.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'located', 'in', 'the', 'huge', 'hon', '##ka', 'log', 'homes', 'building', ',', 'by', 'wal', '##mart', 'off', 'of', 'evergreen', 'parkway', '.', '[SEP]']\n",
            "Original: ['The', 'store', 'was', 'decorated', 'with', 'furnishings', '&', 'accessories.']\n",
            "Tokenized: ['[CLS]', 'the', 'store', 'was', 'decorated', 'with', 'furnishings', '&', 'accessories', '.', '[SEP]']\n",
            "Original: ['The', 'friendly', 'crew', 'working', 'was', 'great', '&', 'very', 'helpful.']\n",
            "Tokenized: ['[CLS]', 'the', 'friendly', 'crew', 'working', 'was', 'great', '&', 'very', 'helpful', '.', '[SEP]']\n",
            "Original: ['This', 'store', 'is', 'what', 'Colorado', 'is', 'all', 'about.']\n",
            "Tokenized: ['[CLS]', 'this', 'store', 'is', 'what', 'colorado', 'is', 'all', 'about', '.', '[SEP]']\n",
            "Original: ['Also,', 'I', 'purchased', 'some', 'furniture', 'last', 'year', 'and', 'all', 'has', 'been', 'great!']\n",
            "Tokenized: ['[CLS]', 'also', ',', 'i', 'purchased', 'some', 'furniture', 'last', 'year', 'and', 'all', 'has', 'been', 'great', '!', '[SEP]']\n",
            "Original: [\"It's\", 'durability', '&', 'look', 'was', 'perfect', 'and', 'I', 'will', 'definitely', 'be', 'adding', 'to', 'my', 'collection', 'soon!']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'du', '##ra', '##bility', '&', 'look', 'was', 'perfect', 'and', 'i', 'will', 'definitely', 'be', 'adding', 'to', 'my', 'collection', 'soon', '!', '[SEP]']\n",
            "Original: ['Easiest', 'Time', 'I', 'ever', 'had', 'purchasing', 'a', 'car!']\n",
            "Tokenized: ['[CLS]', 'easiest', 'time', 'i', 'ever', 'had', 'purchasing', 'a', 'car', '!', '[SEP]']\n",
            "Original: ['Excellent', 'service,', 'Not', 'only', 'did', 'they', 'get', 'the', 'exact', 'car', 'I', 'wanted', 'win', 'in', '48', 'hours', 'but', 'the', 'sales', 'man', 'also', 'took', 'me', 'out', 'to', 'lunch.']\n",
            "Tokenized: ['[CLS]', 'excellent', 'service', ',', 'not', 'only', 'did', 'they', 'get', 'the', 'exact', 'car', 'i', 'wanted', 'win', 'in', '48', 'hours', 'but', 'the', 'sales', 'man', 'also', 'took', 'me', 'out', 'to', 'lunch', '.', '[SEP]']\n",
            "Original: ['Very', 'kind', 'and', 'reliable.']\n",
            "Tokenized: ['[CLS]', 'very', 'kind', 'and', 'reliable', '.', '[SEP]']\n",
            "Original: ['I', 'highly', 'recommend', 'this', 'dealership', 'if', 'you', 'would', 'not', 'like', 'to', 'hassle', 'on', 'price', 'and', 'receive', 'friendly', 'service.']\n",
            "Tokenized: ['[CLS]', 'i', 'highly', 'recommend', 'this', 'dealers', '##hip', 'if', 'you', 'would', 'not', 'like', 'to', 'has', '##sle', 'on', 'price', 'and', 'receive', 'friendly', 'service', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'since', 'purchased', 'two', 'cars', 'from', 'this', 'dealership,', 'The', 'first', 'one', 'was', 'from', 'Phillip', 'and', 'the', 'second', 'was', 'from', 'Richard.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'since', 'purchased', 'two', 'cars', 'from', 'this', 'dealers', '##hip', ',', 'the', 'first', 'one', 'was', 'from', 'phillip', 'and', 'the', 'second', 'was', 'from', 'richard', '.', '[SEP]']\n",
            "Original: ['Both', 'were', 'excellent', 'sales', 'men', 'who', 'put', 'my', 'needs', 'first.']\n",
            "Tokenized: ['[CLS]', 'both', 'were', 'excellent', 'sales', 'men', 'who', 'put', 'my', 'needs', 'first', '.', '[SEP]']\n",
            "Original: ['great', 'tmobile', 'service']\n",
            "Tokenized: ['[CLS]', 'great', 't', '##mobile', 'service', '[SEP]']\n",
            "Original: ['I', 'was', 'with', 'verizon', 'and', 'I', 'checked', 'my', 'service', 'with', 'tmobile', 'and', 'it', 'was', 'great', 'so', 'I', 'thought', 'I', 'would', 'try', 'tmobile.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'with', 've', '##riz', '##on', 'and', 'i', 'checked', 'my', 'service', 'with', 't', '##mobile', 'and', 'it', 'was', 'great', 'so', 'i', 'thought', 'i', 'would', 'try', 't', '##mobile', '.', '[SEP]']\n",
            "Original: ['It', 'turned', 'out', 'being', 'very', 'good', 'quality', 'tmobile', 'service', 'and', 'I', 'was', 'happy', 'with', 'the', 'new', 'tmobile', 'phone.']\n",
            "Tokenized: ['[CLS]', 'it', 'turned', 'out', 'being', 'very', 'good', 'quality', 't', '##mobile', 'service', 'and', 'i', 'was', 'happy', 'with', 'the', 'new', 't', '##mobile', 'phone', '.', '[SEP]']\n",
            "Original: ['Great', 'first', 'experience.']\n",
            "Tokenized: ['[CLS]', 'great', 'first', 'experience', '.', '[SEP]']\n",
            "Original: [\"I'm\", '22,', 'and', 'my', 'hairdresser', 'was', 'great', '(and', 'not', '\"old\"', 'like', 'one', 'of', 'the', 'reviews', 'says)', '-', 'she', 'really', 'listened', 'to', 'what', 'I', 'wanted', 'and', 'gave', 'me', 'tons', 'of', 'tips', 'on', 'how', 'to', 'style', 'my', 'hair', 'so', 'I', 'could', 'get', 'it', 'to', 'look', 'the', 'way', 'I', 'wanted', 'it', 'to.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 'm', '22', ',', 'and', 'my', 'hair', '##dre', '##sser', 'was', 'great', '(', 'and', 'not', '\"', 'old', '\"', 'like', 'one', 'of', 'the', 'reviews', 'says', ')', '-', 'she', 'really', 'listened', 'to', 'what', 'i', 'wanted', 'and', 'gave', 'me', 'tons', 'of', 'tips', 'on', 'how', 'to', 'style', 'my', 'hair', 'so', 'i', 'could', 'get', 'it', 'to', 'look', 'the', 'way', 'i', 'wanted', 'it', 'to', '.', '[SEP]']\n",
            "Original: ['She', 'deep', 'conditioned', 'my', 'hair', 'and', 'took', 'the', 'time', 'to', 'style', 'it', 'properly.']\n",
            "Tokenized: ['[CLS]', 'she', 'deep', 'conditioned', 'my', 'hair', 'and', 'took', 'the', 'time', 'to', 'style', 'it', 'properly', '.', '[SEP]']\n",
            "Original: ['She', 'recommended', 'products', 'but', 'absolutely', \"didn't\", 'pressure', 'me', 'to', 'buy.']\n",
            "Tokenized: ['[CLS]', 'she', 'recommended', 'products', 'but', 'absolutely', 'didn', \"'\", 't', 'pressure', 'me', 'to', 'buy', '.', '[SEP]']\n",
            "Original: [\"It's\", 'a', 'cute', 'place', 'with', 'a', 'really', 'friendly,', 'laid-back', 'atmosphere.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'a', 'cute', 'place', 'with', 'a', 'really', 'friendly', ',', 'laid', '-', 'back', 'atmosphere', '.', '[SEP]']\n",
            "Original: ['I', 'would', 'highly', 'recommend', 'it', 'and', 'will', 'be', 'going', 'back', 'for', 'my', 'next', 'haircut.']\n",
            "Tokenized: ['[CLS]', 'i', 'would', 'highly', 'recommend', 'it', 'and', 'will', 'be', 'going', 'back', 'for', 'my', 'next', 'hair', '##cut', '.', '[SEP]']\n",
            "Original: ['Oh!']\n",
            "Tokenized: ['[CLS]', 'oh', '!', '[SEP]']\n",
            "Original: ['And', 'students', 'get', '$5', 'off,', \"can't\", 'argue', 'with', 'that.']\n",
            "Tokenized: ['[CLS]', 'and', 'students', 'get', '$', '5', 'off', ',', 'can', \"'\", 't', 'argue', 'with', 'that', '.', '[SEP]']\n",
            "Original: ['This', 'is', 'the', 'best', 'Mediterranean', 'Restaurant', 'in', 'the', 'West', 'Valley,', 'I', 'have', 'friend', 'who', 'drive', 'from', 'central', 'Phx', 'to', 'come', 'here.']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'the', 'best', 'mediterranean', 'restaurant', 'in', 'the', 'west', 'valley', ',', 'i', 'have', 'friend', 'who', 'drive', 'from', 'central', 'ph', '##x', 'to', 'come', 'here', '.', '[SEP]']\n",
            "Original: ['Too', 'many', 'kids,', 'too', 'many', 'knifings,', 'too', 'many', 'taserings.']\n",
            "Tokenized: ['[CLS]', 'too', 'many', 'kids', ',', 'too', 'many', 'kn', '##if', '##ings', ',', 'too', 'many', 'ta', '##ser', '##ings', '.', '[SEP]']\n",
            "Original: ['Worth', 'Every', 'Penny']\n",
            "Tokenized: ['[CLS]', 'worth', 'every', 'penny', '[SEP]']\n",
            "Original: ['My', 'girlfriend', 'and', 'I', 'ate', 'at', 'The', 'Grill', 'last', 'night,', 'and', 'our', 'experience', 'was', 'amazing.']\n",
            "Tokenized: ['[CLS]', 'my', 'girlfriend', 'and', 'i', 'ate', 'at', 'the', 'grill', 'last', 'night', ',', 'and', 'our', 'experience', 'was', 'amazing', '.', '[SEP]']\n",
            "Original: ['Everything', 'we', 'ordered', 'was', 'prepared', 'to', 'perfection,', 'and', 'was', 'presented', 'perfectly.']\n",
            "Tokenized: ['[CLS]', 'everything', 'we', 'ordered', 'was', 'prepared', 'to', 'perfection', ',', 'and', 'was', 'presented', 'perfectly', '.', '[SEP]']\n",
            "Original: ['The', 'asparagus,', 'seared', 'tuna,', 'and', 'lobster', 'tail', 'were', 'the', 'best', 'we', 'ever', 'had.']\n",
            "Tokenized: ['[CLS]', 'the', 'as', '##para', '##gus', ',', 'sea', '##red', 'tuna', ',', 'and', 'lobster', 'tail', 'were', 'the', 'best', 'we', 'ever', 'had', '.', '[SEP]']\n",
            "Original: ['Then', 'the', 'desserts', 'came,', 'and', 'they', 'were', 'hands', 'down', 'the', 'best', 'dessert', 'we', 'ever', 'had.']\n",
            "Tokenized: ['[CLS]', 'then', 'the', 'dessert', '##s', 'came', ',', 'and', 'they', 'were', 'hands', 'down', 'the', 'best', 'dessert', 'we', 'ever', 'had', '.', '[SEP]']\n",
            "Original: ['I', 'will', 'sum', 'it', 'up', 'with,', 'it', 'was', 'worth', 'every', 'penny!']\n",
            "Tokenized: ['[CLS]', 'i', 'will', 'sum', 'it', 'up', 'with', ',', 'it', 'was', 'worth', 'every', 'penny', '!', '[SEP]']\n",
            "Original: ['great!']\n",
            "Tokenized: ['[CLS]', 'great', '!', '[SEP]']\n",
            "Original: ['I', 'go', 'Disco', 'dancing', 'and', 'Cheerleading.']\n",
            "Tokenized: ['[CLS]', 'i', 'go', 'disco', 'dancing', 'and', 'cheerleading', '.', '[SEP]']\n",
            "Original: ['Its', 'fab!']\n",
            "Tokenized: ['[CLS]', 'its', 'fa', '##b', '!', '[SEP]']\n",
            "Original: ['so', 'goand', 'get', 'dancing!!!!!!!!!!!!!!!!!!!!!!!!!']\n",
            "Tokenized: ['[CLS]', 'so', 'goa', '##nd', 'get', 'dancing', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '!', '[SEP]']\n",
            "Original: ['By', 'samantha', 'Fox']\n",
            "Tokenized: ['[CLS]', 'by', 'samantha', 'fox', '[SEP]']\n",
            "Original: ['Great', 'quality', 'doors', 'and', 'great', 'quality', 'people!']\n",
            "Tokenized: ['[CLS]', 'great', 'quality', 'doors', 'and', 'great', 'quality', 'people', '!', '[SEP]']\n",
            "Original: ['The', 'door', 'is', 'easy', 'to', 'use', 'and', 'it', 'keeps', 'the', 'cold', 'out', 'during', 'the', 'winter.']\n",
            "Tokenized: ['[CLS]', 'the', 'door', 'is', 'easy', 'to', 'use', 'and', 'it', 'keeps', 'the', 'cold', 'out', 'during', 'the', 'winter', '.', '[SEP]']\n",
            "Original: ['The', 'sales', 'staff', 'and', 'the', 'installation', 'staff', 'were', 'all', 'easy', 'to', 'get', 'along', 'with.']\n",
            "Tokenized: ['[CLS]', 'the', 'sales', 'staff', 'and', 'the', 'installation', 'staff', 'were', 'all', 'easy', 'to', 'get', 'along', 'with', '.', '[SEP]']\n",
            "Original: ['I', 'highly', 'recommend', 'Garage', 'Pros', 'to', 'my', 'friends.']\n",
            "Tokenized: ['[CLS]', 'i', 'highly', 'recommend', 'garage', 'pro', '##s', 'to', 'my', 'friends', '.', '[SEP]']\n",
            "Original: ['This', 'place', 'has', 'done', 'a', 'great', 'job', 'of', 'taking', 'care', 'of', 'the', 'usual', 'maintenance', 'on', 'my', 'hooptie.']\n",
            "Tokenized: ['[CLS]', 'this', 'place', 'has', 'done', 'a', 'great', 'job', 'of', 'taking', 'care', 'of', 'the', 'usual', 'maintenance', 'on', 'my', 'hoop', '##tie', '.', '[SEP]']\n",
            "Original: ['I', 'also', 'never', 'have', 'to', 'wait', 'long', 'for', 'a', 'yearly', 'inspection', 'sticker...and', 'never', 'get', 'the', 'usual', 'excuses', 'other', 'shops', 'always', 'gave', 'me...\"the', 'inspection', 'guy', \"isn't\", 'here', 'today\"....for', 'example.']\n",
            "Tokenized: ['[CLS]', 'i', 'also', 'never', 'have', 'to', 'wait', 'long', 'for', 'a', 'yearly', 'inspection', 'stick', '##er', '.', '.', '.', 'and', 'never', 'get', 'the', 'usual', 'excuses', 'other', 'shops', 'always', 'gave', 'me', '.', '.', '.', '\"', 'the', 'inspection', 'guy', 'isn', \"'\", 't', 'here', 'today', '\"', '.', '.', '.', '.', 'for', 'example', '.', '[SEP]']\n",
            "Original: ['Today', 'I', 'went', 'into', 'Kwik', 'Kar', 'and', 'there', 'were', 'two', 'cars', 'in', 'front', 'of', 'me', 'for', 'inspection...but', 'I', 'was', 'still', 'out', 'of', 'there', 'pretty', 'quick...barely', 'had', 'time', 'to', 'read', 'a', 'chapter', 'in', 'my', 'book.']\n",
            "Tokenized: ['[CLS]', 'today', 'i', 'went', 'into', 'kw', '##ik', 'ka', '##r', 'and', 'there', 'were', 'two', 'cars', 'in', 'front', 'of', 'me', 'for', 'inspection', '.', '.', '.', 'but', 'i', 'was', 'still', 'out', 'of', 'there', 'pretty', 'quick', '.', '.', '.', 'barely', 'had', 'time', 'to', 'read', 'a', 'chapter', 'in', 'my', 'book', '.', '[SEP]']\n",
            "Original: ['I', 'appreciate', 'the', 'quick,', 'good', 'service', 'and', 'the', 'reasonable', 'prices', 'and', 'will', 'definitely', 'use', 'American', 'Pride', 'Irrigation', '&', 'Landscaping', 'again.']\n",
            "Tokenized: ['[CLS]', 'i', 'appreciate', 'the', 'quick', ',', 'good', 'service', 'and', 'the', 'reasonable', 'prices', 'and', 'will', 'definitely', 'use', 'american', 'pride', 'irrigation', '&', 'landscaping', 'again', '.', '[SEP]']\n",
            "Original: ['Watch', 'out', 'for', 'your', 'wallet']\n",
            "Tokenized: ['[CLS]', 'watch', 'out', 'for', 'your', 'wallet', '[SEP]']\n",
            "Original: ['Watch', 'out', 'for', 'your', 'wallet!']\n",
            "Tokenized: ['[CLS]', 'watch', 'out', 'for', 'your', 'wallet', '!', '[SEP]']\n",
            "Original: ['This', 'company', 'is', 'overpriced', 'for', 'their', 'services.']\n",
            "Tokenized: ['[CLS]', 'this', 'company', 'is', 'over', '##pr', '##ice', '##d', 'for', 'their', 'services', '.', '[SEP]']\n",
            "Original: ['They', 'pride', 'themselves', 'on', 'being', 'an', 'event', 'and', 'team', 'building', 'company', 'for', 'corporate', 'clients', 'but', 'you', 'better', 'believe', 'they', 'are', 'going', 'to', 'mark', 'you', 'up', 'on', 'that', 'feel', 'good', 'premise.']\n",
            "Tokenized: ['[CLS]', 'they', 'pride', 'themselves', 'on', 'being', 'an', 'event', 'and', 'team', 'building', 'company', 'for', 'corporate', 'clients', 'but', 'you', 'better', 'believe', 'they', 'are', 'going', 'to', 'mark', 'you', 'up', 'on', 'that', 'feel', 'good', 'premise', '.', '[SEP]']\n",
            "Original: ['On', 'a', 'recent', 'event', 'quote', 'that', 'I', 'had', 'done,', 'they', 'came', 'in', 'thousands', 'of', 'dollars', 'over', 'their', 'local', 'competition.']\n",
            "Tokenized: ['[CLS]', 'on', 'a', 'recent', 'event', 'quote', 'that', 'i', 'had', 'done', ',', 'they', 'came', 'in', 'thousands', 'of', 'dollars', 'over', 'their', 'local', 'competition', '.', '[SEP]']\n",
            "Original: ['The', 'craziest', 'part', 'is', 'that', 'they', \"aren't\", 'even', 'based', 'locally', 'at', 'the', 'city', \"I'm\", 'in-', 'they', 'just', 'have', \"'teams'\", 'in', 'areas', 'through', 'the', 'country.']\n",
            "Tokenized: ['[CLS]', 'the', 'cr', '##azi', '##est', 'part', 'is', 'that', 'they', 'aren', \"'\", 't', 'even', 'based', 'locally', 'at', 'the', 'city', 'i', \"'\", 'm', 'in', '-', 'they', 'just', 'have', \"'\", 'teams', \"'\", 'in', 'areas', 'through', 'the', 'country', '.', '[SEP]']\n",
            "Original: [\"I've\", 'spoken', 'to', 'vendors', 'who', 'are', 'used', 'by', 'them', 'who', 'had', 'nothing', 'good', 'to', 'say', 'about', 'the', 'company', 'as', 'well', 'and', 'in', 'fact', 'were', 'afraid', 'to', 'quote', 'events', 'against', 'them', 'even', 'though', 'they', 'openly', 'admitted', 'that', 'they', 'felt', 'Canadian', 'Outback', 'was', 'pricing', 'their', 'entertainment', 'out', 'of', 'the', 'market', 'and', 'doing', 'more', 'to', 'hurt', 'their', 'business', 'than', 'help', 'them.']\n",
            "Tokenized: ['[CLS]', 'i', \"'\", 've', 'spoken', 'to', 'vendors', 'who', 'are', 'used', 'by', 'them', 'who', 'had', 'nothing', 'good', 'to', 'say', 'about', 'the', 'company', 'as', 'well', 'and', 'in', 'fact', 'were', 'afraid', 'to', 'quote', 'events', 'against', 'them', 'even', 'though', 'they', 'openly', 'admitted', 'that', 'they', 'felt', 'canadian', 'out', '##back', 'was', 'pricing', 'their', 'entertainment', 'out', 'of', 'the', 'market', 'and', 'doing', 'more', 'to', 'hurt', 'their', 'business', 'than', 'help', 'them', '.', '[SEP]']\n",
            "Original: ['Be', 'warned-', \"they'll\", 'ltake', 'you', 'for', 'everything', 'they', 'can.']\n",
            "Tokenized: ['[CLS]', 'be', 'warned', '-', 'they', \"'\", 'll', 'lt', '##ake', 'you', 'for', 'everything', 'they', 'can', '.', '[SEP]']\n",
            "Original: ['Extremely', 'greasy.']\n",
            "Tokenized: ['[CLS]', 'extremely', 'greasy', '.', '[SEP]']\n",
            "Original: ['Hit', 'or', 'miss', 'on', 'the', 'service.']\n",
            "Tokenized: ['[CLS]', 'hit', 'or', 'miss', 'on', 'the', 'service', '.', '[SEP]']\n",
            "Original: ['My', 'fries', \"weren't\", 'fully', 'cooked', 'last', 'time', 'I', 'went', 'there.']\n",
            "Tokenized: ['[CLS]', 'my', 'fries', 'weren', \"'\", 't', 'fully', 'cooked', 'last', 'time', 'i', 'went', 'there', '.', '[SEP]']\n",
            "Original: ['Pretty', 'spendy', 'for', 'really', 'not', 'great', 'quality']\n",
            "Tokenized: ['[CLS]', 'pretty', 'spend', '##y', 'for', 'really', 'not', 'great', 'quality', '[SEP]']\n",
            "Original: ['this', 'is', 'a', 'good', 'place']\n",
            "Tokenized: ['[CLS]', 'this', 'is', 'a', 'good', 'place', '[SEP]']\n",
            "Original: ['I', 'have', 'been', 'here', 'before', 'and', 'the', 'service', 'was', 'absoulutely', 'great.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'been', 'here', 'before', 'and', 'the', 'service', 'was', 'abs', '##ou', '##lu', '##tel', '##y', 'great', '.', '[SEP]']\n",
            "Original: ['They', 'had', 'a', 'great', 'selection', 'of', 'colors', 'to', 'choose', 'from', 'and', 'their', 'seats', 'are', 'super', 'comfty.']\n",
            "Tokenized: ['[CLS]', 'they', 'had', 'a', 'great', 'selection', 'of', 'colors', 'to', 'choose', 'from', 'and', 'their', 'seats', 'are', 'super', 'com', '##ft', '##y', '.', '[SEP]']\n",
            "Original: ['I', 'enjoy', 'going', 'there', 'although', \"i've\", 'only', 'been', 'there', 'once,', 'i', 'will', 'be', 'returning', 'toda', 'to', 'recieve', 'a', 'pair', 'of', 'french', 'tips', 'and', 'i', 'will', 'only', 'go', 'to', 'the', 'best', 'and', 'to', 'me', 'the', 'best', 'is', 'here.']\n",
            "Tokenized: ['[CLS]', 'i', 'enjoy', 'going', 'there', 'although', 'i', \"'\", 've', 'only', 'been', 'there', 'once', ',', 'i', 'will', 'be', 'returning', 'tod', '##a', 'to', 'rec', '##ie', '##ve', 'a', 'pair', 'of', 'french', 'tips', 'and', 'i', 'will', 'only', 'go', 'to', 'the', 'best', 'and', 'to', 'me', 'the', 'best', 'is', 'here', '.', '[SEP]']\n",
            "Original: ['i', 'reccomend', 'you', 'to', 'go', 'and', 'enjoy', 'their', 'wonderful', 'hospitality.']\n",
            "Tokenized: ['[CLS]', 'i', 'rec', '##com', '##end', 'you', 'to', 'go', 'and', 'enjoy', 'their', 'wonderful', 'hospitality', '.', '[SEP]']\n",
            "Original: ['WONDERFUL', 'service.']\n",
            "Tokenized: ['[CLS]', 'wonderful', 'service', '.', '[SEP]']\n",
            "Original: ['I', 'have', 'used', 'them', 'once', 'and', 'will', 'use', 'them', 'in', 'the', 'future.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'used', 'them', 'once', 'and', 'will', 'use', 'them', 'in', 'the', 'future', '.', '[SEP]']\n",
            "Original: ['Beautiful', 'work,', 'fast', 'shipping', 'and', 'great', 'communication.']\n",
            "Tokenized: ['[CLS]', 'beautiful', 'work', ',', 'fast', 'shipping', 'and', 'great', 'communication', '.', '[SEP]']\n",
            "Original: ['Tire', 'Gooroo']\n",
            "Tokenized: ['[CLS]', 'tire', 'goo', '##ro', '##o', '[SEP]']\n",
            "Original: ['David', 'Bundren', 'is', 'the', 'Tire', 'GooRoo.']\n",
            "Tokenized: ['[CLS]', 'david', 'bun', '##dre', '##n', 'is', 'the', 'tire', 'goo', '##ro', '##o', '.', '[SEP]']\n",
            "Original: [\"Don't\", 'bother.']\n",
            "Tokenized: ['[CLS]', 'don', \"'\", 't', 'bother', '.', '[SEP]']\n",
            "Original: [\"It's\", 'impossible', 'to', 'understand', 'how', 'this', 'place', 'has', 'survived.']\n",
            "Tokenized: ['[CLS]', 'it', \"'\", 's', 'impossible', 'to', 'understand', 'how', 'this', 'place', 'has', 'survived', '.', '[SEP]']\n",
            "Original: ['Worst', 'experience', 'ever', 'like', 'a', 'sardine', 'can', 'and', 'the', 'bartender', 'downstairs', 'is', 'the', 'rudest', 'person', 'I', 'have', 'ever', 'met.']\n",
            "Tokenized: ['[CLS]', 'worst', 'experience', 'ever', 'like', 'a', 'sar', '##dine', 'can', 'and', 'the', 'bartender', 'downstairs', 'is', 'the', 'rude', '##st', 'person', 'i', 'have', 'ever', 'met', '.', '[SEP]']\n",
            "Original: ['DONt', 'Go', 'here']\n",
            "Tokenized: ['[CLS]', 'don', '##t', 'go', 'here', '[SEP]']\n",
            "Original: ['Restored', 'my', 'faith', 'in', 'Mechaincs.']\n",
            "Tokenized: ['[CLS]', 'restored', 'my', 'faith', 'in', 'me', '##chai', '##nc', '##s', '.', '[SEP]']\n",
            "Original: ['I', 'spent', '3', 'months', 'going', 'from', 'shop', 'to', 'shop', 'trying', 'to', 'get', 'my', 'Ferrari', 'to', 'run', 'and', 'drive', 'the', 'way', 'it', 'should.']\n",
            "Tokenized: ['[CLS]', 'i', 'spent', '3', 'months', 'going', 'from', 'shop', 'to', 'shop', 'trying', 'to', 'get', 'my', 'ferrari', 'to', 'run', 'and', 'drive', 'the', 'way', 'it', 'should', '.', '[SEP]']\n",
            "Original: ['I', 'was', 'about', 'to', 'give', 'up', 'when', 'I', 'met', 'Jason', 'and', 'Neal.']\n",
            "Tokenized: ['[CLS]', 'i', 'was', 'about', 'to', 'give', 'up', 'when', 'i', 'met', 'jason', 'and', 'neal', '.', '[SEP]']\n",
            "Original: ['They', 'took', 'on', 'the', 'challenge', 'of', 'making', 'my', 'Ferrari', 'all', 'I', 'dreamed', 'of', 'and', 'more.']\n",
            "Tokenized: ['[CLS]', 'they', 'took', 'on', 'the', 'challenge', 'of', 'making', 'my', 'ferrari', 'all', 'i', 'dreamed', 'of', 'and', 'more', '.', '[SEP]']\n",
            "Original: ['The', 'crew', 'at', 'The', 'Creative', 'Workshop', 'went', 'over', 'and', 'above', 'the', 'call', 'of', 'duty', 'and', 'gave', 'me', 'back', 'a', 'car', 'I', 'can', 'drive', 'anywhere', 'and', 'finally', 'enjoy', 'owning.']\n",
            "Tokenized: ['[CLS]', 'the', 'crew', 'at', 'the', 'creative', 'workshop', 'went', 'over', 'and', 'above', 'the', 'call', 'of', 'duty', 'and', 'gave', 'me', 'back', 'a', 'car', 'i', 'can', 'drive', 'anywhere', 'and', 'finally', 'enjoy', 'owning', '.', '[SEP]']\n",
            "Original: ['I', 'cannot', 'say', 'enough', 'about', 'this', 'place.']\n",
            "Tokenized: ['[CLS]', 'i', 'cannot', 'say', 'enough', 'about', 'this', 'place', '.', '[SEP]']\n",
            "Original: ['They', 'have', 'restored', 'my', 'faith', 'in', 'Mechanics.']\n",
            "Tokenized: ['[CLS]', 'they', 'have', 'restored', 'my', 'faith', 'in', 'mechanics', '.', '[SEP]']\n",
            "Original: ['Do', 'yourself', 'a', 'favor,', 'call', 'these', 'guys', 'first', 'and', 'enjoy', 'driving', 'your', 'car', 'again..']\n",
            "Tokenized: ['[CLS]', 'do', 'yourself', 'a', 'favor', ',', 'call', 'these', 'guys', 'first', 'and', 'enjoy', 'driving', 'your', 'car', 'again', '.', '.', '[SEP]']\n",
            "Original: ['I', 'guess', 'you', 'get', 'what', 'you', 'pay', 'for.']\n",
            "Tokenized: ['[CLS]', 'i', 'guess', 'you', 'get', 'what', 'you', 'pay', 'for', '.', '[SEP]']\n",
            "Original: ['I', 'tried', 'to', 'stay', 'here', 'for', 'a', 'few', 'nights', 'with', 'my', 'girlfriend,', 'so', 'we', 'asked', 'for', 'a', 'single', 'queen', 'bed.']\n",
            "Tokenized: ['[CLS]', 'i', 'tried', 'to', 'stay', 'here', 'for', 'a', 'few', 'nights', 'with', 'my', 'girlfriend', ',', 'so', 'we', 'asked', 'for', 'a', 'single', 'queen', 'bed', '.', '[SEP]']\n",
            "Original: ['We', 'got', 'there,', 'and', 'we', 'were', 'treated', 'to', 'the', 'free', '\"upgrade\"', 'of', 'a', 'room', 'with', 'two', 'double', 'beds.']\n",
            "Tokenized: ['[CLS]', 'we', 'got', 'there', ',', 'and', 'we', 'were', 'treated', 'to', 'the', 'free', '\"', 'upgrade', '\"', 'of', 'a', 'room', 'with', 'two', 'double', 'beds', '.', '[SEP]']\n",
            "Original: ['Since', 'they', 'consider', 'this', 'an', 'upgrade,', 'they', 'let', 'their', 'other', 'rooms', 'fill', 'up', 'and', 'would', 'not', 'change', 'our', 'room.']\n",
            "Tokenized: ['[CLS]', 'since', 'they', 'consider', 'this', 'an', 'upgrade', ',', 'they', 'let', 'their', 'other', 'rooms', 'fill', 'up', 'and', 'would', 'not', 'change', 'our', 'room', '.', '[SEP]']\n",
            "Original: ['Then', 'we', 'got', 'put', 'in', 'a', 'room', 'with', 'a', 'huge', 'gap', 'under', 'the', 'door....right', 'next', 'to', 'the', 'ice', 'machine.']\n",
            "Tokenized: ['[CLS]', 'then', 'we', 'got', 'put', 'in', 'a', 'room', 'with', 'a', 'huge', 'gap', 'under', 'the', 'door', '.', '.', '.', '.', 'right', 'next', 'to', 'the', 'ice', 'machine', '.', '[SEP]']\n",
            "Original: ['We', 'could', 'hear', 'every', 'single', 'thing', 'that', 'happened', 'outside', 'like', 'it', 'was', 'inside', 'our', 'room.']\n",
            "Tokenized: ['[CLS]', 'we', 'could', 'hear', 'every', 'single', 'thing', 'that', 'happened', 'outside', 'like', 'it', 'was', 'inside', 'our', 'room', '.', '[SEP]']\n",
            "Original: ['When', 'I', 'finally', 'found', 'someone', 'at', 'the', 'desk', 'who', 'could', 'speak', 'English,', 'they', 'moved', 'our', 'room,', 'but', 'we', 'still', 'did', 'not', 'receive', 'the', 'single', 'queen', 'we', 'had', '\"reserved\"']\n",
            "Tokenized: ['[CLS]', 'when', 'i', 'finally', 'found', 'someone', 'at', 'the', 'desk', 'who', 'could', 'speak', 'english', ',', 'they', 'moved', 'our', 'room', ',', 'but', 'we', 'still', 'did', 'not', 'receive', 'the', 'single', 'queen', 'we', 'had', '\"', 'reserved', '\"', '[SEP]']\n",
            "Original: ['Great', 'pet', 'care']\n",
            "Tokenized: ['[CLS]', 'great', 'pet', 'care', '[SEP]']\n",
            "Original: ['I', 'have', 'used', 'Just', 'Like', 'Family', 'several', 'times', 'now', 'and', 'they', 'have', 'provided', 'great', 'care', 'for', 'my', 'two', 'dogs.']\n",
            "Tokenized: ['[CLS]', 'i', 'have', 'used', 'just', 'like', 'family', 'several', 'times', 'now', 'and', 'they', 'have', 'provided', 'great', 'care', 'for', 'my', 'two', 'dogs', '.', '[SEP]']\n",
            "Original: ['Lynda', 'is', 'professional', 'and', 'has', 'great', 'compassion', 'for', 'animals.']\n",
            "Tokenized: ['[CLS]', 'l', '##yn', '##da', 'is', 'professional', 'and', 'has', 'great', 'compassion', 'for', 'animals', '.', '[SEP]']\n",
            "Original: ['The', 'real', 'testament', 'is', 'not', 'in', 'how', 'much', 'she', 'likes', 'your', 'animals', 'but', 'how', 'much', 'they', 'like', 'her.']\n",
            "Tokenized: ['[CLS]', 'the', 'real', 'testament', 'is', 'not', 'in', 'how', 'much', 'she', 'likes', 'your', 'animals', 'but', 'how', 'much', 'they', 'like', 'her', '.', '[SEP]']\n",
            "Original: ['I', 'highly', 'recommend', 'her.']\n",
            "Tokenized: ['[CLS]', 'i', 'highly', 'recommend', 'her', '.', '[SEP]']\n",
            "Original: ['Con', 'Garage']\n",
            "Tokenized: ['[CLS]', 'con', 'garage', '[SEP]']\n",
            "Original: ['I', 'brought', 'my', 'car', 'in', 'for', 'a', 'simple', 'emissions', 'test.']\n",
            "Tokenized: ['[CLS]', 'i', 'brought', 'my', 'car', 'in', 'for', 'a', 'simple', 'emissions', 'test', '.', '[SEP]']\n",
            "Original: ['I', 'guess', 'they', 'figured', 'me', 'for', 'an', 'easy', 'mark,', 'and', 'tried', 'to', 'explain', 'that', 'my', 'car', \"wouldn't\", 'pass', 'unless', 'I', 'replaced', 'a', 'hose.']\n",
            "Tokenized: ['[CLS]', 'i', 'guess', 'they', 'figured', 'me', 'for', 'an', 'easy', 'mark', ',', 'and', 'tried', 'to', 'explain', 'that', 'my', 'car', 'wouldn', \"'\", 't', 'pass', 'unless', 'i', 'replaced', 'a', 'hose', '.', '[SEP]']\n",
            "Original: ['Ten', 'minutes', 'later,', 'I', 'took', 'my', 'car', 'down', 'the', 'street', 'and', 'it', 'passed', 'the', 'emissions', 'test', 'with', 'flying', 'colors.']\n",
            "Tokenized: ['[CLS]', 'ten', 'minutes', 'later', ',', 'i', 'took', 'my', 'car', 'down', 'the', 'street', 'and', 'it', 'passed', 'the', 'emissions', 'test', 'with', 'flying', 'colors', '.', '[SEP]']\n",
            "Original: ['If', \"you're\", 'a', 'fan', 'of', 'herpes,', 'being', 'ripped', 'off,', 'and', 'child', 'molesters,', 'this', 'is', 'the', 'garage', 'for', 'you.']\n",
            "Tokenized: ['[CLS]', 'if', 'you', \"'\", 're', 'a', 'fan', 'of', 'her', '##pes', ',', 'being', 'ripped', 'off', ',', 'and', 'child', 'mole', '##sters', ',', 'this', 'is', 'the', 'garage', 'for', 'you', '.', '[SEP]']\n",
            "Original: ['If', 'not,', 'go', 'somewhere', 'else.']\n",
            "Tokenized: ['[CLS]', 'if', 'not', ',', 'go', 'somewhere', 'else', '.', '[SEP]']\n",
            "Original: ['Great', 'work', 'and', 'honest', 'establishment!']\n",
            "Tokenized: ['[CLS]', 'great', 'work', 'and', 'honest', 'establishment', '!', '[SEP]']\n",
            "Original: ['I', 'typically', 'have', 'work', 'done', 'on', 'my', 'Jeep', 'at', 'the', 'dealership,', 'but', 'it', 'is', '6', 'years', 'old', 'now', 'and', 'getting', 'charged', 'dealership', 'prices', 'just', \"didn't\", 'seem', 'cost', 'effective', 'anymore.']\n",
            "Tokenized: ['[CLS]', 'i', 'typically', 'have', 'work', 'done', 'on', 'my', 'jeep', 'at', 'the', 'dealers', '##hip', ',', 'but', 'it', 'is', '6', 'years', 'old', 'now', 'and', 'getting', 'charged', 'dealers', '##hip', 'prices', 'just', 'didn', \"'\", 't', 'seem', 'cost', 'effective', 'anymore', '.', '[SEP]']\n",
            "Original: ['I', 'had', 'tried', 'out', 'few', 'place', 'around', 'the', 'area', 'and', 'had', 'been', 'ripped', 'off', 'a', 'few', 'times.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'tried', 'out', 'few', 'place', 'around', 'the', 'area', 'and', 'had', 'been', 'ripped', 'off', 'a', 'few', 'times', '.', '[SEP]']\n",
            "Original: ['I', 'had', 'hear', 'great', 'things', 'about', 'Phet', 'and', 'G&G', 'Automotive', 'so', 'I', 'decided', 'to', 'give', 'him', 'a', 'try.']\n",
            "Tokenized: ['[CLS]', 'i', 'had', 'hear', 'great', 'things', 'about', 'ph', '##et', 'and', 'g', '&', 'g', 'automotive', 'so', 'i', 'decided', 'to', 'give', 'him', 'a', 'try', '.', '[SEP]']\n",
            "Original: ['The', 'service', 'was', 'excellent', 'and', 'personable.']\n",
            "Tokenized: ['[CLS]', 'the', 'service', 'was', 'excellent', 'and', 'persona', '##ble', '.', '[SEP]']\n",
            "Original: ['He', 'checked', 'out', 'what', 'I', 'needed', 'to', 'have', 'done', 'told', 'me', 'what', 'needed', 'be', 'fixed', 'before', 'he', 'did', 'any', 'work', 'and', 'did', 'great', 'repair', 'work.']\n",
            "Tokenized: ['[CLS]', 'he', 'checked', 'out', 'what', 'i', 'needed', 'to', 'have', 'done', 'told', 'me', 'what', 'needed', 'be', 'fixed', 'before', 'he', 'did', 'any', 'work', 'and', 'did', 'great', 'repair', 'work', '.', '[SEP]']\n",
            "Original: ['The', 'price', 'was', 'actually', 'lower', 'than', 'what', 'I', 'had', 'anticipated', 'and', 'used', 'to', 'compared', 'to', 'other', 'places,', 'plus', 'he', 'showed', 'me', 'the', 'work', 'he', 'did', 'when', 'I', 'came', 'into', 'pick', 'up', 'the', 'car.']\n",
            "Tokenized: ['[CLS]', 'the', 'price', 'was', 'actually', 'lower', 'than', 'what', 'i', 'had', 'anticipated', 'and', 'used', 'to', 'compared', 'to', 'other', 'places', ',', 'plus', 'he', 'showed', 'me', 'the', 'work', 'he', 'did', 'when', 'i', 'came', 'into', 'pick', 'up', 'the', 'car', '.', '[SEP]']\n",
            "Original: ['Also,', 'a', 'week', 'after', 'the', 'work,', 'Phet', 'called', 'me', 'up', 'to', 'see', 'how', 'my', 'car', 'was', 'running', 'and', 'to', 'let', 'me', 'know', 'that', 'they', 'had', 'accidentally', 'overcharged', 'me', 'for', 'part', 'of', 'the', 'work', 'and', 'wanted', 'to', 'give', 'me', 'a', 'refund', 'for', 'that', 'amount.']\n",
            "Tokenized: ['[CLS]', 'also', ',', 'a', 'week', 'after', 'the', 'work', ',', 'ph', '##et', 'called', 'me', 'up', 'to', 'see', 'how', 'my', 'car', 'was', 'running', 'and', 'to', 'let', 'me', 'know', 'that', 'they', 'had', 'accidentally', 'over', '##charged', 'me', 'for', 'part', 'of', 'the', 'work', 'and', 'wanted', 'to', 'give', 'me', 'a', 'ref', '##und', 'for', 'that', 'amount', '.', '[SEP]']\n",
            "Original: ['That', 'is', 'just', 'unheard', 'of', 'these', 'days!']\n",
            "Tokenized: ['[CLS]', 'that', 'is', 'just', 'un', '##heard', 'of', 'these', 'days', '!', '[SEP]']\n",
            "Original: ['I', 'grew', 'up', 'in', 'a', 'small', 'town', 'where', 'you', 'knew', 'and', 'trusted', 'your', 'mechanic', 'and', 'was', 'really', 'cynical', 'about', 'city', 'auto', 'repair', 'shops', 'since', 'I', 'moved', 'here,', 'but', 'Phet', 'has', 'shown', 'that', 'there', 'really', 'are', 'honest', 'hard', 'working', 'mechanics', 'around.']\n",
            "Tokenized: ['[CLS]', 'i', 'grew', 'up', 'in', 'a', 'small', 'town', 'where', 'you', 'knew', 'and', 'trusted', 'your', 'mechanic', 'and', 'was', 'really', 'cynical', 'about', 'city', 'auto', 'repair', 'shops', 'since', 'i', 'moved', 'here', ',', 'but', 'ph', '##et', 'has', 'shown', 'that', 'there', 'really', 'are', 'honest', 'hard', 'working', 'mechanics', 'around', '.', '[SEP]']\n",
            "Original: ['He', 'is', 'my', 'mechanic', 'going', 'forward!']\n",
            "Tokenized: ['[CLS]', 'he', 'is', 'my', 'mechanic', 'going', 'forward', '!', '[SEP]']\n",
            "Original: ['Friendly,', 'clean', 'and', 'excellent', 'location']\n",
            "Tokenized: ['[CLS]', 'friendly', ',', 'clean', 'and', 'excellent', 'location', '[SEP]']\n",
            "Original: ['The', 'staff', 'was', 'very', 'helpful,', 'and', 'gave', 'us', 'good', 'advice', 'on', 'day', 'and', 'night', 'time', 'activities.']\n",
            "Tokenized: ['[CLS]', 'the', 'staff', 'was', 'very', 'helpful', ',', 'and', 'gave', 'us', 'good', 'advice', 'on', 'day', 'and', 'night', 'time', 'activities', '.', '[SEP]']\n",
            "Original: ['Common', 'room', 'was', 'comfortable', 'and', 'clean,', 'very', 'good', 'room', 'to', 'read', 'or', 'relax.', '–']\n",
            "Tokenized: ['[CLS]', 'common', 'room', 'was', 'comfortable', 'and', 'clean', ',', 'very', 'good', 'room', 'to', 'read', 'or', 'relax', '.', '–', '[SEP]']\n",
            "Original: ['A', 'great', 'breakfast', 'which', 'was', 'included', 'every', 'morning', 'until', '9:30am;', 'yummy', 'fresh', 'Parisian', 'croissants.']\n",
            "Tokenized: ['[CLS]', 'a', 'great', 'breakfast', 'which', 'was', 'included', 'every', 'morning', 'until', '9', ':', '30', '##am', ';', 'yu', '##mmy', 'fresh', 'parisian', 'cr', '##ois', '##sant', '##s', '.', '[SEP]']\n",
            "Original: ['Comfortable', 'and', 'clean', 'beds,', 'a', 'bit', 'noisy', 'when', 'people', 'were', 'coming', 'in', 'late', 'from', 'a', 'night', 'out,', 'but', 'we', \"didn't\", 'mind', 'too', 'much', 'as', 'we', 'were', 'also', 'just', 'coming', 'in', 'from', 'a', 'night', 'out!']\n",
            "Tokenized: ['[CLS]', 'comfortable', 'and', 'clean', 'beds', ',', 'a', 'bit', 'noisy', 'when', 'people', 'were', 'coming', 'in', 'late', 'from', 'a', 'night', 'out', ',', 'but', 'we', 'didn', \"'\", 't', 'mind', 'too', 'much', 'as', 'we', 'were', 'also', 'just', 'coming', 'in', 'from', 'a', 'night', 'out', '!', '[SEP]']\n",
            "Original: ['The', 'location', 'is', 'really', 'stellar!']\n",
            "Tokenized: ['[CLS]', 'the', 'location', 'is', 'really', 'stellar', '!', '[SEP]']\n",
            "Original: ['It', 'is', 'next', 'to', 'Gare', 'du', 'Nord', 'and', 'a', 'five', 'minute', 'walk', 'to', 'Sacre', 'Coeur', 'which', 'is', 'excellent', 'for', 'shopping.']\n",
            "Tokenized: ['[CLS]', 'it', 'is', 'next', 'to', 'ga', '##re', 'du', 'nord', 'and', 'a', 'five', 'minute', 'walk', 'to', 'sac', '##re', 'coe', '##ur', 'which', 'is', 'excellent', 'for', 'shopping', '.', '[SEP]']\n",
            "Original: ['It', 'is', 'close', 'to', 'bus', 'lines', 'for', 'Opera', 'Plaza,', 'Galleries', 'Lafayette', ',', 'and', 'the', 'famous', 'flea', 'Market.']\n",
            "Tokenized: ['[CLS]', 'it', 'is', 'close', 'to', 'bus', 'lines', 'for', 'opera', 'plaza', ',', 'galleries', 'lafayette', ',', 'and', 'the', 'famous', 'flea', 'market', '.', '[SEP]']\n",
            "Original: ['We', 'really', 'enjoyed', 'our', 'stay', 'and', 'would', 'definitely', 'stay', 'at', 'the', 'Vintage', 'Hostel', 'again.']\n",
            "Tokenized: ['[CLS]', 'we', 'really', 'enjoyed', 'our', 'stay', 'and', 'would', 'definitely', 'stay', 'at', 'the', 'vintage', 'hostel', 'again', '.', '[SEP]']\n",
            "UPOS: ADV\n",
            "  pos: Expected Layer 5.89\n",
            "  posm: Expected Layer 5.85\n",
            "  word: Expected Layer 7.29\n",
            "  wordm: Expected Layer 8.31\n",
            "UPOS: VERB\n",
            "  pos: Expected Layer 6.27\n",
            "  posm: Expected Layer 6.36\n",
            "  word: Expected Layer 8.42\n",
            "  wordm: Expected Layer 8.65\n",
            "UPOS: PRON\n",
            "  pos: Expected Layer 6.49\n",
            "  posm: Expected Layer 6.35\n",
            "  word: Expected Layer 7.82\n",
            "  wordm: Expected Layer 8.41\n",
            "UPOS: NUM\n",
            "  pos: Expected Layer 7.21\n",
            "  posm: Expected Layer 8.67\n",
            "  word: Expected Layer 7.66\n",
            "  wordm: Expected Layer 9.10\n",
            "UPOS: NOUN\n",
            "  pos: Expected Layer 7.25\n",
            "  posm: Expected Layer 7.21\n",
            "  word: Expected Layer 9.11\n",
            "  wordm: Expected Layer 9.01\n",
            "UPOS: ADJ\n",
            "  pos: Expected Layer 7.80\n",
            "  posm: Expected Layer 7.89\n",
            "  word: Expected Layer 8.70\n",
            "  wordm: Expected Layer 8.94\n",
            "UPOS: DET\n",
            "  pos: Expected Layer 9.25\n",
            "  posm: Expected Layer 0.00\n",
            "  word: Expected Layer 6.59\n",
            "  wordm: Expected Layer 6.60\n",
            "UPOS: PUNCT\n",
            "  pos: Expected Layer 5.56\n",
            "  posm: Expected Layer 3.79\n",
            "  word: Expected Layer 5.90\n",
            "  wordm: Expected Layer 7.57\n",
            "UPOS: INTJ\n",
            "  pos: Expected Layer 7.60\n",
            "  posm: Expected Layer 0.00\n",
            "  word: Expected Layer 8.18\n",
            "  wordm: Expected Layer 10.25\n",
            "UPOS: ADP\n",
            "  pos: Expected Layer 6.07\n",
            "  posm: Expected Layer 6.01\n",
            "  word: Expected Layer 7.38\n",
            "  wordm: Expected Layer 7.84\n",
            "UPOS: PROPN\n",
            "  pos: Expected Layer 6.73\n",
            "  posm: Expected Layer 6.85\n",
            "  word: Expected Layer 8.63\n",
            "  wordm: Expected Layer 8.59\n",
            "UPOS: CCONJ\n",
            "  pos: Expected Layer 5.56\n",
            "  posm: Expected Layer 5.49\n",
            "  word: Expected Layer 5.51\n",
            "  wordm: Expected Layer 5.55\n",
            "UPOS: SCONJ\n",
            "  pos: Expected Layer 7.05\n",
            "  posm: Expected Layer 7.09\n",
            "  word: Expected Layer 7.79\n",
            "  wordm: Expected Layer 5.83\n",
            "UPOS: AUX\n",
            "  pos: Expected Layer 6.79\n",
            "  posm: Expected Layer 6.76\n",
            "  word: Expected Layer 7.62\n",
            "  wordm: Expected Layer 7.60\n",
            "UPOS: SYM\n",
            "  pos: Expected Layer 7.78\n",
            "  posm: Expected Layer 7.75\n",
            "  word: Expected Layer 7.86\n",
            "  wordm: Expected Layer 8.12\n",
            "UPOS: PART\n",
            "  pos: Expected Layer 6.09\n",
            "  posm: Expected Layer 5.77\n",
            "  word: Expected Layer 6.15\n",
            "  wordm: Expected Layer 5.70\n",
            "UPOS: X\n",
            "  pos: Expected Layer 7.18\n",
            "  posm: Expected Layer 5.67\n",
            "  word: Expected Layer 8.69\n",
            "  wordm: Expected Layer 0.00\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import json\n",
        "import spacy\n",
        "spacy.require_gpu()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForMaskedLM.from_pretrained(model_name, output_hidden_states=True)\n",
        "model.eval()\n",
        "\n",
        "# Check if GPU is available and set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "model.to(device)\n",
        "\n",
        "# Load data\n",
        "file_path = 'streusle-4.4/streusle.json'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "def get_pos(word):\n",
        "    doc = nlp(word)\n",
        "    return doc[0].pos_ if doc else \"UNK\"\n",
        "\n",
        "def process_data(data):\n",
        "    results = defaultdict(lambda: defaultdict(lambda: [0, 0, 0, 0, 0]))  # pos, posm, word, wordm\n",
        "\n",
        "    for sentence in data:\n",
        "        tokens = sentence[\"text\"].split()\n",
        "        encoded_tokens = tokenizer(tokens, is_split_into_words=True, add_special_tokens=True)\n",
        "        decoded_tokens = [tokenizer.decode([tid]) for tid in encoded_tokens['input_ids']]\n",
        "        print(\"Original:\", tokens)\n",
        "        print(\"Tokenized:\", decoded_tokens)\n",
        "        token_ids = torch.tensor([encoded_tokens['input_ids']]).to(device)  # Ensure tokens are on the correct device\n",
        "\n",
        "        mwe_indices = {num for idx in sentence.get(\"smwes\", {}).values() for num in idx.get(\"toknums\", [])}\n",
        "\n",
        "        # Adjusting indices for [CLS] which is at the start\n",
        "        offset = 1  # Account for [CLS] at the beginning\n",
        "        for idx, token_info in enumerate(sentence[\"toks\"]):\n",
        "            original_word = token_info[\"word\"]\n",
        "            upos = token_info[\"upos\"]\n",
        "            is_mwe = idx + 1 in mwe_indices\n",
        "\n",
        "            # Adjust index for [CLS] and ensure it's within bounds\n",
        "            masked_index = idx + offset\n",
        "            if masked_index < len(token_ids[0]):\n",
        "                token_ids[0, masked_index] = tokenizer.mask_token_id\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(input_ids=token_ids, output_hidden_states=True)\n",
        "\n",
        "                # Only process if masked index is valid\n",
        "                for layer_index, layer_outputs in enumerate(outputs.hidden_states):\n",
        "                    logits = model.cls(layer_outputs)\n",
        "                    prediction_scores = torch.softmax(logits, dim=-1)\n",
        "                    predicted_token_id = prediction_scores[0, masked_index].argmax().item()\n",
        "                    predicted_word = tokenizer.decode([predicted_token_id]).strip()\n",
        "                    predicted_pos = get_pos(predicted_word)  # Using spaCy for POS tagging\n",
        "\n",
        "                    correct_word = predicted_word.lower() == original_word.lower()\n",
        "                    correct_upos = predicted_pos == upos\n",
        "                    # print(\"predicted word:\"+ predicted_word.lower() + \"original word:\"+ original_word.lower())\n",
        "                    # print(\"predicted pos:\"+ predicted_pos + \"original_pos:\" + upos)\n",
        "                    # Update counts\n",
        "                    if correct_word:\n",
        "                        results[upos][layer_index][2] += 1  # word match count\n",
        "                        if is_mwe:\n",
        "                            results[upos][layer_index][3] += 1  # wordM match count\n",
        "                    if correct_upos:\n",
        "                        results[upos][layer_index][0] += 1  # pos match count\n",
        "                        if is_mwe:\n",
        "                            results[upos][layer_index][1] += 1  # posM match count\n",
        "                    results[upos][layer_index][4] += 1  # Total count\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "results = process_data(data)\n",
        "\n",
        "# Calculate expected layers\n",
        "expected_layers = defaultdict(dict)\n",
        "for upos, layers in results.items():\n",
        "    for measure_idx, measure in enumerate(['pos', 'posm', 'word', 'wordm']):\n",
        "        accuracies = [layers[layer][measure_idx] / layers[layer][4] if layers[layer][4] > 0 else 0 for layer in range(12)]\n",
        "        expected_layer = np.dot(range(12), accuracies) / sum(accuracies) if sum(accuracies) > 0 else 0\n",
        "        expected_layers[upos][measure] = expected_layer\n",
        "\n",
        "# Print results\n",
        "for upos, measures in expected_layers.items():\n",
        "    print(f\"UPOS: {upos}\")\n",
        "    for measure, layer in measures.items():\n",
        "        print(f\"  {measure}: Expected Layer {layer:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbfwNFQ5ro-d",
        "outputId": "087539e6-0e85-4682-c342-c5e287db32cf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(<function __main__.process_data.<locals>.<lambda>()>,\n",
              "            {'ADV': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [537, 77, 27, 5, 3834],\n",
              "                          1: [377, 48, 20, 4, 3834],\n",
              "                          2: [794, 90, 63, 4, 3834],\n",
              "                          3: [835, 117, 163, 12, 3834],\n",
              "                          4: [984, 132, 209, 8, 3834],\n",
              "                          5: [1024, 159, 208, 8, 3834],\n",
              "                          6: [880, 122, 246, 21, 3834],\n",
              "                          7: [966, 137, 250, 21, 3834],\n",
              "                          8: [872, 118, 263, 33, 3834],\n",
              "                          9: [937, 125, 320, 41, 3834],\n",
              "                          10: [849, 111, 320, 50, 3834],\n",
              "                          11: [750, 88, 355, 72, 3834],\n",
              "                          12: [581, 70, 324, 78, 3834]}),\n",
              "             'VERB': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [54, 11, 0, 0, 6383],\n",
              "                          1: [2087, 416, 32, 9, 6383],\n",
              "                          2: [2101, 515, 44, 9, 6383],\n",
              "                          3: [1270, 342, 38, 5, 6383],\n",
              "                          4: [1397, 407, 65, 9, 6383],\n",
              "                          5: [1667, 437, 112, 30, 6383],\n",
              "                          6: [1864, 488, 149, 40, 6383],\n",
              "                          7: [1780, 479, 190, 56, 6383],\n",
              "                          8: [1997, 539, 268, 74, 6383],\n",
              "                          9: [2129, 562, 333, 92, 6383],\n",
              "                          10: [2190, 560, 414, 116, 6383],\n",
              "                          11: [2416, 590, 500, 155, 6383],\n",
              "                          12: [2021, 538, 541, 196, 6383]}),\n",
              "             'PRON': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [33, 0, 0, 0, 6217],\n",
              "                          1: [428, 4, 30, 0, 6217],\n",
              "                          2: [2141, 18, 180, 1, 6217],\n",
              "                          3: [3295, 15, 402, 1, 6217],\n",
              "                          4: [3360, 15, 671, 2, 6217],\n",
              "                          5: [3385, 13, 803, 2, 6217],\n",
              "                          6: [3357, 15, 908, 3, 6217],\n",
              "                          7: [3348, 16, 929, 3, 6217],\n",
              "                          8: [3170, 14, 1068, 3, 6217],\n",
              "                          9: [3153, 18, 1463, 9, 6217],\n",
              "                          10: [3054, 17, 1495, 9, 6217],\n",
              "                          11: [3046, 14, 1649, 8, 6217],\n",
              "                          12: [3298, 18, 1780, 10, 6217]}),\n",
              "             'NUM': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [15, 1, 2, 1, 640],\n",
              "                          1: [5, 0, 3, 0, 640],\n",
              "                          2: [32, 1, 5, 0, 640],\n",
              "                          3: [93, 0, 9, 0, 640],\n",
              "                          4: [118, 1, 16, 0, 640],\n",
              "                          5: [132, 2, 19, 0, 640],\n",
              "                          6: [152, 2, 26, 0, 640],\n",
              "                          7: [149, 3, 19, 0, 640],\n",
              "                          8: [155, 5, 24, 1, 640],\n",
              "                          9: [160, 9, 27, 1, 640],\n",
              "                          10: [176, 11, 35, 3, 640],\n",
              "                          11: [182, 10, 43, 4, 640],\n",
              "                          12: [181, 12, 48, 4, 640]}),\n",
              "             'NOUN': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [49, 10, 4, 2, 9218],\n",
              "                          1: [424, 95, 4, 4, 9218],\n",
              "                          2: [515, 108, 3, 2, 9218],\n",
              "                          3: [673, 122, 6, 2, 9218],\n",
              "                          4: [576, 112, 8, 4, 9218],\n",
              "                          5: [862, 159, 16, 5, 9218],\n",
              "                          6: [795, 133, 29, 3, 9218],\n",
              "                          7: [1076, 181, 44, 3, 9218],\n",
              "                          8: [1021, 205, 53, 8, 9218],\n",
              "                          9: [1100, 217, 84, 15, 9218],\n",
              "                          10: [1493, 278, 132, 31, 9218],\n",
              "                          11: [1830, 349, 195, 55, 9218],\n",
              "                          12: [1588, 297, 199, 52, 9218]}),\n",
              "             'ADJ': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [0, 0, 0, 0, 4382],\n",
              "                          1: [241, 20, 1, 0, 4382],\n",
              "                          2: [61, 3, 1, 0, 4382],\n",
              "                          3: [51, 4, 9, 2, 4382],\n",
              "                          4: [127, 5, 21, 3, 4382],\n",
              "                          5: [325, 16, 35, 5, 4382],\n",
              "                          6: [414, 16, 32, 7, 4382],\n",
              "                          7: [434, 20, 39, 7, 4382],\n",
              "                          8: [513, 28, 50, 10, 4382],\n",
              "                          9: [601, 31, 78, 14, 4382],\n",
              "                          10: [713, 41, 105, 24, 4382],\n",
              "                          11: [734, 53, 140, 31, 4382],\n",
              "                          12: [559, 42, 156, 31, 4382]}),\n",
              "             'DET': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [0, 0, 0, 0, 4330],\n",
              "                          1: [0, 0, 123, 1, 4330],\n",
              "                          2: [0, 0, 1220, 115, 4330],\n",
              "                          3: [0, 0, 1626, 158, 4330],\n",
              "                          4: [0, 0, 1629, 159, 4330],\n",
              "                          5: [0, 0, 1677, 159, 4330],\n",
              "                          6: [0, 0, 1723, 153, 4330],\n",
              "                          7: [1, 0, 1760, 171, 4330],\n",
              "                          8: [1, 0, 1740, 173, 4330],\n",
              "                          9: [2, 0, 1737, 160, 4330],\n",
              "                          10: [3, 0, 1649, 152, 4330],\n",
              "                          11: [1, 0, 1584, 147, 4330],\n",
              "                          12: [0, 0, 1782, 161, 4330]}),\n",
              "             'PUNCT': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [2570, 41, 984, 0, 5958],\n",
              "                          1: [2949, 17, 1142, 1, 5958],\n",
              "                          2: [2425, 6, 1808, 2, 5958],\n",
              "                          3: [2290, 6, 1644, 1, 5958],\n",
              "                          4: [2363, 6, 1404, 0, 5958],\n",
              "                          5: [2324, 5, 1591, 1, 5958],\n",
              "                          6: [2543, 5, 1742, 2, 5958],\n",
              "                          7: [2876, 8, 1778, 2, 5958],\n",
              "                          8: [2597, 6, 1793, 2, 5958],\n",
              "                          9: [2613, 5, 1772, 1, 5958],\n",
              "                          10: [2697, 7, 1727, 4, 5958],\n",
              "                          11: [2685, 11, 1747, 5, 5958],\n",
              "                          12: [3273, 24, 2098, 13, 5958]}),\n",
              "             'INTJ': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [0, 0, 0, 0, 136],\n",
              "                          1: [0, 0, 0, 0, 136],\n",
              "                          2: [6, 0, 2, 0, 136],\n",
              "                          3: [18, 0, 5, 0, 136],\n",
              "                          4: [13, 0, 3, 0, 136],\n",
              "                          5: [22, 0, 4, 0, 136],\n",
              "                          6: [28, 0, 5, 0, 136],\n",
              "                          7: [29, 0, 7, 0, 136],\n",
              "                          8: [31, 0, 7, 0, 136],\n",
              "                          9: [35, 0, 11, 1, 136],\n",
              "                          10: [38, 0, 15, 1, 136],\n",
              "                          11: [38, 0, 17, 2, 136],\n",
              "                          12: [25, 0, 12, 2, 136]}),\n",
              "             'ADP': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [766, 176, 9, 6, 4074],\n",
              "                          1: [424, 80, 94, 14, 4074],\n",
              "                          2: [449, 72, 89, 16, 4074],\n",
              "                          3: [1365, 207, 200, 21, 4074],\n",
              "                          4: [1307, 194, 245, 30, 4074],\n",
              "                          5: [1308, 194, 264, 41, 4074],\n",
              "                          6: [1191, 187, 281, 40, 4074],\n",
              "                          7: [1111, 173, 281, 48, 4074],\n",
              "                          8: [1257, 213, 425, 81, 4074],\n",
              "                          9: [1231, 210, 437, 91, 4074],\n",
              "                          10: [1204, 207, 488, 99, 4074],\n",
              "                          11: [1069, 182, 478, 112, 4074],\n",
              "                          12: [1052, 192, 528, 127, 4074]}),\n",
              "             'PROPN': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [0, 0, 1, 1, 1863],\n",
              "                          1: [58, 39, 0, 0, 1863],\n",
              "                          2: [105, 61, 1, 1, 1863],\n",
              "                          3: [214, 120, 4, 4, 1863],\n",
              "                          4: [241, 147, 7, 7, 1863],\n",
              "                          5: [279, 170, 8, 8, 1863],\n",
              "                          6: [358, 227, 12, 12, 1863],\n",
              "                          7: [270, 170, 15, 12, 1863],\n",
              "                          8: [264, 171, 23, 21, 1863],\n",
              "                          9: [246, 159, 27, 26, 1863],\n",
              "                          10: [280, 191, 37, 35, 1863],\n",
              "                          11: [269, 181, 46, 42, 1863],\n",
              "                          12: [216, 141, 50, 48, 1863]}),\n",
              "             'CCONJ': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [357, 9, 286, 6, 2225],\n",
              "                          1: [160, 5, 131, 3, 2225],\n",
              "                          2: [491, 12, 355, 6, 2225],\n",
              "                          3: [236, 2, 167, 1, 2225],\n",
              "                          4: [144, 2, 85, 0, 2225],\n",
              "                          5: [247, 1, 137, 0, 2225],\n",
              "                          6: [353, 5, 234, 3, 2225],\n",
              "                          7: [332, 6, 213, 4, 2225],\n",
              "                          8: [351, 9, 245, 5, 2225],\n",
              "                          9: [283, 9, 209, 6, 2225],\n",
              "                          10: [264, 4, 204, 2, 2225],\n",
              "                          11: [332, 7, 243, 4, 2225],\n",
              "                          12: [255, 11, 213, 9, 2225]}),\n",
              "             'SCONJ': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [0, 0, 20, 3, 907],\n",
              "                          1: [0, 0, 1, 0, 907],\n",
              "                          2: [0, 0, 0, 0, 907],\n",
              "                          3: [102, 2, 10, 0, 907],\n",
              "                          4: [242, 8, 45, 1, 907],\n",
              "                          5: [203, 8, 78, 2, 907],\n",
              "                          6: [210, 5, 86, 0, 907],\n",
              "                          7: [187, 7, 88, 1, 907],\n",
              "                          8: [170, 6, 86, 1, 907],\n",
              "                          9: [189, 7, 106, 1, 907],\n",
              "                          10: [199, 6, 114, 1, 907],\n",
              "                          11: [171, 5, 131, 2, 907],\n",
              "                          12: [129, 2, 142, 1, 907]}),\n",
              "             'AUX': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [15, 0, 2, 0, 3629],\n",
              "                          1: [511, 13, 36, 2, 3629],\n",
              "                          2: [586, 18, 135, 4, 3629],\n",
              "                          3: [744, 15, 176, 4, 3629],\n",
              "                          4: [862, 18, 267, 7, 3629],\n",
              "                          5: [1013, 22, 381, 10, 3629],\n",
              "                          6: [1106, 25, 399, 12, 3629],\n",
              "                          7: [1122, 25, 415, 14, 3629],\n",
              "                          8: [1222, 28, 509, 17, 3629],\n",
              "                          9: [1258, 30, 565, 18, 3629],\n",
              "                          10: [1239, 29, 601, 18, 3629],\n",
              "                          11: [1270, 29, 680, 17, 3629],\n",
              "                          12: [1160, 29, 657, 17, 3629]}),\n",
              "             'SYM': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [0, 0, 0, 0, 159],\n",
              "                          1: [1, 0, 1, 0, 159],\n",
              "                          2: [0, 0, 0, 0, 159],\n",
              "                          3: [14, 1, 5, 1, 159],\n",
              "                          4: [12, 1, 8, 0, 159],\n",
              "                          5: [29, 1, 24, 0, 159],\n",
              "                          6: [31, 1, 26, 1, 159],\n",
              "                          7: [36, 1, 30, 1, 159],\n",
              "                          8: [38, 1, 31, 1, 159],\n",
              "                          9: [42, 2, 31, 1, 159],\n",
              "                          10: [42, 2, 32, 1, 159],\n",
              "                          11: [40, 2, 31, 2, 159],\n",
              "                          12: [27, 0, 26, 1, 159]}),\n",
              "             'PART': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [0, 0, 0, 0, 1565],\n",
              "                          1: [124, 24, 105, 23, 1565],\n",
              "                          2: [299, 43, 265, 42, 1565],\n",
              "                          3: [380, 61, 332, 60, 1565],\n",
              "                          4: [371, 50, 321, 49, 1565],\n",
              "                          5: [211, 24, 191, 23, 1565],\n",
              "                          6: [244, 31, 219, 29, 1565],\n",
              "                          7: [224, 27, 203, 23, 1565],\n",
              "                          8: [305, 40, 281, 37, 1565],\n",
              "                          9: [305, 38, 278, 34, 1565],\n",
              "                          10: [318, 38, 289, 36, 1565],\n",
              "                          11: [231, 27, 212, 25, 1565],\n",
              "                          12: [358, 54, 321, 51, 1565]}),\n",
              "             'X': defaultdict(<function __main__.process_data.<locals>.<lambda>.<locals>.<lambda>()>,\n",
              "                         {0: [0, 0, 0, 0, 69],\n",
              "                          1: [0, 0, 0, 0, 69],\n",
              "                          2: [0, 0, 0, 0, 69],\n",
              "                          3: [0, 0, 0, 0, 69],\n",
              "                          4: [3, 1, 2, 0, 69],\n",
              "                          5: [3, 0, 1, 0, 69],\n",
              "                          6: [2, 1, 0, 0, 69],\n",
              "                          7: [2, 1, 0, 0, 69],\n",
              "                          8: [2, 0, 1, 0, 69],\n",
              "                          9: [0, 0, 2, 0, 69],\n",
              "                          10: [2, 0, 3, 0, 69],\n",
              "                          11: [3, 0, 4, 0, 69],\n",
              "                          12: [0, 0, 5, 0, 69]})})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wM1gOJTZr6Tz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7a0207d8cc954eb7bd5f31c7e99acd35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6dd818ca61604cbbbb27fcc4f00967fe",
              "IPY_MODEL_5b787e4fe8b24dde943e73c7db2ff841",
              "IPY_MODEL_af4a40eb086243efa6fa1d21d16815fc"
            ],
            "layout": "IPY_MODEL_9dc22ef36b2043c3abb99d68996546ec"
          }
        },
        "6dd818ca61604cbbbb27fcc4f00967fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cd48d2524fe4f2787e06d9522113ea9",
            "placeholder": "​",
            "style": "IPY_MODEL_62334709da264209a608305893874ced",
            "value": "config.json: 100%"
          }
        },
        "5b787e4fe8b24dde943e73c7db2ff841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7c36aa00bf74c34a63a9aa1a704f66a",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_542de6b91c48491cb08219d3e870df71",
            "value": 570
          }
        },
        "af4a40eb086243efa6fa1d21d16815fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d70f14c1d9ef4954996aae8706c6e671",
            "placeholder": "​",
            "style": "IPY_MODEL_38e655ecbaa541c1b3ff9707dac6ccd1",
            "value": " 570/570 [00:00&lt;00:00, 40.5kB/s]"
          }
        },
        "9dc22ef36b2043c3abb99d68996546ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cd48d2524fe4f2787e06d9522113ea9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62334709da264209a608305893874ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7c36aa00bf74c34a63a9aa1a704f66a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "542de6b91c48491cb08219d3e870df71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d70f14c1d9ef4954996aae8706c6e671": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38e655ecbaa541c1b3ff9707dac6ccd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2034c74e9af6449f869ebde329efb7b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e203c2dbb2874371ae42f4c347a4a264",
              "IPY_MODEL_482a4ac81f244af1920d9ed64b2e297c",
              "IPY_MODEL_97faa8c43737489e95e1c2b6d064c640"
            ],
            "layout": "IPY_MODEL_17246e9f9dbd4c3f8ed1403328225ed9"
          }
        },
        "e203c2dbb2874371ae42f4c347a4a264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e44f610976f34572bcf82ccdfe6354c9",
            "placeholder": "​",
            "style": "IPY_MODEL_33de9fab55dd4a608babb18cd92f91f2",
            "value": "model.safetensors: 100%"
          }
        },
        "482a4ac81f244af1920d9ed64b2e297c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf2ad21e39204fc29721209d48d43dc6",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_916bc11a08a94f11a7d4fe792c09ea7a",
            "value": 440449768
          }
        },
        "97faa8c43737489e95e1c2b6d064c640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46e59ecf49a04b71b2bdb98b0bbdb4d0",
            "placeholder": "​",
            "style": "IPY_MODEL_0a9c869453b6435d85c6e27a7f19b368",
            "value": " 440M/440M [00:01&lt;00:00, 330MB/s]"
          }
        },
        "17246e9f9dbd4c3f8ed1403328225ed9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e44f610976f34572bcf82ccdfe6354c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33de9fab55dd4a608babb18cd92f91f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf2ad21e39204fc29721209d48d43dc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "916bc11a08a94f11a7d4fe792c09ea7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46e59ecf49a04b71b2bdb98b0bbdb4d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a9c869453b6435d85c6e27a7f19b368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5acb23abcff441febf19d5cab9ee8b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1a09f5904c14f2b85da39fb64c4fae7",
              "IPY_MODEL_61be08e0e9624774b855d521da030c85",
              "IPY_MODEL_3ed37deb32ea480d805b7dd2f4d73c90"
            ],
            "layout": "IPY_MODEL_9f52c09f94e1472abebd9a894b5c42c5"
          }
        },
        "d1a09f5904c14f2b85da39fb64c4fae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7f0370d15b5432da1db9ea9c0dd0f4e",
            "placeholder": "​",
            "style": "IPY_MODEL_c4ee8ce740c8491e9945af618a9b1304",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "61be08e0e9624774b855d521da030c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e203a49fd734fd5b0d74d92c738630c",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2ec41d698b54acfb973b11223ca506f",
            "value": 48
          }
        },
        "3ed37deb32ea480d805b7dd2f4d73c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64653220c76149aeb2fcb3db8c46d95f",
            "placeholder": "​",
            "style": "IPY_MODEL_808ba03997a64faa9b7c38044d69cc43",
            "value": " 48.0/48.0 [00:00&lt;00:00, 4.12kB/s]"
          }
        },
        "9f52c09f94e1472abebd9a894b5c42c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7f0370d15b5432da1db9ea9c0dd0f4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4ee8ce740c8491e9945af618a9b1304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e203a49fd734fd5b0d74d92c738630c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2ec41d698b54acfb973b11223ca506f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64653220c76149aeb2fcb3db8c46d95f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "808ba03997a64faa9b7c38044d69cc43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dde54559e6945f7b0563f2829859b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05b92727d01741bc8014396532f0a3b1",
              "IPY_MODEL_d85b4f2fca9741e49b21527f48431243",
              "IPY_MODEL_aaa5dfc544e646d9aaba569f2a871c72"
            ],
            "layout": "IPY_MODEL_91db76b682cf490d95d3dd8762d8a5ac"
          }
        },
        "05b92727d01741bc8014396532f0a3b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7e5f8bfc8cb43f294a4679b80f47c57",
            "placeholder": "​",
            "style": "IPY_MODEL_cf21181902ae4d0288962562dfb2a05e",
            "value": "vocab.txt: 100%"
          }
        },
        "d85b4f2fca9741e49b21527f48431243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eebd9d66e5854f5daf43cd5f162e886c",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a6ed28be5494151a85d0bcec83e0051",
            "value": 231508
          }
        },
        "aaa5dfc544e646d9aaba569f2a871c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89eda6ed499e461c8067b1e7832b5974",
            "placeholder": "​",
            "style": "IPY_MODEL_08f98b93c72f4214bf201aba8ef3284c",
            "value": " 232k/232k [00:00&lt;00:00, 583kB/s]"
          }
        },
        "91db76b682cf490d95d3dd8762d8a5ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7e5f8bfc8cb43f294a4679b80f47c57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf21181902ae4d0288962562dfb2a05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eebd9d66e5854f5daf43cd5f162e886c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a6ed28be5494151a85d0bcec83e0051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89eda6ed499e461c8067b1e7832b5974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08f98b93c72f4214bf201aba8ef3284c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "746a123458d24c4fa74412d69e994e48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb03462c0fa047de94b117b777e1bf89",
              "IPY_MODEL_badd3cfa57a3438e9bf0b0e7e3563bba",
              "IPY_MODEL_29afeada0dd843cbbfdd39d05bde7307"
            ],
            "layout": "IPY_MODEL_6ffdca63f6bf45ceb4a927305deaa09e"
          }
        },
        "cb03462c0fa047de94b117b777e1bf89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8d9db59d43c41b2a007af4b01c5e4d6",
            "placeholder": "​",
            "style": "IPY_MODEL_7d1e1fd3f71744e99bcc730361ca9a5f",
            "value": "tokenizer.json: 100%"
          }
        },
        "badd3cfa57a3438e9bf0b0e7e3563bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_429a9bba7c3944289fc1dd5d590de26e",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8f96a172e1e4d738d49a34b4ac61a5b",
            "value": 466062
          }
        },
        "29afeada0dd843cbbfdd39d05bde7307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_135bfa486ccf4fdaabf6322b54badcf7",
            "placeholder": "​",
            "style": "IPY_MODEL_d5e2049398bb4e53b47301d72270bf91",
            "value": " 466k/466k [00:00&lt;00:00, 771kB/s]"
          }
        },
        "6ffdca63f6bf45ceb4a927305deaa09e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8d9db59d43c41b2a007af4b01c5e4d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d1e1fd3f71744e99bcc730361ca9a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "429a9bba7c3944289fc1dd5d590de26e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8f96a172e1e4d738d49a34b4ac61a5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "135bfa486ccf4fdaabf6322b54badcf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5e2049398bb4e53b47301d72270bf91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52cdf811fa1a429ebdfc7a26952b026c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f33895b09064cb7843fdc23e830c800",
              "IPY_MODEL_b12cd11750ae42bdb1891a44fa5353a6",
              "IPY_MODEL_05768e037d584fa2b5bdf5f34bdac0a7"
            ],
            "layout": "IPY_MODEL_e496fb6ece9d4943910f6b1cd596a8b7"
          }
        },
        "3f33895b09064cb7843fdc23e830c800": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7976d0abf363498897ca8bf1e0d08999",
            "placeholder": "​",
            "style": "IPY_MODEL_86418c61262443999cb49f960d0ed1b1",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: "
          }
        },
        "b12cd11750ae42bdb1891a44fa5353a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc0e21c8255b4c17ad880a4ed6c11f2f",
            "max": 47208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e30566d719a74bdebab1e48f41a5f47f",
            "value": 47208
          }
        },
        "05768e037d584fa2b5bdf5f34bdac0a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5fd425818f5408d8f06c7b73e7bc516",
            "placeholder": "​",
            "style": "IPY_MODEL_08cfdb214f6b42ab929d4f273c719687",
            "value": " 379k/? [00:00&lt;00:00, 9.46MB/s]"
          }
        },
        "e496fb6ece9d4943910f6b1cd596a8b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7976d0abf363498897ca8bf1e0d08999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86418c61262443999cb49f960d0ed1b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc0e21c8255b4c17ad880a4ed6c11f2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e30566d719a74bdebab1e48f41a5f47f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5fd425818f5408d8f06c7b73e7bc516": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08cfdb214f6b42ab929d4f273c719687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e0558316ef441739127bd16a203c362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4801be5d6a924a6c81a4a72e9361fcd8",
              "IPY_MODEL_8c6e7734096f4b4aab4d5c9f78ecae22",
              "IPY_MODEL_6ed10af62eda4278bf2a6528d31799b2"
            ],
            "layout": "IPY_MODEL_1af225ddc1614da8a6d5011758e95308"
          }
        },
        "4801be5d6a924a6c81a4a72e9361fcd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_442ce855a39041038bf014e3ddf22c44",
            "placeholder": "​",
            "style": "IPY_MODEL_39bd75748d014329a90fe55225db5354",
            "value": "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/default.zip: 100%"
          }
        },
        "8c6e7734096f4b4aab4d5c9f78ecae22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22e9e645a203495085d3042b43f58f8b",
            "max": 526684143,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57313bf489aa4588bab97806f102a365",
            "value": 526684143
          }
        },
        "6ed10af62eda4278bf2a6528d31799b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fc27f10e9294228ac21ba4f055e48d6",
            "placeholder": "​",
            "style": "IPY_MODEL_06570b4dceab45de8083dc6e1de87966",
            "value": " 527M/527M [00:06&lt;00:00, 89.5MB/s]"
          }
        },
        "1af225ddc1614da8a6d5011758e95308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "442ce855a39041038bf014e3ddf22c44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39bd75748d014329a90fe55225db5354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22e9e645a203495085d3042b43f58f8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57313bf489aa4588bab97806f102a365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fc27f10e9294228ac21ba4f055e48d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06570b4dceab45de8083dc6e1de87966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dda49002e9647e8ba785fd85dd253e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_452dee80d1b64e7881d439874c472788",
              "IPY_MODEL_77cee74c2a1a49c48d689c5619d5b544",
              "IPY_MODEL_7bb767d7aac742348a38be85ffc0865c"
            ],
            "layout": "IPY_MODEL_3edd9b9fd268409ca79470e559a031ba"
          }
        },
        "452dee80d1b64e7881d439874c472788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3b93e2eddbb497cb982cb5b36cf002d",
            "placeholder": "​",
            "style": "IPY_MODEL_f21e1bf57d5344539092db3c8c1e720f",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: "
          }
        },
        "77cee74c2a1a49c48d689c5619d5b544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b50a00e634df4b7ca8822a2d8ec50cd3",
            "max": 47208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3fb5b6d966c9499eb71d3e4e869053b4",
            "value": 47208
          }
        },
        "7bb767d7aac742348a38be85ffc0865c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d87757fc3424196934d36c033119ad8",
            "placeholder": "​",
            "style": "IPY_MODEL_1f775c1542b1440e9a72fcdf05782ad7",
            "value": " 379k/? [00:00&lt;00:00, 9.66MB/s]"
          }
        },
        "3edd9b9fd268409ca79470e559a031ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3b93e2eddbb497cb982cb5b36cf002d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f21e1bf57d5344539092db3c8c1e720f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b50a00e634df4b7ca8822a2d8ec50cd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fb5b6d966c9499eb71d3e4e869053b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d87757fc3424196934d36c033119ad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f775c1542b1440e9a72fcdf05782ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}